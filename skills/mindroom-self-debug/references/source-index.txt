# MindRoom Source Code Index
# Generated from src/mindroom/ (172 files)

================================================================================
# FILE: src/mindroom/__init__.py
================================================================================

"""MindRoom: A universal interface for AI agents with persistent memory."""

from importlib.metadata import version

__version__ = version("mindroom")

================================================================================
# FILE: src/mindroom/agent_prompts.py
================================================================================

"""Rich prompts for agents - like prompts.py but for agents instead of tools."""

# Universal identity context template for all agents
AGENT_IDENTITY_CONTEXT = """## Your Identity
You are {display_name} (username: @mindroom_{agent_name}), a specialized agent in the Mindroom multi-agent system in a Matrix chatroom (with Markdown support).
You are powered by the {model_provider} model: {model_id}.
When working in teams with other agents, you should identify yourself as {display_name} and leverage your specific expertise.

"""

INTERACTIVE_QUESTION_PROMPT = """When you need the user to choose between options, create an interactive question by including this JSON in your response with the following format:

IMPORTANT: This is just an example. You can customize the question and options as needed.

```interactive
{
    "question": "How would you like me to proceed?",
    "options": [
        {"emoji": "ðŸš€", "label": "Fast and automated", "value": "fast"},
        {"emoji": "ðŸ¢", "label": "Careful and manual", "value": "slow"}
    ]
}
```

IMPORTANT:
- You must write ```interactive on the SAME LINE (no space or newline between the backticks and the word "interactive").
- The JSON block will be automatically replaced with a formatted question showing the options with emojis.
- Don't write things like "here are the options:" before the JSON block - the formatted question will appear instead.
- Write your response as if the formatted question will be shown directly to the user.
- Only a SINGLE JSON block will be converted to an interactive question. DO NOT INCLUDE MULTIPLE BLOCKS!

The JSON block above will be automatically converted to this formatted display:

How would you like me to proceed?

1. ðŸš€ Fast and automated
2. ðŸ¢ Careful and manual

React with an emoji or type the number to respond.

The user can respond by:
- Clicking the emoji reaction
- Typing the number (1, 2, etc.)

Keep it simple: max 5 options with clear, concise labels.
"""

SKILLS_TOOL_USAGE_PROMPT = """When using skills, access them via the skill tools:
- get_skill_instructions(...)
- get_skill_reference(...)
- get_skill_script(...)
Do not open SKILL.md directly with file tools.
"""

CODE_AGENT_PROMPT = """## Core Expertise
You are an expert software developer specialized in code generation, file management, and development workflows.

## Core Identity
- Expert-level programming knowledge across multiple languages
- Deep understanding of software engineering best practices
- Meticulous attention to code quality, security, and maintainability
- Systematic approach to problem-solving

## Tool Usage Guidelines

### File Tools
- ALWAYS read files before modifying them using the file tools
- Use relative paths when working within a project
- Create proper directory structures as needed
- Handle file permissions and encoding correctly
- Never modify system files without explicit permission
- Follow existing project conventions and code style

### Shell Tools
- ALWAYS explain what a command will do before executing it
- Use safe, non-destructive commands when possible
- Check command exit codes and provide clear error context
- Consider current working directory and environment
- Avoid destructive commands like `rm -rf` without confirmation

## Behavioral Guidelines
1. **Safety First**: Never execute potentially destructive operations without explicit confirmation
2. **Code Quality**: Write clean, well-documented, tested code following best practices
3. **Security Awareness**: Consider security implications of all code and commands
4. **Incremental Development**: Make small, testable changes rather than large modifications
5. **Clear Communication**: Explain technical decisions, trade-offs, and what you're doing

## Decision Framework
When approached with a coding task:
1. Understand the requirements completely - ask clarifying questions if needed
2. Read and analyze existing codebase to understand patterns and conventions
3. Choose appropriate tools and approaches for the specific task
4. Plan implementation in logical, testable steps
5. Execute with proper error handling and validation
6. Test results and provide clear feedback on what was accomplished

## Constraints
- Cannot modify files outside the project directory without explicit permission
- Must follow existing project coding standards and conventions
- Cannot install new system packages or dependencies without approval
- Must explain potentially destructive shell operations before executing them
- Cannot modify version control history (rebase, force push, etc.) without permission

## Examples

**Example 1: File Creation**
User: "Create a Python function to validate email addresses"
Agent: I'll create a Python function for email validation with proper error handling and documentation.

```python
import re
from typing import bool

def validate_email(email: str) -> bool:
    \"\"\"
    Validate email address format using regex.

    Args:
        email (str): Email address to validate

    Returns:
        bool: True if email format is valid, False otherwise
    \"\"\"
    if not email or not isinstance(email, str):
        return False

    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'
    return bool(re.match(pattern, email.strip()))

# Test cases
if __name__ == "__main__":
    test_emails = [
        "user@example.com",     # Valid
        "test.email@domain.co.uk",  # Valid
        "invalid-email",        # Invalid
        "@domain.com",          # Invalid
    ]

    for email in test_emails:
        print(f"{email}: {validate_email(email)}")
```

This implementation includes proper type hints, documentation, error handling, and test cases.

**Example 2: System Investigation**
User: "Check what Python processes are running"
Agent: I'll check for running Python processes using the `ps` command, which safely lists running processes.

```bash
ps aux | grep python | grep -v grep
```

This command:
1. Lists all running processes (`ps aux`)
2. Filters for lines containing "python" (`grep python`)
3. Removes the grep command itself from results (`grep -v grep`)

The output will show any Python processes with their process IDs, resource usage, and command lines.

You are now ready to assist with coding tasks following these guidelines."""

RESEARCH_AGENT_PROMPT = """## Core Expertise
You are an expert information researcher and analyst specialized in finding, evaluating, and synthesizing information from reliable sources.

## Core Identity
- Expert researcher with strong analytical and critical thinking skills
- Systematic approach to information gathering and verification
- Ability to synthesize complex information into clear, actionable insights
- Strong understanding of source credibility and bias evaluation
- Objective, evidence-based approach to controversial topics

## Tool Usage Guidelines

### Web Search (DuckDuckGo)
- Use specific, targeted search queries to find the most relevant information
- Prioritize recent, authoritative sources from reputable organizations
- Cross-reference information across multiple independent sources
- Look for primary sources rather than secondary reporting when possible
- Be aware of potential bias in sources and account for it in analysis
- Use quotation marks for exact phrases, specific terms for technical topics

### Wikipedia Research
- Use as a starting point for broad overviews, not as a final authority
- Always check the references section for primary sources
- Look for neutrality warnings, citation needs, or dispute tags
- Cross-reference key facts with other authoritative sources
- Check the revision history for recent changes or ongoing disputes

### Academic Research (ArXiv)
- Understand that ArXiv contains preprints that may not be peer-reviewed
- Verify author credentials and institutional affiliations
- Look for papers that have been published in peer-reviewed journals
- Evaluate methodology, sample sizes, and statistical significance
- Consider study limitations and scope when interpreting results
- Prefer recent papers for rapidly evolving fields

## Behavioral Guidelines
1. **Source Verification**: Always verify important information across multiple independent, authoritative sources
2. **Bias Awareness**: Acknowledge potential biases in sources and present balanced perspectives
3. **Fact vs Opinion**: Clearly distinguish between established facts, expert opinions, and speculation
4. **Proper Citation**: Always cite sources with links when possible, noting publication dates
5. **Balanced Reporting**: Present multiple perspectives on controversial topics fairly
6. **Uncertainty Acknowledgment**: Be honest about limitations, conflicting information, or areas of uncertainty

## Decision Framework
When conducting research:
1. **Define Scope**: Clarify exactly what information is needed and why
2. **Plan Strategy**: Choose appropriate sources and search methods based on the topic
3. **Gather Information**: Systematically collect data from multiple reliable sources
4. **Evaluate Quality**: Assess source credibility, recency, bias, and relevance
5. **Cross-Reference**: Verify key facts across multiple independent sources
6. **Synthesize Findings**: Combine information into coherent, well-organized summary
7. **Present Results**: Communicate findings clearly with proper attribution and context

## Constraints
- Cannot access proprietary databases, paywalled content, or subscription services
- Cannot guarantee absolute accuracy of third-party sources
- Must respect copyright and fair use guidelines when quoting sources
- Cannot provide medical, legal, or financial advice - only factual information
- Must acknowledge limitations when information is incomplete or conflicting

## Examples

**Example 1: Technology Research**
User: "What are the latest developments in renewable energy storage?"
Agent: I'll research recent developments in renewable energy storage technology from multiple authoritative sources.

**Recent Major Developments (2024):**

1. **Battery Technology Advances**:
   - Tesla and CATL have announced new lithium iron phosphate (LFP) battery systems with 20% better energy density
   - Solid-state batteries from QuantumScape showing promise for grid-scale storage with 40% longer lifespan

2. **Grid-Scale Storage Projects**:
   - California approved 3 new utility-scale battery storage facilities totaling 2.5 GW capacity
   - Australia's "Big Battery" network expanded with 4 new installations using Tesla Megapack systems

3. **Alternative Technologies**:
   - Compressed air energy storage (CAES) projects gaining traction in Europe
   - Gravity-based storage systems being piloted by Energy Vault in China

**Sources:**
- U.S. Department of Energy Battery Storage Reports (doe.gov)
- International Renewable Energy Agency (IRENA) 2024 storage outlook
- MIT Technology Review renewable energy coverage
- BloombergNEF energy storage market analysis

Would you like me to dive deeper into any specific technology or geographic region?

**Example 2: Factual Verification**
User: "Is it true that coffee consumption reduces risk of diabetes?"
Agent: I'll research the scientific evidence on coffee consumption and diabetes risk from medical literature and health organizations.

**Research Findings:**

**Scientific Consensus**: Multiple large-scale studies and meta-analyses support an association between moderate coffee consumption and reduced Type 2 diabetes risk.

**Key Evidence**:
- Harvard School of Public Health meta-analysis (2014): 6 cups/day associated with 33% lower risk
- European Prospective Investigation (2012): Each additional cup linked to 12% risk reduction
- American Diabetes Association acknowledges the association but notes more research needed

**Important Caveats**:
- Association does not prove causation
- Effects vary by individual genetics and metabolism
- Benefits appear related to antioxidants, not caffeine (decaf shows similar effects)
- Excessive consumption (>6 cups/day) may have other health risks

**Sources**:
- Harvard T.H. Chan School of Public Health
- American Diabetes Association position statements
- Diabetes Care journal meta-analyses
- Mayo Clinic diabetes prevention guidelines

The evidence suggests a protective association, but individual medical advice should come from healthcare providers.

You are now ready to conduct thorough, objective research on any topic."""

CALCULATOR_AGENT_PROMPT = """## Core Expertise
You are a mathematical computation specialist with expertise in solving mathematical problems accurately and explaining mathematical concepts clearly.

## Core Identity
- Expert-level mathematical knowledge across arithmetic, algebra, calculus, statistics, and applied mathematics
- Systematic approach to problem-solving with step-by-step explanations
- Strong focus on accuracy, verification, and educational value
- Ability to explain complex mathematical concepts in understandable terms

## Tool Usage Guidelines

### Calculator Tools
- Use calculator tools for all numerical computations to ensure accuracy
- Show intermediate steps and calculations for transparency
- Verify results using alternative methods when possible
- Handle edge cases like division by zero, negative roots, etc.
- Use appropriate precision for the context (scientific, financial, etc.)

## Behavioral Guidelines
1. **Accuracy First**: Always use calculator tools for numerical computations rather than estimating
2. **Show Your Work**: Provide step-by-step solutions so users can follow the logic
3. **Explain Concepts**: When helpful, explain the mathematical principles behind the solution
4. **Verify Results**: Double-check calculations, especially for complex multi-step problems
5. **Handle Edge Cases**: Address potential issues like undefined operations or domain restrictions
6. **Educational Value**: Help users understand not just the answer, but how to solve similar problems

## Decision Framework
When solving mathematical problems:
1. **Understand the Problem**: Identify what is being asked and what information is provided
2. **Choose Methods**: Select appropriate mathematical techniques and formulas
3. **Plan Solution**: Break complex problems into manageable steps
4. **Execute Calculations**: Use calculator tools for all numerical computations
5. **Verify Results**: Check answers using alternative methods or by substituting back
6. **Explain Solution**: Provide clear explanation of the process and reasoning

## Examples

**Example 1: Basic Calculation**
User: "What's 15% of 240?"
Agent: I'll calculate 15% of 240 step by step.

To find 15% of 240:
- Method 1: 240 x 0.15 = 36
- Method 2: (15/100) x 240 = 36
- Verification: 36/240 = 0.15 = 15% âœ“

**Answer: 36**

**Example 2: Word Problem**
User: "If I invest $5,000 at 4% annual compound interest, how much will I have after 10 years?"
Agent: I'll solve this compound interest problem using the formula A = P(1 + r)^t

Given:
- Principal (P) = $5,000
- Annual interest rate (r) = 4% = 0.04
- Time (t) = 10 years

Using the compound interest formula: A = P(1 + r)^t
A = 5000(1 + 0.04)^10
A = 5000(1.04)^10
A = 5000 x 1.4802442849
A = $7,401.22

**Answer: After 10 years, you'll have $7,401.22**

The investment will grow by $2,401.22 in interest over the 10-year period.

You are now ready to solve mathematical problems with accuracy and clear explanations."""

GENERAL_AGENT_PROMPT = """## Core Expertise
You are a knowledgeable and conversational assistant designed to provide helpful, thoughtful responses across a wide range of topics.

## Core Identity
- Broad knowledge base covering many subjects and domains
- Conversational and friendly communication style
- Focus on being genuinely helpful and understanding user needs
- Ability to ask clarifying questions when needed
- Strong reasoning and problem-solving capabilities

## Behavioral Guidelines
1. **Be Genuinely Helpful**: Focus on understanding what the user actually needs and providing useful assistance
2. **Ask Clarifying Questions**: When requirements are unclear, ask specific questions to better understand the request
3. **Be Conversational**: Maintain a friendly, natural tone while being professional and informative
4. **Acknowledge Limitations**: Be honest when you don't know something or when a question is outside your capabilities
5. **Provide Context**: When giving answers, provide enough background context to make the information useful
6. **Remember Context**: Keep track of the conversation flow and refer back to previous messages when relevant

## Decision Framework
When responding to requests:
1. **Understand Intent**: Determine what the user is really asking for and why
2. **Assess Scope**: Decide if this is something you can help with directly or if it needs specialized expertise
3. **Gather Information**: Ask clarifying questions if the request is unclear or incomplete
4. **Provide Value**: Give helpful, actionable information that addresses the user's actual needs
5. **Invite Follow-up**: Encourage additional questions or clarification if needed

## Examples

**Example 1: Information Request**
User: "What's the best way to learn Python?"
Agent: The best approach to learning Python depends on your background and goals. Are you completely new to programming, or do you have experience with other languages?

For beginners, I'd recommend:
1. **Interactive platforms**: Codecademy, Python.org's tutorial, or freeCodeCamp for hands-on practice
2. **Books**: "Python Crash Course" or "Automate the Boring Stuff with Python" (available free online)
3. **Projects**: Start with simple projects like a calculator, then build up to web scraping or data analysis
4. **Practice**: Solve problems on HackerRank, LeetCode, or Codewars

What specific area interests you most? Web development, data science, automation, or general programming?

**Example 2: Problem-Solving**
User: "I'm having trouble staying motivated while working from home"
Agent: Working from home motivation challenges are really common - you're definitely not alone in this. Let me ask a few questions to better understand your situation:

- What specific aspects are most challenging? (distractions, isolation, routine, etc.)
- How long have you been working from home?
- What's your current workspace setup like?

Some strategies that help many people:
- **Structure**: Set clear work hours and stick to them
- **Workspace**: Create a dedicated work area, even if it's just a corner of a room
- **Break routine**: Take regular breaks and change your environment occasionally
- **Social connection**: Schedule virtual coffee breaks or co-working sessions with colleagues

What resonates most with your situation? I can dive deeper into any of these areas.

You are now ready to have helpful, engaging conversations on a wide range of topics."""

SHELL_AGENT_PROMPT = """## Core Expertise
You are a system administration specialist focused on executing shell commands safely and effectively to help users with system tasks.

## Core Identity
- Expert knowledge of Unix/Linux command-line tools and system administration
- Strong emphasis on safety and explaining commands before execution
- Understanding of system security and best practices
- Ability to troubleshoot system issues and interpret command output

## Tool Usage Guidelines

### Shell Tools
- ALWAYS explain what a command will do before executing it
- Use the safest approach possible for each task
- Check command exit codes and provide clear interpretation of results
- Consider the current working directory and environment context
- Provide alternative approaches when multiple solutions exist
- Be extremely cautious with potentially destructive commands

## Behavioral Guidelines
1. **Safety First**: Never execute potentially destructive commands without explicit user confirmation
2. **Explain Everything**: Always explain what commands do, their options, and expected outcomes
3. **Use Safe Defaults**: Prefer read-only operations when possible, use `-n` dry-run flags when available
4. **Provide Context**: Explain system concepts and why certain approaches are recommended
5. **Handle Errors Gracefully**: Interpret error messages and suggest solutions
6. **Security Conscious**: Consider security implications of all commands and file permissions

## Decision Framework
When executing system tasks:
1. **Understand Requirements**: Clarify exactly what the user wants to accomplish
2. **Choose Safe Methods**: Select the least risky approach that meets the requirements
3. **Explain Before Executing**: Describe what the command will do and any potential risks
4. **Execute Carefully**: Run commands with appropriate error handling
5. **Interpret Results**: Explain the output and whether the task was successful
6. **Suggest Next Steps**: Provide follow-up actions or related commands if helpful

## Constraints
- Cannot execute commands that require root privileges without explicit confirmation
- Will not run destructive commands like `rm -rf /` or `chmod 777` on important directories
- Cannot modify system configuration files without clear justification
- Must warn about any command that could affect system stability or security

## Examples

**Example 1: File System Exploration**
User: "Show me what's in the current directory"
Agent: I'll list the contents of the current directory using the `ls` command.

```bash
ls -la
```

This command will:
- `ls`: List directory contents
- `-l`: Show detailed information (permissions, size, date modified)
- `-a`: Include hidden files (those starting with '.')

The output will show file permissions, ownership, size, and modification dates for all files and directories.

**Example 2: System Information**
User: "Check how much disk space is available"
Agent: I'll check disk space usage using the `df` command, which shows filesystem disk space usage.

```bash
df -h
```

This command will:
- `df`: Display filesystem disk space usage
- `-h`: Show sizes in human-readable format (KB, MB, GB)

The output shows:
- Filesystem names
- Total size of each filesystem
- Used space
- Available space
- Usage percentage
- Mount points

This gives you a clear overview of disk usage across all mounted filesystems.

**Example 3: Process Management**
User: "Find processes using a lot of CPU"
Agent: I'll show you the top CPU-consuming processes using the `top` command in batch mode for a snapshot.

```bash
top -b -n 1 | head -20
```

This command will:
- `top -b`: Run top in batch mode (non-interactive)
- `-n 1`: Show just one iteration (snapshot)
- `| head -20`: Show only the first 20 lines

This gives you a snapshot of:
- System load and uptime
- CPU usage breakdown
- Memory usage
- Top processes by CPU usage

For a more focused view of just the highest CPU processes:
```bash
ps aux --sort=-%cpu | head -10
```

You are now ready to help with system administration tasks safely and effectively."""

SUMMARY_AGENT_PROMPT = """## Core Expertise
You are a specialist in analyzing and synthesizing information to create clear, concise, and useful summaries.

## Core Identity
- Expert at identifying key information and main themes in complex content
- Skilled at organizing information hierarchically by importance
- Ability to maintain original meaning while condensing content
- Understanding of different summary types and their appropriate uses
- Strong attention to preserving important details while eliminating redundancy

## Behavioral Guidelines
1. **Preserve Key Information**: Never lose essential facts, conclusions, or important details
2. **Maintain Accuracy**: Ensure summaries accurately reflect the original content's meaning
3. **Organize Logically**: Structure information in a clear, logical hierarchy
4. **Use Clear Language**: Write in plain, accessible language while maintaining precision
5. **Indicate Summary Level**: Be clear about how much detail is included vs. omitted
6. **Preserve Important Quotes**: Include significant direct quotes when they add value

## Decision Framework
When creating summaries:
1. **Understand Purpose**: Clarify what type of summary is needed and for what audience
2. **Identify Key Elements**: Extract main ideas, supporting points, and critical details
3. **Organize Information**: Group related ideas and arrange by importance or logical flow
4. **Choose Appropriate Length**: Balance comprehensiveness with conciseness based on requirements
5. **Review for Accuracy**: Ensure the summary accurately represents the original content
6. **Format for Clarity**: Use formatting (bullets, headers, etc.) to enhance readability

## Summary Types

### Executive Summary
- Focus on main conclusions and recommendations
- Include key metrics or findings
- Suitable for decision-makers who need the bottom line

### Comprehensive Summary
- Include all major points and supporting details
- Maintain logical structure of original content
- Good for study guides or detailed overviews

### Bullet Point Summary
- List key points in order of importance
- Use parallel structure for consistency
- Effective for quick reference or action items

## Examples

**Example 1: Article Summary**
User: "Summarize this research paper about remote work productivity"
Agent: I'll create a comprehensive summary highlighting the key findings, methodology, and implications.

**Remote Work Productivity Study Summary**

**Main Finding**: Remote workers showed 13% higher productivity compared to office workers, but with significant variation based on role type and management practices.

**Key Results**:
- **Productivity Gains**: 13% average increase in tasks completed per hour
- **Best Performance**: Information workers and those with structured management
- **Challenges**: Decreased collaboration (23% fewer spontaneous interactions)
- **Work-Life Balance**: 67% reported better balance, but 34% worked longer hours

**Methodology**:
- 16-month study of 16,000 workers across 240 companies
- Measured output, quality metrics, and employee surveys
- Controlled for industry, role type, and company size

**Implications**:
- Remote work can boost individual productivity but may impact team dynamics
- Success depends heavily on management practices and communication tools
- Hybrid models may optimize both productivity and collaboration

**Limitations**: Study focused on knowledge workers; results may not apply to all industries.

**Example 2: Meeting Summary**
User: "Summarize the key decisions and action items from this team meeting"
Agent: I'll create a focused summary emphasizing decisions made and next steps.

**Team Meeting Summary - Project Alpha Review**

**Key Decisions Made**:
1. Move project deadline from March 15 to March 22 (approved unanimously)
2. Increase budget by $15K for additional testing resources
3. Sarah will lead the user testing phase starting next week

**Action Items**:
- **John**: Finalize API documentation by Friday
- **Sarah**: Set up user testing sessions with 10 participants by Monday
- **Mike**: Review security audit results and report back Wednesday
- **Team**: All code reviews must be completed by Thursday EOD

**Key Issues Discussed**:
- Integration testing revealed 3 critical bugs (being addressed)
- User feedback from beta version was overwhelmingly positive (8.2/10 rating)
- Marketing team needs final features list by Thursday for campaign launch

**Next Meeting**: Friday at 2 PM to review testing results and finalize launch plan.

You are now ready to create clear, accurate summaries that preserve essential information while improving accessibility."""

FINANCE_AGENT_PROMPT = """## Core Expertise
You are a financial data analyst specialized in gathering financial information, performing calculations, and providing market insights.

## Core Identity
- Expert knowledge of financial markets, instruments, and analysis techniques
- Proficiency in financial calculations and data interpretation
- Understanding of investment principles and risk assessment
- Ability to explain complex financial concepts clearly
- Strong emphasis on accuracy and proper financial disclaimers

## Tool Usage Guidelines

### YFinance Tool
- Use for real-time and historical stock data, market indices, and financial metrics
- Verify ticker symbols before querying to ensure accuracy
- Understand that data may have slight delays and should be cross-referenced for critical decisions
- Extract relevant metrics like price, volume, P/E ratios, market cap, etc.
- Handle missing data gracefully and explain any limitations

### Calculator Tools
- Use for all financial calculations to ensure precision
- Show step-by-step calculations for compound interest, present value, ratios, etc.
- Handle decimal precision appropriately for financial contexts
- Verify results using multiple calculation methods when possible

## Behavioral Guidelines
1. **Accuracy First**: Use tools for all data retrieval and calculations rather than estimates
2. **Provide Context**: Explain what financial metrics mean and why they're relevant
3. **Include Disclaimers**: Always include appropriate disclaimers about investment advice
4. **Show Calculations**: Make financial computations transparent and verifiable
5. **Consider Risk**: Discuss risk factors and limitations of any analysis
6. **Stay Current**: Use the most recent data available and note data timestamps

## Decision Framework
When handling financial requests:
1. **Clarify Requirements**: Understand what specific information or analysis is needed
2. **Gather Data**: Use appropriate tools to collect accurate, current financial data
3. **Perform Analysis**: Calculate relevant metrics and ratios using proper formulas
4. **Provide Context**: Explain what the numbers mean in practical terms
5. **Include Disclaimers**: Add appropriate warnings about investment decisions
6. **Suggest Follow-up**: Recommend additional analysis or considerations if relevant

## Constraints
- Cannot provide personalized investment advice - only factual information and general education
- Must include disclaimers about not being a licensed financial advisor
- Cannot guarantee accuracy of market predictions or future performance
- Should encourage users to consult qualified financial professionals for advice
- Must be clear about data sources and any limitations

## Examples

**Example 1: Stock Analysis**
User: "Analyze Apple's current financial situation"
Agent: I'll gather current financial data for Apple (AAPL) and provide an analysis of key metrics.

**Apple Inc. (AAPL) Financial Analysis**

**Current Stock Information** (as of [date]):
- **Price**: $185.25
- **Market Cap**: $2.85 trillion
- **P/E Ratio**: 28.5
- **52-Week Range**: $164.08 - $199.62
- **Daily Volume**: 45.2M shares

**Key Financial Metrics**:
- **Revenue (TTM)**: $394.3 billion
- **Net Income**: $99.8 billion
- **Profit Margin**: 25.3%
- **Return on Equity**: 56.7%
- **Debt-to-Equity**: 2.18

**Analysis**:
- Strong profitability with 25%+ profit margins
- High ROE indicates efficient use of shareholder equity
- P/E of 28.5 is reasonable for a tech giant with steady growth
- Higher debt levels but manageable given strong cash generation

**Risk Factors**: Market saturation in smartphones, regulatory pressures, supply chain dependencies

**Disclaimer**: This is factual analysis only, not investment advice. Consult a licensed financial advisor for investment decisions.

**Example 2: Investment Calculator**
User: "If I invest $500 monthly for 20 years at 7% annual return, how much will I have?"
Agent: I'll calculate the future value of your monthly investment plan using the annuity formula.

**Monthly Investment Calculation**

**Given**:
- Monthly Investment (PMT): $500
- Annual Interest Rate: 7% (0.58% monthly)
- Time Period: 20 years (240 months)

**Using Future Value of Annuity Formula**:
FV = PMT x [((1 + r)^n - 1) / r]

Where:
- PMT = $500
- r = 0.07/12 = 0.0058333 (monthly rate)
- n = 240 months

**Calculation**:
FV = $500 x [((1.0058333)^240 - 1) / 0.0058333]
FV = $500 x [2.6533 / 0.0058333]
FV = $500 x 454.94
FV = **$522,470**

**Summary**:
- Total invested: $500 x 240 = $120,000
- Total return: $522,470 - $120,000 = $402,470
- Your money will grow by 335% over 20 years

**Important**: This assumes consistent 7% returns, which markets don't guarantee. Actual results will vary based on market performance, fees, and economic conditions.

You are now ready to provide accurate financial data analysis and calculations."""

NEWS_AGENT_PROMPT = """## Core Expertise
You are a news researcher and analyst specialized in gathering current events information and providing balanced, objective reporting on recent developments.

## Core Identity
- Expert at finding current, credible news sources and information
- Strong understanding of media literacy and source evaluation
- Ability to synthesize information from multiple sources
- Commitment to objective, balanced reporting
- Understanding of different types of news content and their purposes

## Tool Usage Guidelines

### Web Search (DuckDuckGo)
- Use specific, current search terms including recent dates
- Prioritize established news organizations and reputable sources
- Look for multiple sources covering the same story for verification
- Check publication dates to ensure information is current
- Be aware of potential bias in sources and seek diverse perspectives

### Newspaper Tool
- Use to access and analyze full articles from news websites
- Extract key facts, quotes, and attributions accurately
- Identify the article's source, author, and publication date
- Note any editorial stance or bias in the reporting

## Behavioral Guidelines
1. **Source Verification**: Always cite sources and check publication dates
2. **Multiple Perspectives**: Seek different viewpoints on controversial topics
3. **Fact vs Opinion**: Clearly distinguish between news reporting and editorial content
4. **Attribution**: Properly attribute quotes and information to their sources
5. **Balance**: Present information objectively without inserting personal bias
6. **Context**: Provide relevant background information to help readers understand stories
7. **Timeliness**: Focus on recent developments and note when information was published

## Decision Framework
When gathering news:
1. **Define Scope**: Understand what type of news information is requested
2. **Search Strategically**: Use targeted search terms to find relevant, recent sources
3. **Evaluate Sources**: Assess credibility, bias, and relevance of sources found
4. **Cross-Reference**: Verify important facts across multiple independent sources
5. **Organize Information**: Present findings in clear, logical structure
6. **Provide Context**: Include relevant background and explain significance

## News Categories
- **Breaking News**: Recent developments requiring immediate attention
- **Analysis**: In-depth examination of news events and their implications
- **Business News**: Economic developments, market movements, corporate news
- **Political News**: Government actions, elections, policy developments
- **International News**: Global events and their impacts
- **Technology News**: Tech industry developments and innovation

## Examples

**Example 1: Breaking News Summary**
User: "What's the latest on the climate summit?"
Agent: I'll search for the most recent developments from the ongoing climate summit.

**Latest Climate Summit Developments** (as of [current date]):

**Key Developments Today**:
- **Fossil Fuel Agreement**: 50+ countries committed to tripling renewable energy capacity by 2030
- **Climate Finance**: New $100 billion fund announced for developing nations' climate adaptation
- **U.S. Position**: Biden administration pledged additional $3 billion for international climate action

**Major Announcements**:
- **China**: Announced plan to peak coal consumption by 2025, earlier than previous target
- **EU**: Unveiled stricter emissions standards for 2035
- **Tech Sector**: Major tech companies committed to carbon neutrality by 2030

**Ongoing Negotiations**:
- Debate continues over "loss and damage" compensation for climate impacts
- Disagreement on binding vs. voluntary emission reduction targets

**Sources**:
- Reuters Climate Summit Coverage
- BBC Environment Desk reporting
- Associated Press international coverage
- Summit official press releases

**Context**: This is the 28th Conference of the Parties (COP28), with focus on transitioning away from fossil fuels and increasing climate finance.

**Example 2: Business News Analysis**
User: "What's happening with tech stocks this week?"
Agent: I'll gather the latest information on technology sector performance and key developments.

**Tech Sector Weekly Recap** (Week of [date]):

**Market Performance**:
- **NASDAQ**: Down 2.3% for the week amid interest rate concerns
- **Major Stocks**: Apple -1.8%, Microsoft -2.1%, Google +0.5%, Amazon -3.2%
- **Semiconductor Sector**: Mixed results with NVIDIA +4% on AI optimism, Intel -5% on guidance cut

**Key News Stories**:
1. **AI Investment Surge**: Major cloud providers announced $50B in combined AI infrastructure spending
2. **Regulatory Pressure**: EU finalizes new data privacy regulations affecting U.S. tech giants
3. **Earnings Season**: Mixed results with strong cloud growth but advertising revenue concerns

**Analyst Commentary**:
- JPMorgan maintains overweight on mega-cap tech despite near-term volatility
- Morgan Stanley notes AI spending creating "investment supercycle"
- Goldman Sachs warns of potential oversupply in semiconductor market

**Looking Ahead**: Federal Reserve meeting next week could impact tech valuations given sector's sensitivity to interest rates.

**Sources**: Wall Street Journal, Financial Times, Bloomberg Technology, company earnings reports

You are now ready to provide current, balanced news reporting and analysis."""

DATA_ANALYST_AGENT_PROMPT = """## Core Expertise
You are a data analysis specialist focused on examining data, identifying patterns, creating insights, and presenting findings clearly.

## Core Identity
- Expert in data analysis techniques, statistical methods, and pattern recognition
- Proficient with data manipulation, cleaning, and visualization concepts
- Strong analytical thinking and problem-solving skills
- Ability to translate complex data findings into actionable insights
- Focus on accuracy, methodology transparency, and clear communication

## Tool Usage Guidelines

### CSV Tools
- Use for reading, analyzing, and manipulating structured data files
- Always examine data structure and quality before analysis
- Handle missing data appropriately and document any assumptions
- Perform data validation to check for inconsistencies or errors
- Create meaningful summaries and aggregations of data

### Calculator Tools
- Use for statistical calculations, percentages, averages, and other metrics
- Show calculations step-by-step for transparency
- Handle precision appropriately for the type of analysis
- Verify results using multiple approaches when possible

## Behavioral Guidelines
1. **Data Quality First**: Always examine and validate data before analysis
2. **Methodology Transparency**: Explain analytical approaches and assumptions clearly
3. **Statistical Rigor**: Use appropriate statistical methods and acknowledge limitations
4. **Visual Thinking**: Describe data patterns and recommend appropriate visualizations
5. **Actionable Insights**: Focus on findings that can inform decisions or actions
6. **Handle Missing Data**: Address gaps in data thoughtfully and document approaches
7. **Avoid Correlation/Causation Errors**: Be careful about inferring causation from correlation

## Decision Framework
When analyzing data:
1. **Understand Objectives**: Clarify what questions the analysis should answer
2. **Examine Data Quality**: Review data structure, completeness, and potential issues
3. **Choose Methods**: Select appropriate analytical techniques for the data and questions
4. **Perform Analysis**: Execute analysis systematically with proper validation
5. **Interpret Results**: Explain what the findings mean in practical terms
6. **Recommend Actions**: Suggest next steps or decisions based on insights

## Analysis Types
- **Descriptive Analysis**: Summarizing what happened in the data
- **Diagnostic Analysis**: Understanding why something happened
- **Exploratory Analysis**: Discovering patterns and relationships
- **Comparative Analysis**: Comparing groups, time periods, or conditions
- **Trend Analysis**: Identifying patterns over time

## Examples

**Example 1: Dataset Overview**
User: "Analyze this sales data CSV file"
Agent: I'll examine the sales data structure and provide a comprehensive analysis.

**Initial Data Examination**:
- **Records**: 2,847 sales transactions
- **Time Period**: January 2023 - December 2023
- **Columns**: Date, Product, Category, Quantity, Unit_Price, Total_Sale, Salesperson, Region

**Data Quality Check**:
- **Missing Values**: 23 records missing Region (0.8% of data)
- **Duplicates**: 5 duplicate transactions identified and flagged
- **Data Types**: All numeric fields properly formatted, dates validated

**Key Findings**:

**Sales Performance**:
- **Total Revenue**: $1,247,832 across all transactions
- **Average Sale**: $438.22 per transaction
- **Peak Month**: December ($156,890) - likely holiday season effect
- **Lowest Month**: February ($78,234)

**Product Analysis**:
- **Top Category**: Electronics (42% of revenue)
- **Best Performer**: Laptop Model X ($89,432 total sales)
- **Quantity Leaders**: Basic accessories (high volume, low margin)

**Regional Breakdown**:
- **West Region**: $412,890 (33.1% of total)
- **East Region**: $385,670 (30.9% of total)
- **Central Region**: $278,445 (22.3% of total)
- **South Region**: $170,827 (13.7% of total)

**Salesperson Performance**:
- **Top Performer**: Sarah Johnson ($127,445)
- **Most Consistent**: Mike Chen (lowest month-to-month variance)

**Recommendations**:
1. Investigate underperformance in South region
2. Analyze December strategies for replication in other months
3. Focus on high-margin electronics category expansion
4. Address missing region data for complete analysis

**Example 2: Trend Analysis**
User: "What trends do you see in our customer satisfaction scores over the past year?"
Agent: I'll analyze the customer satisfaction data to identify trends and patterns.

**Customer Satisfaction Trend Analysis**

**Dataset Overview**:
- **Survey Responses**: 1,456 over 12 months
- **Score Range**: 1-10 scale
- **Response Rate**: 23.4% average monthly response rate

**Overall Trends**:
- **Annual Average**: 7.2/10 (up from 6.8 previous year)
- **Trend Direction**: Generally upward with seasonal variations
- **Best Quarter**: Q4 2023 (7.8 average)
- **Challenging Period**: March-May 2023 (6.4-6.7 range)

**Key Patterns Identified**:

**Seasonal Variations**:
- **Holiday Season Boost**: December scores 15% above average
- **Spring Dip**: March-May consistently lower (possibly due to increased volume)
- **Summer Stability**: June-August most consistent scores

**Category Breakdown** (where available):
- **Product Quality**: 8.1/10 (highest rated aspect)
- **Customer Service**: 7.0/10 (room for improvement)
- **Delivery Experience**: 6.8/10 (lowest rated)

**Statistical Significance**:
- Q4 improvement statistically significant (p < 0.05)
- Month-to-month variations within normal range except for March dip

**Actionable Insights**:
1. **Delivery Focus**: Lowest scores suggest priority improvement area
2. **Seasonal Staffing**: March-May period needs additional support
3. **Holiday Success**: Analyze Q4 practices for year-round application
4. **Service Training**: Customer service scores have improvement potential

**Methodology Note**: Analysis based on numerical scores with 95% confidence intervals. Missing demographic data limits deeper segmentation analysis.

You are now ready to provide thorough, accurate data analysis with clear insights and recommendations."""

================================================================================
# FILE: src/mindroom/agents.py
================================================================================

"""Agent loader that reads agent configurations from YAML file."""

from __future__ import annotations

from datetime import datetime
from typing import TYPE_CHECKING
from zoneinfo import ZoneInfo

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.learn import LearningMachine, LearningMode, UserMemoryConfig, UserProfileConfig

from . import agent_prompts
from . import tools as _tools_module  # noqa: F401
from .constants import ROUTER_AGENT_NAME, STORAGE_PATH_OBJ
from .logging_config import get_logger
from .plugins import load_plugins
from .skills import build_agent_skills
from .tools_metadata import get_tool_by_name

if TYPE_CHECKING:
    from pathlib import Path

    from agno.knowledge.knowledge import Knowledge

    from .config import AgentConfig, Config, DefaultsConfig

logger = get_logger(__name__)

# Maximum length for instruction descriptions to include in agent summary
MAX_INSTRUCTION_LENGTH = 100


def get_datetime_context(timezone_str: str) -> str:
    """Generate current date and time context for the agent.

    Args:
        timezone_str: Timezone string (e.g., 'America/New_York', 'UTC')

    Returns:
        Formatted string with current date and time information

    """
    tz = ZoneInfo(timezone_str)
    now = datetime.now(tz)

    # Format the datetime in a clear, readable way
    date_str = now.strftime("%A, %B %d, %Y")
    time_str = now.strftime("%H:%M %Z")  # 24-hour format

    return f"""## Current Date and Time
Today is {date_str}.
The current time is {time_str} ({timezone_str} timezone).

"""


# Rich prompt mapping - agents that use detailed prompts instead of simple roles
RICH_PROMPTS = {
    "code": agent_prompts.CODE_AGENT_PROMPT,
    "research": agent_prompts.RESEARCH_AGENT_PROMPT,
    "calculator": agent_prompts.CALCULATOR_AGENT_PROMPT,
    "general": agent_prompts.GENERAL_AGENT_PROMPT,
    "shell": agent_prompts.SHELL_AGENT_PROMPT,
    "summary": agent_prompts.SUMMARY_AGENT_PROMPT,
    "finance": agent_prompts.FINANCE_AGENT_PROMPT,
    "news": agent_prompts.NEWS_AGENT_PROMPT,
    "data_analyst": agent_prompts.DATA_ANALYST_AGENT_PROMPT,
}


def is_learning_enabled(agent_config: AgentConfig, defaults: DefaultsConfig) -> bool:
    """Check if learning is enabled for an agent, falling back to defaults."""
    learning = agent_config.learning if agent_config.learning is not None else defaults.learning
    return learning is not False


def resolve_agent_learning(
    agent_config: AgentConfig,
    defaults: DefaultsConfig,
    learning_storage: SqliteDb | None = None,
) -> bool | LearningMachine:
    """Resolve Agent.learning setting from MindRoom agent configuration."""
    if not is_learning_enabled(agent_config, defaults):
        return False

    learning_mode = agent_config.learning_mode or defaults.learning_mode
    learning_mode_value = LearningMode.AGENTIC if learning_mode == "agentic" else LearningMode.ALWAYS

    return LearningMachine(
        db=learning_storage,
        user_profile=UserProfileConfig(mode=learning_mode_value),
        user_memory=UserMemoryConfig(mode=learning_mode_value),
    )


def create_session_storage(agent_name: str, storage_path: Path) -> SqliteDb:
    """Create persistent session storage for an agent."""
    sessions_dir = storage_path / "sessions"
    sessions_dir.mkdir(parents=True, exist_ok=True)
    return SqliteDb(session_table=f"{agent_name}_sessions", db_file=str(sessions_dir / f"{agent_name}.db"))


def create_learning_storage(agent_name: str, storage_path: Path) -> SqliteDb:
    """Create persistent learning storage for an agent."""
    learning_dir = storage_path / "learning"
    learning_dir.mkdir(parents=True, exist_ok=True)
    return SqliteDb(session_table=f"{agent_name}_learning_sessions", db_file=str(learning_dir / f"{agent_name}.db"))


def create_agent(
    agent_name: str,
    config: Config,
    *,
    storage_path: Path | None = None,
    knowledge: Knowledge | None = None,
) -> Agent:
    """Create an agent instance from configuration.

    Args:
        agent_name: Name of the agent to create
        config: Application configuration
        storage_path: Runtime storage path. Falls back to the
            module-level ``STORAGE_PATH_OBJ`` when *None*.
        knowledge: Optional shared knowledge base instance for RAG-enabled agents.

    Returns:
        Configured Agent instance

    Raises:
        ValueError: If agent_name is not found in configuration

    """
    from .ai import get_model_instance  # noqa: PLC0415

    resolved_storage_path = storage_path if storage_path is not None else STORAGE_PATH_OBJ

    # Use passed config (config_path is deprecated)
    agent_config = config.get_agent(agent_name)
    defaults = config.defaults

    load_plugins(config)

    tool_names = list(agent_config.tools)

    # Create tools
    tools: list = []  # Use list type to satisfy Agent's parameter type
    for tool_name in tool_names:
        try:
            if tool_name == "memory":
                from .custom_tools.memory import MemoryTools  # noqa: PLC0415

                tools.append(
                    MemoryTools(
                        agent_name=agent_name,
                        storage_path=resolved_storage_path,
                        config=config,
                    ),
                )
            else:
                tools.append(get_tool_by_name(tool_name))
        except ValueError as e:
            logger.warning(f"Could not load tool '{tool_name}' for agent '{agent_name}': {e}")

    storage = create_session_storage(agent_name, resolved_storage_path)
    learning_storage = (
        create_learning_storage(agent_name, resolved_storage_path)
        if is_learning_enabled(agent_config, defaults)
        else None
    )

    # Get model config for identity context
    model_name = agent_config.model or "default"
    if model_name in config.models:
        model_config = config.models[model_name]
        model_provider = model_config.provider.title()  # Capitalize provider name
        model_id = model_config.id
    else:
        # Fallback if model not found
        model_provider = "AI"
        model_id = model_name

    # Add identity context to all agents using the unified template
    identity_context = agent_prompts.AGENT_IDENTITY_CONTEXT.format(
        display_name=agent_config.display_name,
        agent_name=agent_name,
        model_provider=model_provider,
        model_id=model_id,
    )

    # Add current date and time context with user's configured timezone
    datetime_context = get_datetime_context(config.timezone)

    # Combine identity and datetime contexts
    full_context = identity_context + datetime_context

    # Use rich prompt if available, otherwise use YAML config
    if agent_name in RICH_PROMPTS:
        logger.info(f"Using rich prompt for agent: {agent_name}")
        # Prepend full context to the rich prompt
        role = full_context + RICH_PROMPTS[agent_name]
        instructions = []  # Instructions are in the rich prompt
    else:
        logger.info(f"Using YAML config for agent: {agent_name}")
        # For YAML agents, prepend full context to role and keep original instructions
        role = full_context + agent_config.role
        instructions = agent_config.instructions

    # Create agent with defaults applied
    model = get_model_instance(config, agent_config.model)
    logger.info(f"Creating agent '{agent_name}' with model: {model.__class__.__name__}(id={model.id})")

    skills = build_agent_skills(agent_name, config)
    if skills and skills.get_skill_names():
        instructions.append(agent_prompts.SKILLS_TOOL_USAGE_PROMPT)

    instructions.append(agent_prompts.INTERACTIVE_QUESTION_PROMPT)

    knowledge_enabled = agent_config.knowledge_base is not None and knowledge is not None

    agent = Agent(
        name=agent_config.display_name,
        role=role,
        model=model,
        tools=tools,
        skills=skills,
        instructions=instructions,
        db=storage,
        learning=resolve_agent_learning(agent_config, defaults, learning_storage),
        markdown=agent_config.markdown if agent_config.markdown is not None else defaults.markdown,
        knowledge=knowledge if knowledge_enabled else None,
        search_knowledge=knowledge_enabled,
    )
    logger.info(f"Created agent '{agent_name}' ({agent_config.display_name}) with {len(tools)} tools")

    return agent


def describe_agent(agent_name: str, config: Config) -> str:
    """Generate a description of an agent or team based on its configuration.

    Args:
        agent_name: Name of the agent or team to describe
        config: Application configuration

    Returns:
        Human-readable description of the agent or team

    """
    # Handle built-in router agent
    if agent_name == ROUTER_AGENT_NAME:
        return (
            "router\n"
            "  - Route messages to the most appropriate agent based on context and expertise.\n"
            "  - Analyzes incoming messages and determines which agent is best suited to respond."
        )

    # Check if it's a team
    if agent_name in config.teams:
        team_config = config.teams[agent_name]
        parts = [f"{agent_name}"]
        if team_config.role:
            parts.append(f"- {team_config.role}")
        parts.append(f"- Team of agents: {', '.join(team_config.agents)}")
        parts.append(f"- Collaboration mode: {team_config.mode}")
        return "\n  ".join(parts)

    # Check if agent exists
    if agent_name not in config.agents:
        return f"{agent_name}: Unknown agent or team"

    agent_config = config.agents[agent_name]

    # Start with agent name (not display name, for routing consistency)
    parts = [f"{agent_name}"]
    if agent_config.role:
        parts.append(f"- {agent_config.role}")

    # Add tools if any
    if agent_config.tools:
        tool_list = ", ".join(agent_config.tools)
        parts.append(f"- Tools: {tool_list}")

    # Add key instructions if any
    if agent_config.instructions:
        # Take first instruction as it's usually the most descriptive
        first_instruction = agent_config.instructions[0]
        if len(first_instruction) < MAX_INSTRUCTION_LENGTH:  # Only include if reasonably short
            parts.append(f"- {first_instruction}")

    return "\n  ".join(parts)


def get_agent_ids_for_room(room_key: str, config: Config) -> list[str]:
    """Get all agent Matrix IDs assigned to a specific room."""
    # Always include the router agent
    agent_ids = [config.ids[ROUTER_AGENT_NAME].full_id]

    # Add agents from config
    for agent_name, agent_cfg in config.agents.items():
        if room_key in agent_cfg.rooms:
            agent_ids.append(config.ids[agent_name].full_id)
    return agent_ids


def get_rooms_for_entity(entity_name: str, config: Config) -> list[str]:
    """Get the list of room aliases that an entity (agent/team) should be in.

    Args:
        entity_name: Name of the agent or team
        config: Configuration object

    Returns:
        List of room aliases the entity should be in

    """
    # TeamBot check (teams)
    if entity_name in config.teams:
        return config.teams[entity_name].rooms

    # Router agent special case - gets all rooms
    if entity_name == ROUTER_AGENT_NAME:
        return list(config.get_all_configured_rooms())

    # Regular agents
    if entity_name in config.agents:
        return config.agents[entity_name].rooms

    return []

================================================================================
# FILE: src/mindroom/ai.py
================================================================================

"""AI integration module for MindRoom agents and memory management."""

from __future__ import annotations

import functools
import os
from typing import TYPE_CHECKING, Any, cast

import diskcache
from agno.models.anthropic import Claude
from agno.models.cerebras import Cerebras
from agno.models.deepseek import DeepSeek
from agno.models.google import Gemini
from agno.models.groq import Groq
from agno.models.ollama import Ollama
from agno.models.openai import OpenAIChat
from agno.models.openrouter import OpenRouter
from agno.run.agent import RunContentEvent, RunOutput, ToolCallCompletedEvent, ToolCallStartedEvent

from .agents import create_agent
from .constants import ENABLE_AI_CACHE
from .credentials import get_credentials_manager
from .credentials_sync import get_api_key_for_provider, get_ollama_host
from .error_handling import get_user_friendly_error_message
from .logging_config import get_logger
from .memory import build_memory_enhanced_prompt
from .tool_events import (
    complete_pending_tool_block,
    extract_tool_completed_info,
    format_tool_combined,
    format_tool_started_event,
)

if TYPE_CHECKING:
    from collections.abc import AsyncIterator
    from pathlib import Path

    from agno.agent import Agent
    from agno.knowledge.knowledge import Knowledge
    from agno.models.base import Model

    from .config import Config, ModelConfig

logger = get_logger(__name__)

AIStreamChunk = str | RunContentEvent | ToolCallStartedEvent | ToolCallCompletedEvent


def _extract_response_content(response: RunOutput) -> str:
    response_parts = []

    # Add main content if present
    if response.content:
        response_parts.append(response.content)

    # Add formatted tool call sections when present.
    if response.tools:
        tool_sections: list[str] = []
        for tool in response.tools:
            tool_name = tool.tool_name or "tool"
            tool_args = tool.tool_args or {}
            combined, _ = format_tool_combined(tool_name, tool_args, tool.result)
            tool_sections.append(combined.strip())
        if tool_sections:
            response_parts.append("\n\n".join(tool_sections))

    return "\n".join(response_parts) if response_parts else ""


@functools.cache
def get_cache(storage_path: Path) -> diskcache.Cache | None:
    """Get or create a cache instance for the given storage path."""
    return diskcache.Cache(storage_path / ".ai_cache") if ENABLE_AI_CACHE else None


def _set_api_key_env_var(provider: str) -> None:
    """Set environment variable for a provider from CredentialsManager.

    Since we sync from .env to CredentialsManager on startup,
    this will always use the latest keys from .env.

    Args:
        provider: Provider name (e.g., 'openai', 'anthropic')

    """
    # Map provider names to environment variable names
    env_vars = {
        "openai": "OPENAI_API_KEY",
        "anthropic": "ANTHROPIC_API_KEY",
        "openrouter": "OPENROUTER_API_KEY",
        "gemini": "GOOGLE_API_KEY",
        "google": "GOOGLE_API_KEY",
        "cerebras": "CEREBRAS_API_KEY",
        "deepseek": "DEEPSEEK_API_KEY",
        "groq": "GROQ_API_KEY",
    }

    if provider not in env_vars:
        return

    # Get API key from CredentialsManager (which has been synced from .env)
    api_key = get_api_key_for_provider(provider)

    # Set environment variable if key exists
    if api_key:
        os.environ[env_vars[provider]] = api_key
        logger.debug(f"Set {env_vars[provider]} from CredentialsManager")


def _create_model_for_provider(provider: str, model_id: str, model_config: ModelConfig, extra_kwargs: dict) -> Model:
    """Create a model instance for a specific provider.

    Args:
        provider: The AI provider name
        model_id: The model identifier
        model_config: The model configuration object
        extra_kwargs: Additional keyword arguments for the model

    Returns:
        Instantiated model for the provider

    Raises:
        ValueError: If provider not supported

    """
    # Handle Ollama separately due to special host configuration
    if provider == "ollama":
        # Priority: model config > env/CredentialsManager > default
        # This allows per-model host configuration in config.yaml
        host = model_config.host or get_ollama_host() or "http://localhost:11434"
        logger.debug(f"Using Ollama host: {host}")
        return Ollama(id=model_id, host=host, **extra_kwargs)

    # Handle OpenRouter separately due to API key capture timing issue
    if provider == "openrouter":
        # OpenRouter needs the API key passed explicitly because it captures
        # the environment variable at import time, not at instantiation time
        api_key = extra_kwargs.pop("api_key", None) or get_api_key_for_provider(provider)
        if not api_key:
            logger.warning("No OpenRouter API key found in environment or CredentialsManager")
        return OpenRouter(id=model_id, api_key=api_key, **extra_kwargs)

    # Map providers to their model classes for simple instantiation
    provider_map: dict[str, type[Model]] = {
        "openai": OpenAIChat,
        "anthropic": Claude,
        "gemini": Gemini,
        "google": Gemini,
        "cerebras": Cerebras,
        "groq": Groq,
        "deepseek": DeepSeek,
    }

    model_class = provider_map.get(provider)
    if model_class is not None:
        return model_class(id=model_id, **extra_kwargs)

    msg = f"Unsupported AI provider: {provider}"
    raise ValueError(msg)


def get_model_instance(config: Config, model_name: str = "default") -> Model:
    """Get a model instance from config.yaml.

    Args:
        config: Application configuration
        model_name: Name of the model configuration to use (default: "default")

    Returns:
        Instantiated model

    Raises:
        ValueError: If model not found or provider not supported

    """
    if model_name not in config.models:
        available = ", ".join(sorted(config.models.keys()))
        msg = f"Unknown model: {model_name}. Available models: {available}"
        raise ValueError(msg)

    model_config = config.models[model_name]
    provider = model_config.provider
    model_id = model_config.id

    logger.info("Using AI model", model=model_name, provider=provider, id=model_id)

    # Get extra kwargs if specified
    extra_kwargs = dict(model_config.extra_kwargs or {})

    # Check for model-specific API key first, then fall back to provider-level
    creds_manager = get_credentials_manager()
    model_creds = creds_manager.load_credentials(f"model:{model_name}")
    model_api_key = model_creds.get("api_key") if model_creds else None

    if model_api_key:
        extra_kwargs["api_key"] = model_api_key
    else:
        # Set environment variable from CredentialsManager for Agno to use
        _set_api_key_env_var(provider)

    return _create_model_for_provider(provider, model_id, model_config, extra_kwargs)


def _build_full_prompt(prompt: str, thread_history: list[dict[str, Any]] | None = None) -> str:
    """Build full prompt with thread history context."""
    if not thread_history:
        return prompt

    context = "Previous conversation in this thread:\n"
    for msg in thread_history:
        context += f"{msg['sender']}: {msg['body']}\n"
    context += "\nCurrent message:\n"
    return context + prompt


def _build_cache_key(agent: Agent, full_prompt: str, session_id: str) -> str:
    model = agent.model
    assert model is not None
    return f"{agent.name}:{model.__class__.__name__}:{model.id}:{full_prompt}:{session_id}"


async def _cached_agent_run(
    agent: Agent,
    full_prompt: str,
    session_id: str,
    agent_name: str,
    storage_path: Path,
    user_id: str | None = None,
) -> RunOutput:
    """Cached wrapper for agent.arun() calls."""
    cache = get_cache(storage_path)
    if cache is None:
        return await agent.arun(full_prompt, session_id=session_id, user_id=user_id)

    model = agent.model
    assert model is not None
    cache_key = _build_cache_key(agent, full_prompt, session_id)
    cached_result = cache.get(cache_key)
    if cached_result is not None:
        logger.info("Cache hit", agent=agent_name)
        return cast("RunOutput", cached_result)

    response = await agent.arun(full_prompt, session_id=session_id, user_id=user_id)

    cache.set(cache_key, response)
    logger.info("Response cached", agent=agent_name)

    return response


async def _prepare_agent_and_prompt(
    agent_name: str,
    prompt: str,
    storage_path: Path,
    room_id: str | None,
    config: Config,
    thread_history: list[dict[str, Any]] | None = None,
    knowledge: Knowledge | None = None,
) -> tuple[Agent, str]:
    """Prepare agent and full prompt for AI processing.

    Returns:
        Tuple of (agent, full_prompt, session_id)

    """
    enhanced_prompt = await build_memory_enhanced_prompt(prompt, agent_name, storage_path, config, room_id)
    full_prompt = _build_full_prompt(enhanced_prompt, thread_history)
    logger.info("Preparing agent and prompt", agent=agent_name, full_prompt=full_prompt)
    agent = create_agent(agent_name, config, storage_path=storage_path, knowledge=knowledge)
    return agent, full_prompt


async def ai_response(
    agent_name: str,
    prompt: str,
    session_id: str,
    storage_path: Path,
    config: Config,
    thread_history: list[dict[str, Any]] | None = None,
    room_id: str | None = None,
    knowledge: Knowledge | None = None,
    user_id: str | None = None,
) -> str:
    """Generates a response using the specified agno Agent with memory integration.

    Args:
        agent_name: Name of the agent to use
        prompt: User prompt
        session_id: Session ID for conversation tracking
        storage_path: Path for storing agent data
        config: Application configuration
        thread_history: Optional thread history
        room_id: Optional room ID for room memory access
        knowledge: Optional shared knowledge base for RAG-enabled agents
        user_id: Matrix user ID of the sender, used by Agno's LearningMachine

    Returns:
        Agent response string

    """
    logger.info("AI request", agent=agent_name)

    # Prepare agent and prompt - this can fail if agent creation fails (e.g., missing API key)
    try:
        agent, full_prompt = await _prepare_agent_and_prompt(
            agent_name,
            prompt,
            storage_path,
            room_id,
            config,
            thread_history,
            knowledge,
        )
    except Exception as e:
        logger.exception("Error preparing agent", agent=agent_name)
        return get_user_friendly_error_message(e, agent_name)

    # Execute the AI call - this can fail for network, rate limits, etc.
    try:
        response = await _cached_agent_run(agent, full_prompt, session_id, agent_name, storage_path, user_id=user_id)
    except Exception as e:
        logger.exception("Error generating AI response", agent=agent_name)
        return get_user_friendly_error_message(e, agent_name)

    # Extract response content - this shouldn't fail
    return _extract_response_content(response)


async def stream_agent_response(  # noqa: C901, PLR0912
    agent_name: str,
    prompt: str,
    session_id: str,
    storage_path: Path,
    config: Config,
    thread_history: list[dict[str, Any]] | None = None,
    room_id: str | None = None,
    knowledge: Knowledge | None = None,
    user_id: str | None = None,
) -> AsyncIterator[AIStreamChunk]:
    """Generate streaming AI response using Agno's streaming API.

    Checks cache first - if found, yields the cached response immediately.
    Otherwise streams the new response and caches it.

    Args:
        agent_name: Name of the agent to use
        prompt: User prompt
        session_id: Session ID for conversation tracking
        storage_path: Path for storing agent data
        config: Application configuration
        thread_history: Optional thread history
        room_id: Optional room ID for room memory access
        knowledge: Optional shared knowledge base for RAG-enabled agents
        user_id: Matrix user ID of the sender, used by Agno's LearningMachine

    Yields:
        Streaming chunks/events as they become available

    """
    logger.info("AI streaming request", agent=agent_name)

    # Prepare agent and prompt - this can fail if agent creation fails
    try:
        agent, full_prompt = await _prepare_agent_and_prompt(
            agent_name,
            prompt,
            storage_path,
            room_id,
            config,
            thread_history,
            knowledge,
        )
    except Exception as e:
        logger.exception("Error preparing agent for streaming", agent=agent_name)
        yield get_user_friendly_error_message(e, agent_name)
        return

    # Check cache (this shouldn't fail)
    cache = get_cache(storage_path)
    if cache is not None:
        model = agent.model
        assert model is not None
        cache_key = _build_cache_key(agent, full_prompt, session_id)
        cached_result = cache.get(cache_key)
        if cached_result is not None:
            logger.info("Cache hit", agent=agent_name)
            response_text = cached_result.content or ""
            yield response_text
            return

    full_response = ""

    # Execute the streaming AI call - this can fail for network, rate limits, etc.
    try:
        stream_generator = agent.arun(
            full_prompt,
            session_id=session_id,
            user_id=user_id,
            stream=True,
            stream_events=True,
        )
    except Exception as e:
        logger.exception("Error starting streaming AI response")
        yield get_user_friendly_error_message(e, agent_name)
        return

    # Process the stream events
    try:
        async for event in stream_generator:
            if isinstance(event, RunContentEvent) and event.content:
                chunk_text = str(event.content)
                full_response += chunk_text
                yield event
            elif isinstance(event, ToolCallStartedEvent):
                tool_msg, _ = format_tool_started_event(event.tool)
                if tool_msg:
                    full_response += tool_msg
                    yield event
            elif isinstance(event, ToolCallCompletedEvent):
                info = extract_tool_completed_info(event.tool)
                if info:
                    tool_name, result = info
                    full_response, _ = complete_pending_tool_block(full_response, tool_name, result)
                    yield event
            else:
                logger.debug("Skipping stream event", event_type=type(event).__name__)
    except Exception as e:
        logger.exception("Error during streaming AI response")
        yield get_user_friendly_error_message(e, agent_name)
        return

    if cache is not None and full_response:
        cached_response = RunOutput(content=full_response)
        cache.set(cache_key, cached_response)
        logger.info("Response cached", agent=agent_name)

================================================================================
# FILE: src/mindroom/api/__init__.py
================================================================================

"""Backend initialization for the widget module."""

================================================================================
# FILE: src/mindroom/api/credentials.py
================================================================================

"""Unified credentials management API."""

from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from mindroom.credentials import get_credentials_manager

router = APIRouter(prefix="/api/credentials", tags=["credentials"])


def _filter_internal_keys(credentials: dict[str, Any]) -> dict[str, Any]:
    """Remove internal metadata keys (prefixed with _) from credentials."""
    return {k: v for k, v in credentials.items() if not k.startswith("_")}


class SetApiKeyRequest(BaseModel):
    """Request to set an API key."""

    service: str
    api_key: str
    key_name: str = "api_key"


class CredentialStatus(BaseModel):
    """Status of a service's credentials."""

    service: str
    has_credentials: bool
    key_names: list[str] | None = None


class SetCredentialsRequest(BaseModel):
    """Request to set multiple credentials for a service."""

    credentials: dict[str, Any]  # Can be strings, booleans, numbers, etc.


@router.get("/list")
async def list_services() -> list[str]:
    """List all services with stored credentials."""
    manager = get_credentials_manager()
    return manager.list_services()


@router.get("/{service}/status")
async def get_credential_status(service: str) -> CredentialStatus:
    """Get the status of credentials for a service."""
    manager = get_credentials_manager()
    credentials = manager.load_credentials(service)

    if credentials:
        filtered = _filter_internal_keys(credentials) if isinstance(credentials, dict) else {}
        return CredentialStatus(
            service=service,
            has_credentials=True,
            key_names=list(filtered.keys()) if filtered else None,
        )

    return CredentialStatus(service=service, has_credentials=False)


@router.post("/{service}")
async def set_credentials(service: str, request: SetCredentialsRequest) -> dict[str, str]:
    """Set multiple credentials for a service."""
    manager = get_credentials_manager()

    # Mark as UI-sourced and save
    creds = {**request.credentials, "_source": "ui"}
    manager.save_credentials(service, creds)

    return {"status": "success", "message": f"Credentials saved for {service}"}


@router.post("/{service}/api-key")
async def set_api_key(service: str, request: SetApiKeyRequest) -> dict[str, str]:
    """Set an API key for a service."""
    if request.service != service:
        raise HTTPException(status_code=400, detail="Service mismatch in request")

    manager = get_credentials_manager()
    credentials = manager.load_credentials(service) or {}
    credentials[request.key_name] = request.api_key
    credentials["_source"] = "ui"
    manager.save_credentials(service, credentials)

    return {"status": "success", "message": f"API key set for {service}"}


@router.get("/{service}/api-key")
async def get_api_key(
    service: str,
    key_name: str = "api_key",
    include_value: bool = False,
) -> dict[str, Any]:
    """Get API key metadata for a service, and optionally the full key value."""
    manager = get_credentials_manager()
    credentials = manager.load_credentials(service) or {}
    api_key = credentials.get(key_name)

    if api_key:
        source = credentials.get("_source")
        response = {
            "service": service,
            "has_key": True,
            "key_name": key_name,
            # Return masked version
            "masked_key": f"{api_key[:4]}...{api_key[-4:]}" if len(api_key) > 8 else "****",
            "source": source,
        }
        if include_value:
            response["api_key"] = api_key
        return response

    return {"service": service, "has_key": False, "key_name": key_name}


@router.get("/{service}")
async def get_credentials(service: str) -> dict[str, Any]:
    """Get credentials for a service (for editing)."""
    manager = get_credentials_manager()
    credentials = manager.load_credentials(service)

    if not credentials:
        return {"service": service, "credentials": {}}

    return {"service": service, "credentials": _filter_internal_keys(credentials)}


@router.delete("/{service}")
async def delete_credentials(service: str) -> dict[str, str]:
    """Delete all credentials for a service."""
    manager = get_credentials_manager()
    manager.delete_credentials(service)

    return {"status": "success", "message": f"Credentials deleted for {service}"}


@router.post("/{service}/copy-from/{source_service}")
async def copy_credentials(service: str, source_service: str) -> dict[str, str]:
    """Copy credentials from one service to another."""
    manager = get_credentials_manager()
    source_creds = manager.load_credentials(source_service)

    if not source_creds:
        raise HTTPException(status_code=404, detail=f"No credentials found for {source_service}")

    # Copy credentials, marking as UI-sourced
    target_creds = {k: v for k, v in source_creds.items() if not k.startswith("_")}
    target_creds["_source"] = "ui"
    manager.save_credentials(service, target_creds)

    return {"status": "success", "message": f"Credentials copied from {source_service} to {service}"}


@router.post("/{service}/test")
async def test_credentials(service: str) -> dict[str, Any]:
    """Test if credentials are valid for a service."""
    # This is a placeholder - actual testing would depend on the service
    manager = get_credentials_manager()
    credentials = manager.load_credentials(service)

    if not credentials:
        raise HTTPException(status_code=404, detail=f"No credentials found for {service}")

    # For now, just check if credentials exist
    # In the future, we could implement actual validation per service
    return {
        "service": service,
        "status": "success",
        "message": "Credentials exist (validation not implemented)",
    }

================================================================================
# FILE: src/mindroom/api/google_integration.py
================================================================================

"""Unified Google Integration for MindRoom.

This module provides a single, comprehensive Google OAuth integration supporting:
- Gmail (read, compose, modify)
- Google Calendar (events, scheduling)
- Google Drive (file access)

Replaces the previous fragmented gmail_config.py, google_auth.py, and google_setup_wizard.py
"""

import os
from pathlib import Path
from typing import Any
from urllib.parse import urlparse

import jwt
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import RedirectResponse
from google.auth.transport.requests import Request as GoogleRequest
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow  # type: ignore[import-untyped]
from pydantic import BaseModel

from mindroom.credentials import CredentialsManager

router = APIRouter(prefix="/api/google", tags=["google-integration"])

# Initialize credentials manager
creds_manager = CredentialsManager()

# OAuth scopes for all Google services needed by MindRoom
SCOPES = [
    # Gmail
    "https://www.googleapis.com/auth/gmail.readonly",
    "https://www.googleapis.com/auth/gmail.modify",
    "https://www.googleapis.com/auth/gmail.compose",
    # Calendar
    "https://www.googleapis.com/auth/calendar",
    # Sheets
    "https://www.googleapis.com/auth/spreadsheets",
    # Drive
    "https://www.googleapis.com/auth/drive.file",
    # User info
    "openid",
    "https://www.googleapis.com/auth/userinfo.email",
    "https://www.googleapis.com/auth/userinfo.profile",
]

# Environment path for OAuth credentials
ENV_PATH = Path(__file__).parent.parent.parent.parent.parent / ".env"

# Get configuration from environment
BACKEND_PORT = os.getenv("BACKEND_PORT", "8765")
REDIRECT_URI = os.getenv("GOOGLE_REDIRECT_URI", f"http://localhost:{BACKEND_PORT}/api/google/callback")


class GoogleStatus(BaseModel):
    """Google integration status."""

    connected: bool
    email: str | None = None
    services: list[str] = []
    error: str | None = None
    has_credentials: bool = False


class GoogleAuthUrl(BaseModel):
    """Google OAuth URL response."""

    auth_url: str


def get_oauth_credentials() -> dict[str, Any] | None:
    """Get OAuth credentials from environment variables."""
    client_id = os.getenv("GOOGLE_CLIENT_ID")
    client_secret = os.getenv("GOOGLE_CLIENT_SECRET")

    if not client_id or not client_secret:
        return None

    return {
        "web": {
            "client_id": client_id,
            "client_secret": client_secret,
            "auth_uri": "https://accounts.google.com/o/oauth2/auth",
            "token_uri": "https://oauth2.googleapis.com/token",
            "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
            "redirect_uris": [REDIRECT_URI],
        },
    }


def get_google_credentials() -> Credentials | None:
    """Get Google credentials from stored token."""
    token_data = creds_manager.load_credentials("google")
    if not token_data:
        return None

    try:
        creds = Credentials(
            token=token_data.get("token"),
            refresh_token=token_data.get("refresh_token"),
            token_uri=token_data.get("token_uri"),
            client_id=token_data.get("client_id"),
            client_secret=token_data.get("client_secret"),
            scopes=token_data.get("scopes", SCOPES),
        )

        # Refresh token if expired
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(GoogleRequest())
            # Save refreshed credentials
            save_credentials(creds)
    except Exception:
        return None
    else:
        return creds if creds and creds.valid else None


def save_credentials(creds: Credentials) -> None:
    """Save credentials using the unified credentials manager."""
    # Full token with all scopes
    token_data = {
        "token": creds.token,
        "refresh_token": creds.refresh_token,
        "token_uri": creds.token_uri,
        "client_id": creds.client_id,
        "client_secret": creds.client_secret,
        "scopes": creds.scopes,
    }

    # Add ID token if available for user info
    if hasattr(creds, "_id_token") and creds._id_token:
        token_data["_id_token"] = creds._id_token

    # Save using credentials manager (handles backward compatibility)
    creds_manager.save_credentials("google", token_data)


def save_env_credentials(client_id: str, client_secret: str, project_id: str | None = None) -> None:
    """Save OAuth credentials to .env file."""
    env_lines = []
    if ENV_PATH.exists():
        with ENV_PATH.open() as f:
            env_lines = f.readlines()

    # Update or add credentials
    # Use current environment variable for redirect URI to support multiple deployments
    current_redirect_uri = os.getenv("GOOGLE_REDIRECT_URI", REDIRECT_URI)
    env_vars = {
        "GOOGLE_CLIENT_ID": client_id,
        "GOOGLE_CLIENT_SECRET": client_secret,
        "GOOGLE_PROJECT_ID": project_id or "mindroom-integration",
        "GOOGLE_REDIRECT_URI": current_redirect_uri,
        "BACKEND_PORT": BACKEND_PORT,
    }

    for key, value in env_vars.items():
        found = False
        for i, line in enumerate(env_lines):
            if line.startswith(f"{key}="):
                env_lines[i] = f"{key}={value}\n"
                found = True
                break
        if not found:
            env_lines.append(f"{key}={value}\n")

    # Write back to .env file
    with ENV_PATH.open("w") as f:
        f.writelines(env_lines)

    # Also set in current environment
    for key, value in env_vars.items():
        os.environ[key] = value


@router.get("/status")
async def get_status() -> GoogleStatus:
    """Check Google integration status."""
    # Check environment variables
    client_id = os.getenv("GOOGLE_CLIENT_ID")
    client_secret = os.getenv("GOOGLE_CLIENT_SECRET")
    has_credentials = bool(client_id and client_secret)

    # Get current credentials
    creds = get_google_credentials()

    if not creds:
        return GoogleStatus(
            connected=False,
            has_credentials=has_credentials,
        )

    try:
        # Check which services are accessible based on scopes
        services = []
        if creds.has_scopes(["https://www.googleapis.com/auth/gmail.modify"]):
            services.append("Gmail")
        if creds.has_scopes(["https://www.googleapis.com/auth/calendar"]):
            services.append("Google Calendar")
        if creds.has_scopes(["https://www.googleapis.com/auth/spreadsheets"]):
            services.append("Google Sheets")
        if creds.has_scopes(["https://www.googleapis.com/auth/drive.file"]):
            services.append("Google Drive")

        # Get user email from token
        email = None
        try:
            if hasattr(creds, "_id_token") and creds._id_token:
                decoded = jwt.decode(creds._id_token, options={"verify_signature": False})
                email = decoded.get("email")
        except Exception:
            email = None

        return GoogleStatus(
            connected=True,
            email=email,
            services=services,
            has_credentials=has_credentials,
        )
    except Exception as e:
        return GoogleStatus(
            connected=False,
            error=str(e),
            has_credentials=has_credentials,
        )


@router.post("/connect")
async def connect() -> GoogleAuthUrl:
    """Start Google OAuth flow."""
    oauth_config = get_oauth_credentials()
    if not oauth_config:
        raise HTTPException(
            status_code=503,
            detail="Google OAuth is not configured. Please set GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET environment variables.",
        )

    try:
        # Create OAuth flow with all scopes
        # Use current environment variable for redirect URI to support multiple deployments
        current_redirect_uri = os.getenv("GOOGLE_REDIRECT_URI", REDIRECT_URI)
        flow = Flow.from_client_config(oauth_config, scopes=SCOPES, redirect_uri=current_redirect_uri)

        # Generate authorization URL
        auth_url, _ = flow.authorization_url(
            access_type="offline",
            include_granted_scopes="true",
            prompt="consent",
        )

        return GoogleAuthUrl(auth_url=auth_url)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start Google OAuth: {e!s}") from e


@router.get("/callback")
async def callback(request: Request) -> RedirectResponse:
    """Handle Google OAuth callback."""
    # Get the authorization code from the callback
    code = request.query_params.get("code")
    if not code:
        raise HTTPException(status_code=400, detail="No authorization code received")

    oauth_config = get_oauth_credentials()
    if not oauth_config:
        raise HTTPException(status_code=503, detail="OAuth not configured")

    try:
        # Create OAuth flow and exchange code for tokens
        # Use current environment variable for redirect URI to support multiple deployments
        current_redirect_uri = os.getenv("GOOGLE_REDIRECT_URI", REDIRECT_URI)
        flow = Flow.from_client_config(oauth_config, scopes=SCOPES, redirect_uri=current_redirect_uri)
        flow.fetch_token(code=code)

        # Save credentials
        save_credentials(flow.credentials)

        # Redirect back to widget with success message
        # Extract the domain from the redirect URI for the final redirect
        parsed_uri = urlparse(current_redirect_uri)
        base_url = f"{parsed_uri.scheme}://{parsed_uri.netloc}"
        return RedirectResponse(url=f"{base_url}/?google=connected")
    except Exception as e:
        # Check if it's a scope change error
        error_msg = str(e)
        if "Scope has changed" in error_msg:
            raise HTTPException(
                status_code=400,
                detail=f"OAuth scope mismatch: {error_msg}. Please disconnect and reconnect to authorize with the new scopes.",
            ) from e
        raise HTTPException(status_code=500, detail=f"OAuth callback failed: {error_msg}") from e


@router.post("/disconnect")
async def disconnect() -> dict[str, str]:
    """Disconnect Google services by removing stored tokens."""
    try:
        # Remove credentials using the manager
        creds_manager.delete_credentials("google")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to disconnect: {e!s}") from e
    else:
        return {"status": "disconnected"}


@router.post("/configure")
async def configure(credentials: dict[str, str]) -> dict[str, Any]:
    """Configure Google OAuth credentials manually."""
    client_id = credentials.get("client_id")
    client_secret = credentials.get("client_secret")
    project_id = credentials.get("project_id", "mindroom-integration")

    if not client_id or not client_secret:
        raise HTTPException(
            status_code=400,
            detail="client_id and client_secret are required",
        )

    try:
        # Save to environment
        save_env_credentials(client_id, client_secret, project_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save credentials: {e!s}") from e
    else:
        return {"success": True, "message": "Google OAuth credentials configured successfully"}


@router.post("/reset")
async def reset() -> dict[str, Any]:
    """Reset Google integration by removing all credentials and tokens."""
    try:
        # Remove credentials using the manager
        creds_manager.delete_credentials("google")

        # Remove from environment variables
        if ENV_PATH.exists():
            with ENV_PATH.open() as f:
                lines = f.readlines()

            # Filter out Google-related variables
            google_vars = [
                "GOOGLE_CLIENT_ID",
                "GOOGLE_CLIENT_SECRET",
                "GOOGLE_PROJECT_ID",
                "GOOGLE_REDIRECT_URI",
            ]
            filtered_lines = [line for line in lines if not any(line.startswith(f"{var}=") for var in google_vars)]

            with ENV_PATH.open("w") as f:
                f.writelines(filtered_lines)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to reset: {e!s}") from e
    else:
        return {"success": True, "message": "Google integration reset successfully"}

================================================================================
# FILE: src/mindroom/api/google_tools_helper.py
================================================================================

"""Helper utilities for Google tools management."""

from typing import Any


def get_google_tool_scopes(tool_name: str) -> list[str]:
    """Get required OAuth scopes for a Google tool."""
    scope_map = {
        "google_calendar": [
            "https://www.googleapis.com/auth/calendar",
            "https://www.googleapis.com/auth/calendar.readonly",
        ],
        "google_sheets": [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/spreadsheets.readonly",
        ],
        "gmail": [
            "https://www.googleapis.com/auth/gmail.modify",
            "https://www.googleapis.com/auth/gmail.readonly",
            "https://www.googleapis.com/auth/gmail.compose",
            "https://www.googleapis.com/auth/gmail.send",
        ],
    }

    return scope_map.get(tool_name, [])


def check_google_tool_configured(tool_name: str, google_creds: dict[str, Any]) -> bool:
    """Check if a Google tool has the required OAuth scopes configured."""
    if not google_creds or "token" not in google_creds:
        return False

    configured_scopes = google_creds.get("scopes", [])
    required_scopes = get_google_tool_scopes(tool_name)

    if not required_scopes:
        return False

    # Check if any of the required scopes are present
    return any(scope in configured_scopes for scope in required_scopes)

================================================================================
# FILE: src/mindroom/api/homeassistant_integration.py
================================================================================

"""Home Assistant Integration for MindRoom.

This module provides OAuth2 integration with Home Assistant, supporting:
- Device control (lights, switches, climate, etc.)
- State monitoring (sensors, binary sensors)
- Scene activation
- Service calls
- Automation triggers

Uses the official Home Assistant REST API.
"""

import os
from typing import Any
from urllib.parse import urljoin

import httpx
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import RedirectResponse
from pydantic import BaseModel

from mindroom.credentials import CredentialsManager

router = APIRouter(prefix="/api/homeassistant", tags=["homeassistant-integration"])

# Initialize credentials manager
creds_manager = CredentialsManager()

# OAuth scopes for Home Assistant
# Home Assistant doesn't use traditional OAuth scopes, but we request full API access
SCOPES: list[str] = []

# Get configuration from environment
BACKEND_PORT = os.getenv("BACKEND_PORT", "8765")
FRONTEND_URL = os.getenv("FRONTEND_URL", "http://localhost:5173")


class HomeAssistantStatus(BaseModel):
    """Home Assistant integration status."""

    connected: bool
    instance_url: str | None = None
    version: str | None = None
    location_name: str | None = None
    error: str | None = None
    has_credentials: bool = False
    entities_count: int = 0


class HomeAssistantAuthUrl(BaseModel):
    """Home Assistant OAuth URL response."""

    auth_url: str


class HomeAssistantConfig(BaseModel):
    """Home Assistant configuration."""

    instance_url: str
    client_id: str | None = None
    long_lived_token: str | None = None


def get_stored_config() -> dict[str, Any] | None:
    """Get stored Home Assistant configuration."""
    return creds_manager.load_credentials("homeassistant")


def save_config(config: dict[str, Any]) -> None:
    """Save Home Assistant configuration."""
    creds_manager.save_credentials("homeassistant", config)


async def test_connection(instance_url: str, token: str) -> dict[str, Any]:
    """Test connection to Home Assistant."""
    async with httpx.AsyncClient() as client:
        try:
            # Test API connection
            response = await client.get(
                urljoin(instance_url, "/api/"),
                headers={"Authorization": f"Bearer {token}"},
                timeout=10.0,
            )

            if response.status_code == 401:
                raise HTTPException(status_code=401, detail="Invalid authentication token")
            if response.status_code != 200:
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to connect to Home Assistant: {response.text}",
                )

            api_info = response.json()

            # Get config for more details
            config_response = await client.get(
                urljoin(instance_url, "/api/config"),
                headers={"Authorization": f"Bearer {token}"},
                timeout=10.0,
            )

            config_info = config_response.json() if config_response.status_code == 200 else {}

            # Get states to count entities
            states_response = await client.get(
                urljoin(instance_url, "/api/states"),
                headers={"Authorization": f"Bearer {token}"},
                timeout=10.0,
            )

            entities = states_response.json() if states_response.status_code == 200 else []

            return {
                "message": api_info.get("message", "API running"),
                "version": config_info.get("version", "unknown"),
                "location_name": config_info.get("location_name", "Home"),
                "entities_count": len(entities),
            }

        except httpx.TimeoutException as e:
            raise HTTPException(
                status_code=504,
                detail="Connection timeout - check if the URL is correct and accessible",
            ) from e
        except httpx.RequestError as e:
            raise HTTPException(
                status_code=503,
                detail=f"Connection error: {e!s}",
            ) from e


@router.get("/status")
async def get_status() -> HomeAssistantStatus:
    """Check Home Assistant integration status."""
    config = get_stored_config()

    if not config:
        return HomeAssistantStatus(
            connected=False,
            has_credentials=False,
        )

    try:
        # Test the connection
        instance_url = config.get("instance_url")
        token = config.get("access_token") or config.get("long_lived_token")

        if not instance_url or not token:
            return HomeAssistantStatus(
                connected=False,
                has_credentials=True,
                error="Missing instance URL or token",
            )

        info = await test_connection(instance_url, token)

        return HomeAssistantStatus(
            connected=True,
            instance_url=instance_url,
            version=info.get("version"),
            location_name=info.get("location_name"),
            has_credentials=True,
            entities_count=info.get("entities_count", 0),
        )

    except HTTPException as e:
        return HomeAssistantStatus(
            connected=False,
            has_credentials=True,
            error=e.detail,
        )
    except Exception as e:
        return HomeAssistantStatus(
            connected=False,
            has_credentials=True,
            error=str(e),
        )


@router.post("/connect/oauth")
async def connect_oauth(config: HomeAssistantConfig) -> HomeAssistantAuthUrl:
    """Start Home Assistant OAuth flow."""
    if not config.instance_url:
        raise HTTPException(
            status_code=400,
            detail="Home Assistant instance URL is required",
        )

    if not config.client_id:
        raise HTTPException(
            status_code=400,
            detail="OAuth Client ID is required for OAuth flow",
        )

    # Build OAuth authorization URL
    # Home Assistant OAuth2 flow: https://developers.home-assistant.io/docs/auth_api/
    redirect_uri = f"{FRONTEND_URL}/homeassistant-callback"

    auth_params = {
        "client_id": config.client_id,
        "redirect_uri": redirect_uri,
        "response_type": "code",
    }

    # Build query string
    query_string = "&".join(f"{k}={v}" for k, v in auth_params.items())
    auth_url = f"{config.instance_url}/auth/authorize?{query_string}"

    # Store config for callback
    temp_config = {
        "instance_url": config.instance_url,
        "client_id": config.client_id,
        "redirect_uri": redirect_uri,
    }
    save_config(temp_config)

    return HomeAssistantAuthUrl(auth_url=auth_url)


@router.post("/connect/token")
async def connect_token(config: HomeAssistantConfig) -> dict[str, str]:
    """Connect using a long-lived access token."""
    if not config.instance_url:
        raise HTTPException(
            status_code=400,
            detail="Home Assistant instance URL is required",
        )

    if not config.long_lived_token:
        raise HTTPException(
            status_code=400,
            detail="Long-lived access token is required",
        )

    # Normalize the instance URL
    instance_url = config.instance_url.rstrip("/")
    if not instance_url.startswith(("http://", "https://")):
        instance_url = f"http://{instance_url}"

    # Test the connection
    try:
        await test_connection(instance_url, config.long_lived_token)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail=f"Failed to connect to Home Assistant: {e!s}",
        ) from e

    # Save configuration
    save_config(
        {
            "instance_url": instance_url,
            "long_lived_token": config.long_lived_token,
        },
    )

    return {"status": "connected", "message": "Successfully connected to Home Assistant"}


@router.get("/callback")
async def callback(request: Request) -> RedirectResponse:
    """Handle Home Assistant OAuth callback."""
    # Get the authorization code from the callback
    code = request.query_params.get("code")
    if not code:
        raise HTTPException(status_code=400, detail="No authorization code received")

    # Get stored config
    config = get_stored_config()
    if not config:
        raise HTTPException(status_code=503, detail="No configuration found")

    instance_url = config.get("instance_url")
    client_id = config.get("client_id")

    if not all([instance_url, client_id]) or not isinstance(instance_url, str):
        raise HTTPException(status_code=503, detail="Incomplete configuration")

    try:
        # Exchange code for access token
        async with httpx.AsyncClient() as client:
            token_response = await client.post(
                urljoin(instance_url, "/auth/token"),
                data={
                    "grant_type": "authorization_code",
                    "code": code,
                    "client_id": client_id,
                },
                timeout=10.0,
            )

            if token_response.status_code != 200:
                raise HTTPException(
                    status_code=token_response.status_code,
                    detail=f"Failed to get access token: {token_response.text}",
                )

            token_data = token_response.json()

            # Save the access token
            save_config(
                {
                    "instance_url": instance_url,
                    "client_id": client_id,
                    "access_token": token_data.get("access_token"),
                    "refresh_token": token_data.get("refresh_token"),
                    "expires_in": token_data.get("expires_in"),
                },
            )

            # Redirect back to widget with success message
            return RedirectResponse(url=f"{FRONTEND_URL}/?homeassistant=connected")

    except httpx.RequestError as e:
        raise HTTPException(status_code=503, detail=f"Failed to exchange code: {e!s}") from e


@router.post("/disconnect")
async def disconnect() -> dict[str, str]:
    """Disconnect Home Assistant by removing stored tokens."""
    try:
        # Remove credentials using the manager
        creds_manager.delete_credentials("homeassistant")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to disconnect: {e!s}") from e
    else:
        return {"status": "disconnected"}


@router.get("/entities")
async def get_entities(domain: str | None = None) -> list[dict[str, Any]]:
    """Get Home Assistant entities."""
    config = get_stored_config()
    if not config:
        raise HTTPException(status_code=401, detail="Not connected to Home Assistant")

    instance_url = config.get("instance_url")
    token = config.get("access_token") or config.get("long_lived_token")

    if not instance_url or not token:
        raise HTTPException(status_code=401, detail="Missing credentials")

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                urljoin(instance_url, "/api/states"),
                headers={"Authorization": f"Bearer {token}"},
                timeout=10.0,
            )

            if response.status_code != 200:
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to get entities: {response.text}",
                )

            entities = response.json()

            # Filter by domain if specified
            if domain:
                entities = [e for e in entities if e["entity_id"].startswith(f"{domain}.")]

            # Simplify the response
            return [
                {
                    "entity_id": e["entity_id"],
                    "state": e["state"],
                    "attributes": e.get("attributes", {}),
                    "last_changed": e.get("last_changed"),
                }
                for e in entities
            ]

    except httpx.RequestError as e:
        raise HTTPException(status_code=503, detail=f"Failed to get entities: {e!s}") from e


@router.post("/service")
async def call_service(
    domain: str,
    service: str,
    entity_id: str | None = None,
    data: dict[str, Any] | None = None,
) -> dict[str, Any]:
    """Call a Home Assistant service."""
    config = get_stored_config()
    if not config:
        raise HTTPException(status_code=401, detail="Not connected to Home Assistant")

    instance_url = config.get("instance_url")
    token = config.get("access_token") or config.get("long_lived_token")

    if not instance_url or not token:
        raise HTTPException(status_code=401, detail="Missing credentials")

    # Build service data
    service_data = data or {}
    if entity_id:
        service_data["entity_id"] = entity_id

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                urljoin(instance_url, f"/api/services/{domain}/{service}"),
                headers={"Authorization": f"Bearer {token}"},
                json=service_data,
                timeout=10.0,
            )

            if response.status_code not in (200, 201):
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Failed to call service: {response.text}",
                )

            return {"success": True, "message": f"Service {domain}.{service} called successfully"}

    except httpx.RequestError as e:
        raise HTTPException(status_code=503, detail=f"Failed to call service: {e!s}") from e

================================================================================
# FILE: src/mindroom/api/integrations.py
================================================================================

"""Third-party service integrations API."""

import os
from typing import Any

from fastapi import APIRouter, HTTPException
from fastapi.responses import RedirectResponse
from pydantic import BaseModel
from spotipy import Spotify, SpotifyOAuth  # type: ignore[import-untyped]

from mindroom.credentials import CredentialsManager
from mindroom.tools_metadata import ensure_tool_registry_loaded, export_tools_metadata

router = APIRouter(prefix="/api/integrations", tags=["integrations"])

# Initialize credentials manager
creds_manager = CredentialsManager()


# Load tool metadata from the single source of truth
def get_tools_metadata() -> dict[str, Any]:
    """Load tool metadata from the in-memory registry."""
    from mindroom.api.main import load_runtime_config  # noqa: PLC0415

    config, config_path = load_runtime_config()
    ensure_tool_registry_loaded(config, config_path=config_path)
    tools = export_tools_metadata()
    return {tool["name"]: tool for tool in tools}


class ServiceStatus(BaseModel):
    """Service connection status."""

    service: str
    connected: bool
    display_name: str
    icon: str
    category: str
    requires_oauth: bool
    requires_api_key: bool
    details: dict[str, Any] | None = None
    error: str | None = None


class ApiKeyRequest(BaseModel):
    """API key configuration request."""

    service: str
    api_key: str
    api_secret: str | None = None


def get_service_credentials(service: str) -> dict[str, Any]:
    """Get stored credentials for a service."""
    credentials = creds_manager.load_credentials(service)
    return credentials if credentials else {}


def save_service_credentials(service: str, credentials: dict[str, Any]) -> None:
    """Save service credentials."""
    creds_manager.save_credentials(service, credentials)


@router.get("/{service}/status")
async def get_service_status(service: str) -> ServiceStatus:
    """Get connection status for a specific service."""
    # Get tool metadata from single source of truth
    tools_metadata = get_tools_metadata()

    if service not in tools_metadata:
        raise HTTPException(status_code=404, detail=f"Unknown service: {service}")

    tool = tools_metadata[service]
    status = ServiceStatus(
        service=service,
        connected=False,
        display_name=tool.get("display_name", service),
        icon=tool.get("icon", "ðŸ“¦"),
        category=tool.get("category", "other"),
        requires_oauth=tool.get("setup_type") == "oauth",
        requires_api_key=tool.get("setup_type") == "api_key",
    )

    creds = get_service_credentials(service)
    if creds:
        if service == "spotify":
            status.connected = "access_token" in creds
            if status.connected:
                try:
                    # Try to get user info
                    sp = Spotify(auth=creds["access_token"])
                    user = sp.current_user()
                    status.details = {
                        "username": user["display_name"],
                        "email": user.get("email"),
                        "product": user.get("product"),
                    }
                except Exception as e:
                    status.connected = False
                    status.error = str(e)
        else:
            status.connected = "api_key" in creds

    return status


# Spotify
@router.post("/spotify/connect")
async def connect_spotify() -> dict[str, str]:
    """Start Spotify OAuth flow."""
    client_id = os.getenv("SPOTIFY_CLIENT_ID")
    client_secret = os.getenv("SPOTIFY_CLIENT_SECRET")

    if not client_id or not client_secret:
        raise HTTPException(
            status_code=500,
            detail="Spotify OAuth not configured. Set SPOTIFY_CLIENT_ID and SPOTIFY_CLIENT_SECRET environment variables.",
        )

    sp_oauth = SpotifyOAuth(
        client_id=client_id,
        client_secret=client_secret,
        redirect_uri="http://localhost:8000/api/integrations/spotify/callback",
        scope="user-read-private user-read-email user-read-playback-state user-read-currently-playing user-top-read",
    )

    auth_url = sp_oauth.get_authorize_url()
    return {"auth_url": auth_url}


@router.get("/spotify/callback")
async def spotify_callback(code: str) -> RedirectResponse:
    """Handle Spotify OAuth callback."""
    client_id = os.getenv("SPOTIFY_CLIENT_ID")
    client_secret = os.getenv("SPOTIFY_CLIENT_SECRET")

    if not client_id or not client_secret:
        raise HTTPException(status_code=500, detail="Spotify OAuth not configured")

    try:
        sp_oauth = SpotifyOAuth(
            client_id=client_id,
            client_secret=client_secret,
            redirect_uri="http://localhost:8000/api/integrations/spotify/callback",
        )

        token_info = sp_oauth.get_access_token(code)

        # Get user info
        sp = Spotify(auth=token_info["access_token"])
        user = sp.current_user()

        # Save credentials
        credentials = {
            "access_token": token_info["access_token"],
            "refresh_token": token_info.get("refresh_token"),
            "expires_at": token_info.get("expires_at"),
            "username": user["display_name"],
        }
        save_service_credentials("spotify", credentials)

        # Redirect back to widget
        return RedirectResponse(url="http://localhost:5173/?spotify=connected")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"OAuth failed: {e!s}") from e


@router.post("/{service}/disconnect")
async def disconnect_service(service: str) -> dict[str, str]:
    """Disconnect a service by removing stored credentials."""
    # Get tool metadata from single source of truth
    tools_metadata = get_tools_metadata()

    if service not in tools_metadata:
        raise HTTPException(status_code=404, detail=f"Unknown service: {service}")

    # Delete credentials using the manager
    creds_manager.delete_credentials(service)

    return {"status": "disconnected"}

================================================================================
# FILE: src/mindroom/api/knowledge.py
================================================================================

"""Knowledge base management API."""

from __future__ import annotations

from datetime import UTC, datetime
from pathlib import Path
from typing import Annotated, Any
from urllib.parse import unquote

from fastapi import APIRouter, File, HTTPException, UploadFile

from mindroom.config import Config
from mindroom.constants import STORAGE_PATH_OBJ
from mindroom.knowledge import (
    KnowledgeManager,
    get_knowledge_manager,
    initialize_knowledge_managers,
)

router = APIRouter(prefix="/api/knowledge", tags=["knowledge"])

_MAX_UPLOAD_BYTES = 1024 * 1024 * 1024  # 1 GiB
_UPLOAD_CHUNK_BYTES = 1024 * 1024  # 1 MiB


def _ensure_base_exists(config: Config, base_id: str) -> None:
    if base_id not in config.knowledge_bases:
        raise HTTPException(status_code=404, detail=f"Knowledge base '{base_id}' not found")


def _knowledge_root(config: Config, base_id: str, *, create: bool = False) -> Path:
    _ensure_base_exists(config, base_id)
    root = Path(config.knowledge_bases[base_id].path).expanduser().resolve()
    if create:
        root.mkdir(parents=True, exist_ok=True)
    return root


def _resolve_within_root(root: Path, relative_path: str) -> Path:
    candidate = Path(relative_path)
    if candidate.is_absolute() or ".." in candidate.parts:
        raise HTTPException(status_code=400, detail="Invalid path")

    resolved = (root / candidate).resolve()
    try:
        resolved.relative_to(root)
    except ValueError as exc:
        raise HTTPException(status_code=400, detail="Path is outside the knowledge folder") from exc
    return resolved


def _list_file_info(root: Path) -> tuple[list[dict[str, Any]], int]:
    files: list[dict[str, Any]] = []
    total_size = 0

    if not root.is_dir():
        return files, total_size

    for file_path in sorted(path for path in root.rglob("*") if path.is_file()):
        stat = file_path.stat()
        total_size += stat.st_size
        file_type = file_path.suffix.lstrip(".").lower() if file_path.suffix else "file"
        files.append(
            {
                "name": file_path.name,
                "path": file_path.relative_to(root).as_posix(),
                "size": stat.st_size,
                "modified": datetime.fromtimestamp(stat.st_mtime, tz=UTC).isoformat(),
                "type": file_type,
            },
        )

    return files, total_size


async def _ensure_managers(config: Config) -> dict[str, KnowledgeManager]:
    return await initialize_knowledge_managers(
        config,
        STORAGE_PATH_OBJ,
        start_watchers=False,
        reindex_on_create=False,
    )


async def _ensure_manager(config: Config, base_id: str) -> KnowledgeManager | None:
    existing = get_knowledge_manager(base_id)
    if existing is not None and existing.matches(config, STORAGE_PATH_OBJ):
        return existing
    managers = await _ensure_managers(config)
    return managers.get(base_id)


def _rollback_uploaded_files(uploaded_paths: list[Path]) -> None:
    for uploaded_path in uploaded_paths:
        uploaded_path.unlink(missing_ok=True)


def _validate_upload_size_hint(upload: UploadFile, filename: str) -> None:
    if not upload.file.seekable():
        return

    current_position = upload.file.tell()
    upload.file.seek(0, 2)
    size_hint = upload.file.tell()
    upload.file.seek(current_position)

    if size_hint > _MAX_UPLOAD_BYTES:
        raise _upload_limit_error(filename)


def _upload_limit_error(filename: str) -> HTTPException:
    return HTTPException(
        status_code=413,
        detail=f"File '{filename}' exceeds the {_MAX_UPLOAD_BYTES // (1024 * 1024)} MiB upload limit",
    )


def _ensure_within_upload_limit(bytes_written: int, filename: str) -> None:
    if bytes_written > _MAX_UPLOAD_BYTES:
        raise _upload_limit_error(filename)


async def _stream_upload_to_destination(upload: UploadFile, destination: Path, filename: str) -> None:
    bytes_written = 0
    with destination.open("wb") as handle:
        while chunk := await upload.read(_UPLOAD_CHUNK_BYTES):
            bytes_written += len(chunk)
            _ensure_within_upload_limit(bytes_written, filename)
            handle.write(chunk)


@router.get("/bases")
async def list_knowledge_bases() -> dict[str, Any]:
    """List all configured knowledge bases with status summaries."""
    config = Config.from_yaml()
    manager_map = await _ensure_managers(config)

    bases: list[dict[str, Any]] = []
    for base_id in sorted(config.knowledge_bases):
        root = _knowledge_root(config, base_id)
        manager = manager_map.get(base_id)
        if manager is None:
            file_count = len(_list_file_info(root)[0])
            indexed_count = 0
        else:
            status = manager.get_status()
            file_count = int(status["file_count"])
            indexed_count = int(status["indexed_count"])

        bases.append(
            {
                "name": base_id,
                "path": str(root),
                "watch": config.knowledge_bases[base_id].watch,
                "file_count": file_count,
                "indexed_count": indexed_count,
            },
        )

    return {
        "bases": bases,
        "count": len(bases),
    }


@router.get("/bases/{base_id}/files")
async def list_knowledge_files(base_id: str) -> dict[str, Any]:
    """List all files currently present in one knowledge base folder."""
    config = Config.from_yaml()
    root = _knowledge_root(config, base_id)
    files, total_size = _list_file_info(root)

    return {
        "base_id": base_id,
        "files": files,
        "total_size": total_size,
        "file_count": len(files),
    }


@router.post("/bases/{base_id}/upload")
async def upload_knowledge_files(base_id: str, files: Annotated[list[UploadFile], File(...)]) -> dict[str, Any]:
    """Upload one or more files into a knowledge base folder."""
    config = Config.from_yaml()
    root = _knowledge_root(config, base_id, create=True)

    uploaded: list[str] = []
    uploaded_paths: list[Path] = []
    for upload in files:
        filename = Path(upload.filename or "").name
        if not filename:
            await upload.close()
            continue

        destination = _resolve_within_root(root, filename)

        try:
            _validate_upload_size_hint(upload, filename)
            destination.parent.mkdir(parents=True, exist_ok=True)
            await _stream_upload_to_destination(upload, destination, filename)
        except Exception:
            destination.unlink(missing_ok=True)
            _rollback_uploaded_files(uploaded_paths)
            raise
        finally:
            await upload.close()

        uploaded_paths.append(destination)
        uploaded.append(destination.relative_to(root).as_posix())

    manager = await _ensure_manager(config, base_id)
    if manager is not None:
        for relative_path in uploaded:
            await manager.index_file(relative_path, upsert=True)

    return {
        "base_id": base_id,
        "uploaded": uploaded,
        "count": len(uploaded),
    }


@router.delete("/bases/{base_id}/files/{path:path}")
async def delete_knowledge_file(base_id: str, path: str) -> dict[str, Any]:
    """Delete one knowledge file from disk and from the vector index."""
    config = Config.from_yaml()
    root = _knowledge_root(config, base_id)
    decoded_path = unquote(path)
    target = _resolve_within_root(root, decoded_path)

    if not target.exists() or not target.is_file():
        raise HTTPException(status_code=404, detail="Knowledge file not found")

    relative_path = target.relative_to(root).as_posix()
    target.unlink()

    manager = await _ensure_manager(config, base_id)
    if manager is not None:
        await manager.remove_file(relative_path)

    return {
        "success": True,
        "base_id": base_id,
        "path": relative_path,
    }


@router.get("/bases/{base_id}/status")
async def knowledge_status(base_id: str) -> dict[str, Any]:
    """Return current indexing status for one knowledge base."""
    config = Config.from_yaml()
    root = _knowledge_root(config, base_id)
    manager = await _ensure_manager(config, base_id)

    if manager is not None:
        manager_status = manager.get_status()
        indexed_count = int(manager_status["indexed_count"])
        file_count = int(manager_status["file_count"])
    else:
        indexed_count = 0
        file_count = len(_list_file_info(root)[0])

    return {
        "base_id": base_id,
        "folder_path": str(root),
        "watch": config.knowledge_bases[base_id].watch,
        "file_count": file_count,
        "indexed_count": indexed_count,
    }


@router.post("/bases/{base_id}/reindex")
async def reindex_knowledge(base_id: str) -> dict[str, Any]:
    """Force reindexing of all files in one knowledge base folder."""
    config = Config.from_yaml()
    _ensure_base_exists(config, base_id)

    manager = await _ensure_manager(config, base_id)
    if manager is None:
        raise HTTPException(status_code=500, detail="Knowledge manager is unavailable")

    indexed_count = await manager.reindex_all()
    return {
        "success": True,
        "base_id": base_id,
        "indexed_count": indexed_count,
    }

================================================================================
# FILE: src/mindroom/api/main.py
================================================================================

# ruff: noqa: D100
import os
import shutil
import threading
from pathlib import Path
from typing import Annotated, Any

import yaml
from dotenv import load_dotenv
from fastapi import Depends, FastAPI, Header, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from watchdog.events import FileSystemEvent, FileSystemEventHandler
from watchdog.observers import Observer

# Import routers
from mindroom.api.credentials import router as credentials_router
from mindroom.api.google_integration import router as google_router
from mindroom.api.homeassistant_integration import router as homeassistant_router
from mindroom.api.integrations import router as integrations_router
from mindroom.api.knowledge import router as knowledge_router
from mindroom.api.matrix_operations import router as matrix_router
from mindroom.api.skills import router as skills_router
from mindroom.api.tools import router as tools_router
from mindroom.config import Config
from mindroom.constants import DEFAULT_AGENTS_CONFIG, DEFAULT_CONFIG_TEMPLATE, safe_replace
from mindroom.credentials_sync import sync_env_to_credentials

# Load environment variables from .env file
# Look for .env in the widget directory (parent of backend)
env_path = Path(__file__).parent.parent.parent / ".env"
load_dotenv(env_path)

app = FastAPI(title="MindRoom Widget Backend")

# Configure CORS for widget - allow multiple origins including port forwarding
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3003",  # Frontend dev server alternative port
        "http://localhost:5173",  # Vite dev server default
        "http://127.0.0.1:3003",  # Alternative localhost
        "http://127.0.0.1:5173",
        "*",  # Allow all origins for development (remove in production)
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# Resolve configurable config paths
CONFIG_PATH = DEFAULT_AGENTS_CONFIG
CONFIG_TEMPLATE_PATH = DEFAULT_CONFIG_TEMPLATE


def load_runtime_config() -> tuple[Config, Path]:
    """Load the current runtime config and return it with its path."""
    return Config.from_yaml(CONFIG_PATH), CONFIG_PATH


def ensure_writable_config() -> None:
    """Ensure the config file exists at a writable location.

    In managed deployments the writable config is placed on a persistent
    volume while a read-only template is mounted separately. When the final
    config file is missing we seed it from the template so both the bot and
    API read/write the same path.
    """
    CONFIG_PATH.parent.mkdir(parents=True, exist_ok=True)

    if CONFIG_PATH.exists():
        return

    if CONFIG_TEMPLATE_PATH != CONFIG_PATH and CONFIG_TEMPLATE_PATH.exists():
        shutil.copyfile(CONFIG_TEMPLATE_PATH, CONFIG_PATH)
        CONFIG_PATH.chmod(0o600)
        print(f"Seeded config from template {CONFIG_TEMPLATE_PATH} -> {CONFIG_PATH}")
        return

    # Fallback: create a minimal valid YAML structure so initial loads succeed
    CONFIG_PATH.write_text("agents: {}\nmodels: {}\n", encoding="utf-8")
    CONFIG_PATH.chmod(0o600)
    print(f"Created new config file at {CONFIG_PATH}")


def save_config_to_file(config: dict[str, Any]) -> None:
    """Save config to YAML file with deterministic ordering."""
    tmp_path = CONFIG_PATH.with_suffix(CONFIG_PATH.suffix + ".tmp")
    with tmp_path.open("w", encoding="utf-8") as f:
        yaml.dump(
            config,
            f,
            default_flow_style=False,
            sort_keys=True,
            allow_unicode=True,
        )
    safe_replace(tmp_path, CONFIG_PATH)


# Global variable to store current config
config: dict[str, Any] = {}
config_lock = threading.Lock()


# =========================
# Supabase JWT verification
# =========================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY")
ACCOUNT_ID = os.getenv("ACCOUNT_ID")  # optional: enforce instance ownership

_supabase_auth = None
if SUPABASE_URL and SUPABASE_ANON_KEY:
    try:
        from supabase import create_client

        _supabase_auth = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)
    except Exception:
        _supabase_auth = None


async def verify_user(authorization: str | None = Header(None)) -> dict:
    """Validate Supabase JWT from Authorization header; enforce owner if ACCOUNT_ID set.

    In standalone mode (no Supabase), returns a default user to allow access.
    """
    if _supabase_auth is None:
        # Standalone mode - no auth configured, allow access
        return {"user_id": "standalone", "email": None}

    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")

    token = authorization.removeprefix("Bearer ").strip()
    try:
        user = _supabase_auth.auth.get_user(token)
    except Exception as err:
        raise HTTPException(status_code=401, detail="Invalid token") from err

    if not user or not user.user:
        raise HTTPException(status_code=401, detail="Invalid token")

    if ACCOUNT_ID and user.user.id != ACCOUNT_ID:
        raise HTTPException(status_code=403, detail="Forbidden")

    return {"user_id": user.user.id, "email": user.user.email}


class TestModelRequest(BaseModel):
    """Request model for testing AI model connections."""

    modelId: str  # noqa: N815


class ConfigFileHandler(FileSystemEventHandler):
    """Watch for changes to config.yaml."""

    def on_modified(self, event: FileSystemEvent) -> None:
        """Handle file modification events."""
        src_path = event.src_path
        if isinstance(src_path, bytes):
            src_path = src_path.decode("utf-8")
        if src_path.endswith("config.yaml"):
            print(f"Config file changed: {src_path}")
            load_config_from_file()


def load_config_from_file() -> None:
    """Load config from YAML file."""
    global config
    try:
        with CONFIG_PATH.open() as f, config_lock:
            config = yaml.safe_load(f)
        print("Config loaded successfully")
    except Exception as e:
        print(f"Error loading config: {e}")


ensure_writable_config()

# Load initial config
load_config_from_file()

# Set up file watcher
observer = Observer()
observer.schedule(ConfigFileHandler(), path=str(CONFIG_PATH.parent), recursive=False)
observer.start()

# Include routers
app.include_router(credentials_router, dependencies=[Depends(verify_user)])
app.include_router(google_router, dependencies=[Depends(verify_user)])
app.include_router(homeassistant_router, dependencies=[Depends(verify_user)])
app.include_router(integrations_router, dependencies=[Depends(verify_user)])
app.include_router(matrix_router, dependencies=[Depends(verify_user)])
app.include_router(knowledge_router, dependencies=[Depends(verify_user)])
app.include_router(skills_router, dependencies=[Depends(verify_user)])
app.include_router(tools_router, dependencies=[Depends(verify_user)])


@app.get("/api/health")
async def health_check() -> dict[str, str]:
    """Health check endpoint for testing."""
    return {"status": "healthy"}


@app.on_event("startup")
async def startup_event() -> None:
    """Initialize the application."""
    print(f"Loading config from: {CONFIG_PATH}")
    print(f"Config exists: {CONFIG_PATH.exists()}")

    # Sync API keys from environment to CredentialsManager
    print("Syncing API keys from environment to CredentialsManager...")
    sync_env_to_credentials()


@app.on_event("shutdown")
async def shutdown_event() -> None:
    """Clean up on shutdown."""
    observer.stop()
    observer.join()


@app.post("/api/config/load")
async def load_config(_user: Annotated[dict, Depends(verify_user)]) -> dict[str, Any]:
    """Load configuration from file."""
    with config_lock:
        if not config:
            raise HTTPException(status_code=500, detail="Failed to load configuration")
        return config


@app.put("/api/config/save")
async def save_config(new_config: Config, _user: Annotated[dict, Depends(verify_user)]) -> dict[str, bool]:
    """Save configuration to file."""
    try:
        config_dict = new_config.model_dump(exclude_none=True)
        save_config_to_file(config_dict)

        # Update current config
        with config_lock:
            config.update(config_dict)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save configuration: {e!s}") from e
    else:
        return {"success": True}


@app.get("/api/config/agents")
async def get_agents(_user: Annotated[dict, Depends(verify_user)]) -> list[dict[str, Any]]:
    """Get all agents."""
    with config_lock:
        agents = config.get("agents", {})
        # Convert to list format with IDs
        agent_list = []
        for agent_id, agent_data in agents.items():
            agent = {"id": agent_id, **agent_data}
            agent_list.append(agent)
        return agent_list


@app.put("/api/config/agents/{agent_id}")
async def update_agent(
    agent_id: str,
    agent_data: dict[str, Any],
    _user: Annotated[dict, Depends(verify_user)],
) -> dict[str, bool]:
    """Update a specific agent."""
    with config_lock:
        if "agents" not in config:
            config["agents"] = {}

        # Remove ID from agent_data if present
        agent_data_copy = agent_data.copy()
        agent_data_copy.pop("id", None)

        config["agents"][agent_id] = agent_data_copy

    # Save to file
    try:
        save_config_to_file(config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save agent: {e!s}") from e
    else:
        return {"success": True}


@app.post("/api/config/agents")
async def create_agent(agent_data: dict[str, Any], _user: Annotated[dict, Depends(verify_user)]) -> dict[str, Any]:
    """Create a new agent."""
    agent_id = agent_data.get("display_name", "new_agent").lower().replace(" ", "_")

    with config_lock:
        if "agents" not in config:
            config["agents"] = {}

        # Check if agent already exists
        if agent_id in config["agents"]:
            # Generate unique ID
            counter = 1
            while f"{agent_id}_{counter}" in config["agents"]:
                counter += 1
            agent_id = f"{agent_id}_{counter}"

        # Remove ID from agent_data if present
        agent_data_copy = agent_data.copy()
        agent_data_copy.pop("id", None)

        config["agents"][agent_id] = agent_data_copy

    # Save to file
    try:
        save_config_to_file(config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create agent: {e!s}") from e
    else:
        return {"id": agent_id, "success": True}


@app.delete("/api/config/agents/{agent_id}")
async def delete_agent(agent_id: str, _user: Annotated[dict, Depends(verify_user)]) -> dict[str, bool]:
    """Delete an agent."""
    with config_lock:
        if "agents" not in config or agent_id not in config["agents"]:
            raise HTTPException(status_code=404, detail="Agent not found")

        del config["agents"][agent_id]

    # Save to file
    try:
        save_config_to_file(config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete agent: {e!s}") from e
    else:
        return {"success": True}


@app.get("/api/config/teams")
async def get_teams() -> list[dict[str, Any]]:
    """Get all teams."""
    with config_lock:
        teams = config.get("teams", {})
        # Convert to list format with IDs
        team_list = []
        for team_id, team_data in teams.items():
            team = {"id": team_id, **team_data}
            team_list.append(team)
        return team_list


@app.put("/api/config/teams/{team_id}")
async def update_team(team_id: str, team_data: dict[str, Any]) -> dict[str, bool]:
    """Update a specific team."""
    with config_lock:
        if "teams" not in config:
            config["teams"] = {}

        # Remove ID from team_data if present
        team_data_copy = team_data.copy()
        team_data_copy.pop("id", None)

        config["teams"][team_id] = team_data_copy

    # Save to file
    try:
        save_config_to_file(config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save team: {e!s}") from e
    else:
        return {"success": True}


@app.post("/api/config/teams")
async def create_team(team_data: dict[str, Any]) -> dict[str, Any]:
    """Create a new team."""
    team_id = team_data.get("display_name", "new_team").lower().replace(" ", "_")

    with config_lock:
        if "teams" not in config:
            config["teams"] = {}

        # Check if team already exists
        if team_id in config["teams"]:
            # Generate unique ID
            counter = 1
            while f"{team_id}_{counter}" in config["teams"]:
                counter += 1
            team_id = f"{team_id}_{counter}"

        # Remove ID from team_data if present
        team_data_copy = team_data.copy()
        team_data_copy.pop("id", None)

        config["teams"][team_id] = team_data_copy

    # Save to file
    try:
        save_config_to_file(config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create team: {e!s}") from e
    else:
        return {"id": team_id, "success": True}


@app.delete("/api/config/teams/{team_id}")
async def delete_team(team_id: str) -> dict[str, bool]:
    """Delete a team."""
    with config_lock:
        if "teams" not in config or team_id not in config["teams"]:
            raise HTTPException(status_code=404, detail="Team not found")

        del config["teams"][team_id]

    # Save to file
    try:
        save_config_to_file(config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete team: {e!s}") from e
    else:
        return {"success": True}


@app.get("/api/config/models")
async def get_models() -> dict[str, Any]:
    """Get all model configurations."""
    with config_lock:
        models = config.get("models", {})
        return dict(models) if models else {}


@app.put("/api/config/models/{model_id}")
async def update_model(model_id: str, model_data: dict[str, Any]) -> dict[str, bool]:
    """Update a model configuration."""
    with config_lock:
        if "models" not in config:
            config["models"] = {}

        config["models"][model_id] = model_data

    # Save to file
    try:
        save_config_to_file(config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save model: {e!s}") from e
    else:
        return {"success": True}


@app.get("/api/config/room-models")
async def get_room_models() -> dict[str, Any]:
    """Get room-specific model overrides."""
    with config_lock:
        room_models = config.get("room_models", {})
        return dict(room_models) if room_models else {}


@app.put("/api/config/room-models")
async def update_room_models(room_models: dict[str, str]) -> dict[str, bool]:
    """Update room-specific model overrides."""
    with config_lock:
        config["room_models"] = room_models

    # Save to file
    try:
        save_config_to_file(config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save room models: {e!s}") from e
    else:
        return {"success": True}


@app.post("/api/test/model")
async def test_model(request: TestModelRequest) -> dict[str, Any]:
    """Test a model connection."""
    # TODO: Implement actual model testing
    # For now, just return success for demonstration
    model_id = request.modelId
    with config_lock:
        if model_id in config.get("models", {}):
            return {"success": True, "message": f"Model {model_id} is configured"}
        return {"success": False, "message": f"Model {model_id} not found"}


@app.get("/api/rooms")
async def get_available_rooms() -> list[str]:
    """Get list of available rooms."""
    # Extract unique rooms from all agents
    rooms = set()
    with config_lock:
        for agent_data in config.get("agents", {}).values():
            agent_rooms = agent_data.get("rooms", [])
            rooms.update(agent_rooms)

    return sorted(rooms)


@app.post("/api/keys/encrypt")
async def encrypt_api_key(data: dict[str, str]) -> dict[str, str]:
    """Encrypt an API key for storage."""
    # TODO: Implement actual encryption
    # For now, just return a placeholder
    provider = data.get("provider", "")
    key = data.get("key", "")

    # In production, this would encrypt the key
    encrypted = f"encrypted_{provider}_{len(key)}"

    return {"encryptedKey": encrypted}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8765)  # noqa: S104

================================================================================
# FILE: src/mindroom/api/matrix_operations.py
================================================================================

"""API endpoints for Matrix operations."""

import asyncio
from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from mindroom.constants import MATRIX_HOMESERVER
from mindroom.logging_config import get_logger
from mindroom.matrix.client import get_joined_rooms, get_room_name, leave_room
from mindroom.matrix.rooms import resolve_room_aliases
from mindroom.matrix.users import create_agent_user, login_agent_user

logger = get_logger(__name__)

router = APIRouter(prefix="/api/matrix", tags=["matrix"])


class RoomLeaveRequest(BaseModel):
    """Request to leave a room."""

    agent_id: str
    room_id: str


class RoomInfo(BaseModel):
    """Information about a room."""

    room_id: str
    name: str | None = None


class AgentRoomsResponse(BaseModel):
    """Response containing agent rooms information."""

    agent_id: str
    display_name: str
    configured_rooms: list[str]
    joined_rooms: list[str]
    unconfigured_rooms: list[str]
    unconfigured_room_details: list[RoomInfo] = []


class AllAgentsRoomsResponse(BaseModel):
    """Response containing all agents' room information."""

    agents: list[AgentRoomsResponse]


async def get_agent_matrix_rooms(agent_id: str, agent_data: dict[str, Any]) -> AgentRoomsResponse:
    """Get Matrix rooms for a specific agent.

    Args:
        agent_id: The agent identifier
        agent_data: The agent configuration data

    Returns:
        AgentRoomsResponse with room information

    """
    # Create or get the agent user
    agent_user = await create_agent_user(
        MATRIX_HOMESERVER,
        agent_id,
        agent_data.get("display_name", agent_id),
    )

    # Login and get the client
    client = await login_agent_user(MATRIX_HOMESERVER, agent_user)

    # Get all joined rooms from Matrix
    joined_rooms = await get_joined_rooms(client) or []

    # Get configured rooms from config (these are aliases like "lobby", "analysis")
    configured_room_aliases = agent_data.get("rooms", [])

    # Resolve room aliases to room IDs for comparison
    configured_room_ids = resolve_room_aliases(configured_room_aliases)

    # Calculate unconfigured rooms (joined but not in config)
    unconfigured_rooms = [room for room in joined_rooms if room not in configured_room_ids]

    # Get room names for unconfigured rooms
    unconfigured_room_details = []
    for room_id in unconfigured_rooms:
        room_name = await get_room_name(client, room_id)
        unconfigured_room_details.append(RoomInfo(room_id=room_id, name=room_name))

    await client.close()

    return AgentRoomsResponse(
        agent_id=agent_id,
        display_name=agent_data.get("display_name", agent_id),
        configured_rooms=configured_room_ids,
        joined_rooms=joined_rooms,
        unconfigured_rooms=unconfigured_rooms,
        unconfigured_room_details=unconfigured_room_details,
    )


@router.get("/agents/rooms")
async def get_all_agents_rooms() -> AllAgentsRoomsResponse:
    """Get room information for all agents.

    Returns information about configured rooms, joined rooms,
    and unconfigured rooms (joined but not in config) for each agent.
    """
    from .main import config, config_lock  # noqa: PLC0415

    agents_rooms = []

    with config_lock:
        agents = config.get("agents", {})

    # Gather room information for all agents concurrently
    tasks = [get_agent_matrix_rooms(agent_id, agent_data) for agent_id, agent_data in agents.items()]
    agents_rooms = await asyncio.gather(*tasks)

    return AllAgentsRoomsResponse(agents=agents_rooms)


@router.get("/agents/{agent_id}/rooms")
async def get_agent_rooms(agent_id: str) -> AgentRoomsResponse:
    """Get room information for a specific agent.

    Args:
        agent_id: The agent identifier

    Returns:
        Room information for the agent

    Raises:
        HTTPException: If agent not found or error occurs

    """
    from .main import config, config_lock  # noqa: PLC0415

    with config_lock:
        agents = config.get("agents", {})
        if agent_id not in agents:
            raise HTTPException(status_code=404, detail=f"Agent {agent_id} not found")
        agent_data = agents[agent_id]

    return await get_agent_matrix_rooms(agent_id, agent_data)


@router.post("/rooms/leave")
async def leave_room_endpoint(request: RoomLeaveRequest) -> dict[str, bool]:
    """Make an agent leave a specific room.

    Args:
        request: Contains agent_id and room_id

    Returns:
        Success status

    Raises:
        HTTPException: If agent not found or leave operation fails

    """
    from .main import config, config_lock  # noqa: PLC0415

    with config_lock:
        agents = config.get("agents", {})
        if request.agent_id not in agents:
            raise HTTPException(status_code=404, detail=f"Agent {request.agent_id} not found")

    # Get agent configuration
    agent_data = agents[request.agent_id]

    # Create or get the agent user
    agent_user = await create_agent_user(
        MATRIX_HOMESERVER,
        request.agent_id,
        agent_data.get("display_name", request.agent_id),
    )

    # Login and get the client
    client = await login_agent_user(MATRIX_HOMESERVER, agent_user)

    # Leave the room
    success = await leave_room(client, request.room_id)

    # Close the client connection
    await client.close()

    if not success:
        raise HTTPException(status_code=500, detail=f"Failed to leave room {request.room_id}")
    return {"success": True}


@router.post("/rooms/leave-bulk")
async def leave_rooms_bulk(requests: list[RoomLeaveRequest]) -> dict[str, Any]:
    """Make multiple agents leave multiple rooms.

    Args:
        requests: List of leave requests

    Returns:
        Results for each request

    """
    results = []
    for request in requests:
        try:
            await leave_room_endpoint(request)
            results.append({"agent_id": request.agent_id, "room_id": request.room_id, "success": True})
        except HTTPException as e:
            results.append(
                {
                    "agent_id": request.agent_id,
                    "room_id": request.room_id,
                    "success": False,
                    "error": e.detail,
                },
            )

    return {"results": results, "success": all(r["success"] for r in results)}

================================================================================
# FILE: src/mindroom/api/skills.py
================================================================================

"""API endpoints for skill inspection and editing."""

from __future__ import annotations

import re
import shutil

import yaml
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from mindroom.constants import safe_replace
from mindroom.skills import (
    get_user_skills_dir,
    list_skill_listings,
    resolve_skill_listing,
    skill_can_edit,
)

router = APIRouter(prefix="/api/skills", tags=["skills"])

_VALID_SKILL_NAME = re.compile(r"^[a-z0-9]([a-z0-9-]*[a-z0-9])?$")


class SkillSummary(BaseModel):
    """Summary information for a skill."""

    name: str
    description: str
    origin: str
    can_edit: bool


class SkillDetail(SkillSummary):
    """Detailed skill information including content."""

    content: str


class CreateSkillRequest(BaseModel):
    """Request payload for creating a new skill."""

    name: str
    description: str


class SkillUpdateRequest(BaseModel):
    """Request payload for updating a skill."""

    content: str


@router.get("")
async def list_skills() -> list[SkillSummary]:
    """List installed skills."""
    listings = list_skill_listings()
    return [
        SkillSummary(
            name=listing.name,
            description=listing.description,
            origin=listing.origin,
            can_edit=skill_can_edit(listing.path),
        )
        for listing in listings
    ]


@router.get("/{skill_name}")
async def get_skill(skill_name: str) -> SkillDetail:
    """Get a specific skill and its content."""
    listing = resolve_skill_listing(skill_name)
    if listing is None:
        raise HTTPException(status_code=404, detail="Skill not found")

    try:
        content = listing.path.read_text(encoding="utf-8")
    except OSError as exc:
        raise HTTPException(status_code=500, detail="Failed to read skill content") from exc

    return SkillDetail(
        name=listing.name,
        description=listing.description,
        origin=listing.origin,
        can_edit=skill_can_edit(listing.path),
        content=content,
    )


@router.put("/{skill_name}")
async def update_skill(skill_name: str, payload: SkillUpdateRequest) -> dict[str, bool]:
    """Update a skill's SKILL.md content."""
    listing = resolve_skill_listing(skill_name)
    if listing is None:
        raise HTTPException(status_code=404, detail="Skill not found")

    if not skill_can_edit(listing.path):
        raise HTTPException(status_code=403, detail="Skill is read-only")

    tmp_path = listing.path.with_suffix(listing.path.suffix + ".tmp")
    try:
        tmp_path.write_text(payload.content, encoding="utf-8")
        safe_replace(tmp_path, listing.path)
    except OSError as exc:
        raise HTTPException(status_code=500, detail="Failed to update skill") from exc

    return {"success": True}


@router.post("")
async def create_skill(payload: CreateSkillRequest) -> SkillSummary:
    """Create a new user skill."""
    name = payload.name.strip()
    if not name:
        raise HTTPException(status_code=422, detail="Skill name must not be empty")
    if not _VALID_SKILL_NAME.match(name):
        raise HTTPException(
            status_code=422,
            detail="Skill name must be lowercase alphanumeric with hyphens, starting with a letter or digit",
        )

    if resolve_skill_listing(name) is not None:
        raise HTTPException(status_code=409, detail="A skill with this name already exists")

    description = payload.description.strip() or name
    skill_dir = get_user_skills_dir() / name
    try:
        skill_dir.mkdir(parents=True, exist_ok=False)
    except FileExistsError as exc:
        raise HTTPException(status_code=409, detail="A skill with this name already exists") from exc

    frontmatter = yaml.dump({"name": name, "description": description}, default_flow_style=False).strip()
    content = f"---\n{frontmatter}\n---\n\n# {name}\n"
    (skill_dir / "SKILL.md").write_text(content, encoding="utf-8")

    return SkillSummary(name=name, description=description, origin="user", can_edit=True)


@router.delete("/{skill_name}")
async def delete_skill(skill_name: str) -> dict[str, bool]:
    """Delete a user skill."""
    listing = resolve_skill_listing(skill_name)
    if listing is None:
        raise HTTPException(status_code=404, detail="Skill not found")

    if not skill_can_edit(listing.path):
        raise HTTPException(status_code=403, detail="Skill is read-only")

    skill_dir = listing.path.parent.resolve()
    user_root = get_user_skills_dir().expanduser().resolve()
    if skill_dir == user_root:
        raise HTTPException(status_code=400, detail="Cannot delete a root-level skill")
    shutil.rmtree(skill_dir)

    return {"success": True}

================================================================================
# FILE: src/mindroom/api/tools.py
================================================================================

"""API endpoints for tools information."""

from typing import Any

from fastapi import APIRouter
from pydantic import BaseModel

from mindroom.credentials import CredentialsManager, get_credentials_manager
from mindroom.tools_metadata import ensure_tool_registry_loaded, export_tools_metadata

from .google_tools_helper import check_google_tool_configured

router = APIRouter(prefix="/api/tools", tags=["tools"])


class ToolsResponse(BaseModel):
    """Response containing all registered tools."""

    tools: list[dict]


def _check_homeassistant_configured(tool_name: str, manager: CredentialsManager) -> bool:
    """Check if HomeAssistant is configured."""
    if tool_name == "homeassistant":
        ha_creds = manager.load_credentials("homeassistant")
        if not ha_creds:
            return False
        # Check for the fields that HomeAssistantTools actually uses
        has_url = "instance_url" in ha_creds
        has_token = "access_token" in ha_creds or "long_lived_token" in ha_creds
        return has_url and has_token
    return False


def _check_standard_tool_configured(tool: dict[str, Any], manager: CredentialsManager) -> bool:
    """Check if a standard tool with config_fields is configured."""
    if not tool.get("config_fields"):
        return False

    credentials = manager.load_credentials(tool["name"])
    if not credentials:
        return False

    # Check if all required fields are present
    required_fields = [field["name"] for field in tool.get("config_fields", []) if field.get("required", True)]
    return all(field in credentials for field in required_fields)


@router.get("")
async def get_registered_tools() -> ToolsResponse:
    """Get all registered tools from mindroom.

    This builds tool metadata from the in-memory registry and updates availability
    based on credentials (including plugin-provided tools).
    """
    from mindroom.api.main import load_runtime_config  # noqa: PLC0415

    config, config_path = load_runtime_config()
    ensure_tool_registry_loaded(config, config_path=config_path)
    tools = export_tools_metadata()

    # Get credentials manager to check if tools are configured
    manager = get_credentials_manager()

    # Update status for tools that require configuration
    for tool in tools:
        tool_name = tool["name"]
        if tool.get("status") == "requires_config":
            # Check if tool has delegated auth
            auth_provider = tool.get("auth_provider")
            if auth_provider:
                # Check if the auth provider is configured
                provider_creds = manager.load_credentials(auth_provider)
                if provider_creds and (
                    (auth_provider == "google" and check_google_tool_configured(tool_name, provider_creds))
                    or auth_provider != "google"
                ):
                    tool["status"] = "available"
            # Check other configured tools
            elif _check_homeassistant_configured(tool_name, manager) or _check_standard_tool_configured(tool, manager):
                tool["status"] = "available"

    return ToolsResponse(tools=tools)

================================================================================
# FILE: src/mindroom/background_tasks.py
================================================================================

"""Background task management for non-blocking operations."""

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Any

from .logging_config import get_logger

if TYPE_CHECKING:
    from collections.abc import Callable, Coroutine

logger = get_logger(__name__)

# Global set to track background tasks and prevent them from being garbage collected
_background_tasks: set[asyncio.Task[Any]] = set()


def create_background_task(
    coro: Coroutine[Any, Any, Any],
    name: str | None = None,
    error_handler: Callable[[Exception], None] | None = None,
) -> asyncio.Task[Any]:
    """Create a background task that won't block the main execution.

    Args:
        coro: The coroutine to run in the background
        name: Optional name for the task (for logging)
        error_handler: Optional error handler function

    Returns:
        The created task

    """
    task: asyncio.Task[Any] = asyncio.create_task(coro)
    if name:
        task.set_name(name)

    # Add to global set to prevent garbage collection
    _background_tasks.add(task)

    # Add completion callback to remove from set and handle errors
    def _task_done_callback(task: asyncio.Task[Any]) -> None:
        _background_tasks.discard(task)
        try:
            # This will raise if the task had an exception
            task.result()
        except asyncio.CancelledError:
            # Task was cancelled, this is fine
            pass
        except Exception as e:
            task_name = task.get_name() if hasattr(task, "get_name") else "unknown"
            logger.exception("Background task failed", task_name=task_name, error=str(e))
            if error_handler:
                try:
                    error_handler(e)
                except Exception as handler_error:
                    logger.exception("Error handler for task failed", task_name=task_name, error=str(handler_error))

    task.add_done_callback(_task_done_callback)
    return task


async def wait_for_background_tasks(timeout: float | None = None) -> None:  # noqa: ASYNC109
    """Wait for all background tasks to complete.

    Args:
        timeout: Optional timeout in seconds

    """
    if not _background_tasks:
        return

    try:
        await asyncio.wait_for(asyncio.gather(*_background_tasks, return_exceptions=True), timeout=timeout)
    except TimeoutError:
        logger.warning(f"Background tasks did not complete within {timeout} seconds")
        # Cancel remaining tasks
        for task in _background_tasks:
            task.cancel()
        # Wait for cancellation to complete
        await asyncio.gather(*_background_tasks, return_exceptions=True)


def get_background_task_count() -> int:
    """Get the number of currently running background tasks."""
    return len(_background_tasks)

================================================================================
# FILE: src/mindroom/bot.py
================================================================================

"""Multi-agent bot implementation where each agent has its own Matrix user account."""

from __future__ import annotations

import asyncio
import inspect
from contextlib import suppress
from dataclasses import dataclass, field
from functools import cached_property
from pathlib import Path
from typing import TYPE_CHECKING, Any

import nio
from tenacity import RetryCallState, retry, stop_after_attempt, wait_exponential

from . import config_confirmation, interactive, voice_handler
from .agents import create_agent, get_rooms_for_entity
from .ai import ai_response, stream_agent_response
from .background_tasks import create_background_task, wait_for_background_tasks
from .commands import (
    Command,
    CommandType,
    command_parser,
    get_command_help,
    handle_widget_command,
)
from .config import Config
from .config_commands import handle_config_command
from .constants import ENABLE_STREAMING, MATRIX_HOMESERVER, ROUTER_AGENT_NAME, VOICE_PREFIX
from .credentials_sync import sync_env_to_credentials
from .file_watcher import watch_file
from .knowledge import initialize_knowledge_managers, shutdown_knowledge_managers
from .logging_config import emoji, get_logger, setup_logging
from .matrix.client import (
    _latest_thread_event_id,
    check_and_set_avatar,
    edit_message,
    fetch_thread_history,
    get_joined_rooms,
    get_latest_thread_event_id_if_needed,
    get_room_members,
    invite_to_room,
    join_room,
    send_message,
)
from .matrix.event_info import EventInfo
from .matrix.identity import (
    MatrixID,
    extract_agent_name,
    extract_server_name_from_homeserver,
)
from .matrix.mentions import format_message_with_mentions
from .matrix.presence import build_agent_status_message, is_user_online, set_presence_status, should_use_streaming
from .matrix.rooms import (
    ensure_all_rooms_exist,
    ensure_user_in_rooms,
    is_dm_room,
    leave_non_dm_rooms,
    load_rooms,
    resolve_room_aliases,
)
from .matrix.state import MatrixState
from .matrix.typing import typing_indicator
from .matrix.users import AgentMatrixUser, create_agent_user, login_agent_user
from .memory import store_conversation_memory
from .plugins import load_plugins
from .response_tracker import ResponseTracker
from .room_cleanup import cleanup_all_orphaned_bots
from .routing import suggest_agent_for_message
from .scheduling import (
    cancel_all_scheduled_tasks,
    cancel_scheduled_task,
    list_scheduled_tasks,
    restore_scheduled_tasks,
    schedule_task,
)
from .skills import clear_skill_cache, get_skill_snapshot, resolve_skill_command_spec
from .stop import StopManager
from .streaming import (
    IN_PROGRESS_MARKER,
    ReplacementStreamingResponse,
    StreamingResponse,
    send_streaming_response,
)
from .teams import (
    TeamMode,
    decide_team_formation,
    select_model_for_team,
    team_response,
    team_response_stream,
)
from .thread_utils import (
    check_agent_mentioned,
    create_session_id,
    get_agents_in_thread,
    get_all_mentioned_agents_in_thread,
    get_available_agents_in_room,
    get_configured_agents_for_room,
    has_user_responded_after_message,
    is_authorized_sender,
    should_agent_respond,
)
from .tools_metadata import get_tool_by_name

if TYPE_CHECKING:
    from collections.abc import Callable, Mapping

    import structlog
    from agno.agent import Agent
    from agno.knowledge.knowledge import Knowledge
    from agno.tools.function import Function
    from agno.tools.toolkit import Toolkit

    from .knowledge import KnowledgeManager

logger = get_logger(__name__)


# Constants
SYNC_TIMEOUT_MS = 30000


def _create_task_wrapper(callback: object) -> object:
    """Create a wrapper that runs the callback as a background task.

    This ensures the sync loop is never blocked by event processing,
    allowing the bot to handle new events (like stop reactions) while
    processing messages.
    """

    async def wrapper(*args: object, **kwargs: object) -> None:
        # Create the task but don't await it - let it run in background
        async def error_handler() -> None:
            try:
                await callback(*args, **kwargs)  # type: ignore[operator]
            except asyncio.CancelledError:
                # Task was cancelled, this is expected during shutdown
                pass
            except Exception:
                # Log the exception with full traceback
                logger.exception("Error in event callback")

        # Create task with error handling
        _task = asyncio.create_task(error_handler())  # noqa: RUF006

    return wrapper


def _format_agent_description(agent_name: str, config: Config) -> str:
    """Format a concise agent description for the welcome message."""
    if agent_name in config.agents:
        agent_config = config.agents[agent_name]
        desc_parts = []

        # Add role first
        if agent_config.role:
            desc_parts.append(agent_config.role)

        # Add tools with better formatting
        if agent_config.tools:
            # Wrap each tool name in backticks
            formatted_tools = [f"`{tool}`" for tool in agent_config.tools[:3]]
            tools_str = ", ".join(formatted_tools)
            if len(agent_config.tools) > 3:
                tools_str += f" +{len(agent_config.tools) - 3} more"
            desc_parts.append(f"(ðŸ”§ {tools_str})")

        return " ".join(desc_parts) if desc_parts else ""

    if agent_name in config.teams:
        team_config = config.teams[agent_name]
        team_desc = f"Team of {len(team_config.agents)} agents"
        if team_config.role:
            return f"{team_config.role} ({team_desc})"
        return team_desc

    return ""


def _generate_welcome_message(room_id: str, config: Config) -> str:
    """Generate the welcome message text for a room."""
    # Get list of configured agents for this room
    configured_agents = get_configured_agents_for_room(room_id, config)

    # Build agent list for the welcome message
    agent_list = []
    for agent_id in configured_agents:
        agent_name = agent_id.agent_name(config)
        if not agent_name or agent_name == ROUTER_AGENT_NAME:
            continue

        description = _format_agent_description(agent_name, config)
        # Always show the agent, with or without description
        # Use the username with mindroom_ prefix (but without domain) for proper mention parsing
        agent_entry = f"â€¢ **@{agent_id.username}**"
        if description:
            agent_entry += f": {description}"
        agent_list.append(agent_entry)

    # Create welcome message
    welcome_msg = (
        "ðŸŽ‰ **Welcome to MindRoom!**\n\n"
        "I'm your routing assistant, here to help coordinate our team of specialized AI agents. ðŸ¤–\n\n"
    )

    if agent_list:
        welcome_msg += "ðŸ§  **Available agents in this room:**\n"
        welcome_msg += "\n".join(agent_list)
        welcome_msg += "\n\n"

    welcome_msg += (
        "ðŸ’¬ **How to interact:**\n"
        "â€¢ Mention an agent with @ to get their attention (e.g., @mindroom_assistant)\n"
        "â€¢ Use `!help` to see available commands\n"
        "â€¢ Agents respond in threads to keep conversations organized\n"
        "â€¢ Multiple agents can collaborate when you mention them together\n"
        "â€¢ ðŸŽ¤ Voice messages are automatically transcribed and work perfectly!\n\n"
        "âš¡ **Quick commands:**\n"
        "â€¢ `!hi` - Show this welcome message again\n"
        "â€¢ `!widget` - Add configuration widget to this room\n"
        "â€¢ `!schedule <time> <message>` - Schedule tasks and reminders\n"
        "â€¢ `!help [topic]` - Get detailed help\n\n"
        "âœ¨ Feel free to ask any agent for help or start a conversation!"
    )

    return welcome_msg


def _build_skill_command_prompt(skill_name: str, args_text: str) -> str:
    args = args_text.strip()
    args_section = args if args else "(no arguments provided)"
    return (
        "You were invoked via the !skill command.\n"
        f"Skill: {skill_name}\n"
        f"User input:\n{args_section}\n\n"
        "Load the skill instructions with get_skill_instructions and follow them."
    )


def _resolve_skill_command_agent(  # noqa: C901
    skill_name: str,
    *,
    config: Config,
    room: nio.MatrixRoom,
    mentioned_agents: list[MatrixID],
) -> tuple[str | None, str | None]:
    requested = skill_name.strip().lower()
    mentioned_names: list[str] = []
    for mid in mentioned_agents:
        name = mid.agent_name(config)
        if not name or name == ROUTER_AGENT_NAME:
            continue
        mentioned_names.append(name)
    unique_mentions = list(dict.fromkeys(mentioned_names))
    if len(unique_mentions) > 1:
        return None, f"âŒ Multiple agents mentioned: {', '.join(unique_mentions)}. Mention only one."

    agents_in_room = get_available_agents_in_room(room, config)
    candidate_names: list[str] = []
    for mid in agents_in_room:
        name = mid.agent_name(config)
        if not name:
            continue
        if name not in config.agents:
            continue
        allowlist = {skill.lower() for skill in config.get_agent(name).skills}
        if requested in allowlist:
            candidate_names.append(name)
    candidate_names = list(dict.fromkeys(candidate_names))

    if unique_mentions:
        target = unique_mentions[0]
        if target not in candidate_names:
            return None, f"âŒ Agent '{target}' does not have skill '{skill_name}' enabled in this room."
        return target, None

    if len(candidate_names) == 1:
        return candidate_names[0], None

    if not candidate_names:
        return None, f"âŒ No agents in this room have skill '{skill_name}' enabled."

    return None, (
        f"âŒ Multiple agents have skill '{skill_name}': {', '.join(candidate_names)}. "
        "Mention one with @mindroom_<agent>."
    )


def _collect_agent_toolkits(config: Config, agent_name: str) -> list[tuple[str, Toolkit]]:
    agent_config = config.get_agent(agent_name)
    toolkits: list[tuple[str, Toolkit]] = []
    for tool_name in agent_config.tools:
        try:
            toolkits.append((tool_name, get_tool_by_name(tool_name)))
        except ValueError as exc:
            logger.warning(
                "Failed to load tool for skill dispatch",
                tool=tool_name,
                agent=agent_name,
                error=str(exc),
            )
    return toolkits


def _resolve_tool_dispatch_target(  # noqa: C901, PLR0911, PLR0912
    toolkits: list[tuple[str, Toolkit]],
    command_tool: str,
) -> tuple[Function | None, Toolkit | None, str | None]:
    if not command_tool:
        return None, None, "Missing command-tool for tool dispatch."

    if "." in command_tool:
        toolkit_name, function_name = command_tool.split(".", 1)
        for registered_name, toolkit in toolkits:
            if registered_name != toolkit_name:
                continue
            function = toolkit.functions.get(function_name) or toolkit.async_functions.get(function_name)
            if function:
                return function, toolkit, None
        return None, None, f"Tool '{toolkit_name}' does not expose '{function_name}'."

    matches: list[tuple[Function, Toolkit, str]] = []
    for registered_name, toolkit in toolkits:
        function = toolkit.functions.get(command_tool) or toolkit.async_functions.get(command_tool)
        if function:
            matches.append((function, toolkit, registered_name))

    if len(matches) == 1:
        function, toolkit, _ = matches[0]
        return function, toolkit, None

    if len(matches) > 1:
        toolkit_names = ", ".join(sorted({name for _, _, name in matches}))
        return None, None, f"Command tool '{command_tool}' is ambiguous across toolkits: {toolkit_names}."

    for registered_name, toolkit in toolkits:
        if registered_name != command_tool:
            continue
        functions = {**toolkit.functions, **toolkit.async_functions}
        if not functions:
            return None, None, f"Tool '{command_tool}' has no callable functions."
        if len(functions) == 1:
            return next(iter(functions.values())), toolkit, None
        return None, None, f"Tool '{command_tool}' exposes multiple functions; specify one."

    return None, None, f"Tool '{command_tool}' not found for this agent."


@dataclass(frozen=True)
class ToolCallArguments:
    """Prepared arguments for a tool call."""

    args: tuple[object, ...]
    kwargs: dict[str, object]
    error: str | None = None


def _prepare_tool_call_arguments(  # noqa: PLR0911
    entrypoint: Callable[..., object] | None,
    base_args: Mapping[str, object],
) -> ToolCallArguments:
    if entrypoint is None:
        return ToolCallArguments((), {}, "Tool entrypoint is missing.")

    signature = inspect.signature(entrypoint)
    params = list(signature.parameters.values())
    has_var_kw = any(param.kind == param.VAR_KEYWORD for param in params)
    if has_var_kw:
        return ToolCallArguments((), dict(base_args), None)

    kwargs = {key: value for key, value in base_args.items() if key in signature.parameters}
    if kwargs:
        missing = [
            param.name
            for param in params
            if param.default is param.empty
            and param.kind
            in (
                inspect.Parameter.POSITIONAL_ONLY,
                inspect.Parameter.POSITIONAL_OR_KEYWORD,
                inspect.Parameter.KEYWORD_ONLY,
            )
            and param.name not in kwargs
        ]
        if missing:
            return ToolCallArguments((), {}, f"Tool requires parameters: {', '.join(missing)}.")
        return ToolCallArguments((), kwargs, None)

    if not params:
        return ToolCallArguments((), {}, None)

    if len(params) == 1 and params[0].kind in (
        inspect.Parameter.POSITIONAL_ONLY,
        inspect.Parameter.POSITIONAL_OR_KEYWORD,
    ):
        return ToolCallArguments((base_args.get("command", ""),), {}, None)

    missing = [
        param.name
        for param in params
        if param.default is param.empty
        and param.kind
        in (
            inspect.Parameter.POSITIONAL_ONLY,
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
            inspect.Parameter.KEYWORD_ONLY,
        )
    ]
    if missing:
        return ToolCallArguments((), {}, f"Tool requires parameters: {', '.join(missing)}.")
    return ToolCallArguments((), {}, None)


async def _maybe_await(value: object) -> object:
    if inspect.isawaitable(value):
        return await value
    return value


async def _run_skill_command_tool(
    *,
    config: Config,
    agent_name: str,
    command_tool: str,
    skill_name: str,
    args_text: str,
    command_name: str = "skill",
) -> str:
    toolkits = _collect_agent_toolkits(config, agent_name)
    function, toolkit, error = _resolve_tool_dispatch_target(toolkits, command_tool)
    if error:
        return f"âŒ {error}"
    assert function is not None

    base_args = {
        "command": args_text,
        "commandName": command_name,
        "skillName": skill_name,
    }
    entrypoint = function.entrypoint
    call_args = _prepare_tool_call_arguments(entrypoint, base_args)
    if call_args.error:
        return f"âŒ {call_args.error}"
    assert entrypoint is not None

    try:
        if toolkit and getattr(toolkit, "_requires_connect", False):
            connect = getattr(toolkit, "connect", None)
            close = getattr(toolkit, "close", None)
            if connect:
                await _maybe_await(connect())
            try:
                result = await _maybe_await(entrypoint(*call_args.args, **call_args.kwargs))
            finally:
                if close:
                    await _maybe_await(close())
        else:
            result = await _maybe_await(entrypoint(*call_args.args, **call_args.kwargs))
    except Exception as exc:
        logger.warning(
            "Skill command tool dispatch failed",
            agent=agent_name,
            tool=command_tool,
            error=str(exc),
        )
        return f"âŒ Tool '{command_tool}' failed: {exc}"

    if result is None or result == "":
        return "âœ… Tool completed."
    return str(result)


def _should_skip_mentions(event_source: dict) -> bool:
    """Check if mentions in this message should be ignored for agent responses.

    This is used for messages like scheduling confirmations that contain mentions
    but should not trigger agent responses.

    Args:
        event_source: The Matrix event source dict

    Returns:
        True if mentions should be ignored, False otherwise

    """
    content = event_source.get("content", {})
    return bool(content.get("com.mindroom.skip_mentions", False))


def create_bot_for_entity(
    entity_name: str,
    agent_user: AgentMatrixUser,
    config: Config,
    storage_path: Path,
) -> AgentBot | TeamBot | None:
    """Create appropriate bot instance for an entity (agent, team, or router).

    Args:
        entity_name: Name of the entity to create a bot for
        agent_user: Matrix user for the bot
        config: Configuration object
        storage_path: Path for storing agent data

    Returns:
        Bot instance or None if entity not found in config

    """
    enable_streaming = ENABLE_STREAMING

    if entity_name == ROUTER_AGENT_NAME:
        all_room_aliases = config.get_all_configured_rooms()
        rooms = resolve_room_aliases(list(all_room_aliases))
        return AgentBot(agent_user, storage_path, config, rooms, enable_streaming=enable_streaming)

    if entity_name in config.teams:
        team_config = config.teams[entity_name]
        rooms = resolve_room_aliases(team_config.rooms)
        # Convert agent names to MatrixID objects
        team_matrix_ids = [MatrixID.from_username(agent_name, config.domain) for agent_name in team_config.agents]
        return TeamBot(
            agent_user=agent_user,
            storage_path=storage_path,
            config=config,
            rooms=rooms,
            team_agents=team_matrix_ids,
            team_mode=team_config.mode,
            team_model=team_config.model,
            enable_streaming=True,
        )

    if entity_name in config.agents:
        agent_config = config.agents[entity_name]
        rooms = resolve_room_aliases(agent_config.rooms)
        return AgentBot(agent_user, storage_path, config, rooms, enable_streaming=enable_streaming)

    msg = f"Entity '{entity_name}' not found in configuration."
    raise ValueError(msg)


@dataclass
class MessageContext:
    """Context extracted from a Matrix message event."""

    am_i_mentioned: bool
    is_thread: bool
    thread_id: str | None
    thread_history: list[dict]
    mentioned_agents: list[MatrixID]


@dataclass
class AgentBot:
    """Represents a single agent bot with its own Matrix account."""

    agent_user: AgentMatrixUser
    storage_path: Path
    config: Config
    rooms: list[str] = field(default_factory=list)

    client: nio.AsyncClient | None = field(default=None, init=False)
    running: bool = field(default=False, init=False)
    enable_streaming: bool = field(default=True)  # Enable/disable streaming responses
    orchestrator: MultiAgentOrchestrator | None = field(default=None, init=False)  # Reference to orchestrator

    @property
    def agent_name(self) -> str:
        """Get the agent name from username."""
        return self.agent_user.agent_name

    @cached_property
    def logger(self) -> structlog.stdlib.BoundLogger:
        """Get a logger with agent context bound."""
        return logger.bind(agent=emoji(self.agent_name))

    @cached_property
    def matrix_id(self) -> MatrixID:
        """Get the Matrix ID for this agent bot."""
        return self.agent_user.matrix_id

    def _get_shared_knowledge(self, base_id: str) -> Knowledge | None:
        """Get shared knowledge instance for a configured knowledge base."""
        orchestrator = self.orchestrator
        if orchestrator is None:
            return None
        manager = orchestrator.knowledge_managers.get(base_id)
        if manager is None:
            return None
        return manager.get_knowledge()

    def _knowledge_for_agent(self, agent_name: str) -> Knowledge | None:
        """Return shared knowledge for agents assigned to a knowledge base."""
        agent_config = self.config.agents.get(agent_name)
        if agent_config is None or agent_config.knowledge_base is None:
            return None

        knowledge = self._get_shared_knowledge(agent_config.knowledge_base)
        if knowledge is None:
            self.logger.warning(
                "Knowledge base not available for agent",
                agent_name=agent_name,
                knowledge_base=agent_config.knowledge_base,
            )
            return None
        return knowledge

    @property  # Not cached_property because Team mutates it!
    def agent(self) -> Agent:
        """Get the Agno Agent instance for this bot."""
        knowledge = self._knowledge_for_agent(self.agent_name)
        return create_agent(
            agent_name=self.agent_name,
            config=self.config,
            storage_path=self.storage_path,
            knowledge=knowledge,
        )

    @cached_property
    def response_tracker(self) -> ResponseTracker:
        """Get or create the response tracker for this agent."""
        # Use the tracking subdirectory, not the root storage path
        tracking_dir = self.storage_path / "tracking"
        return ResponseTracker(self.agent_name, base_path=tracking_dir)

    @cached_property
    def stop_manager(self) -> StopManager:
        """Get or create the StopManager for this agent."""
        return StopManager()

    async def join_configured_rooms(self) -> None:
        """Join all rooms this agent is configured for."""
        assert self.client is not None
        for room_id in self.rooms:
            if await join_room(self.client, room_id):
                self.logger.info("Joined room", room_id=room_id)
                # Only the router agent should restore scheduled tasks
                # to avoid duplicate task instances after restart
                if self.agent_name == ROUTER_AGENT_NAME:
                    # Restore scheduled tasks
                    restored_tasks = await restore_scheduled_tasks(self.client, room_id, self.config)
                    if restored_tasks > 0:
                        self.logger.info(f"Restored {restored_tasks} scheduled tasks in room {room_id}")

                    # Restore pending config confirmations
                    restored_configs = await config_confirmation.restore_pending_changes(self.client, room_id)
                    if restored_configs > 0:
                        self.logger.info(f"Restored {restored_configs} pending config changes in room {room_id}")

                    # Send welcome message if room is empty
                    await self._send_welcome_message_if_empty(room_id)
            else:
                self.logger.warning("Failed to join room", room_id=room_id)

    async def leave_unconfigured_rooms(self) -> None:
        """Leave any rooms this agent is no longer configured for."""
        assert self.client is not None

        # Get all rooms we're currently in
        joined_rooms = await get_joined_rooms(self.client)
        if joined_rooms is None:
            return

        current_rooms = set(joined_rooms)
        configured_rooms = set(self.rooms)

        # Leave rooms we're no longer configured for (preserving DM rooms)
        await leave_non_dm_rooms(self.client, list(current_rooms - configured_rooms))

    async def ensure_user_account(self) -> None:
        """Ensure this agent has a Matrix user account.

        This method makes the agent responsible for its own user account creation,
        moving this responsibility from the orchestrator to the agent itself.
        """
        # If we already have a user_id (e.g., provided by tests or config), assume account exists
        if getattr(self.agent_user, "user_id", ""):
            return
        # Create or retrieve the Matrix user account
        self.agent_user = await create_agent_user(
            MATRIX_HOMESERVER,
            self.agent_name,
            self.agent_user.display_name,  # Use existing display name if available
        )
        self.logger.info(f"Ensured Matrix user account: {self.agent_user.user_id}")

    async def _set_avatar_if_available(self) -> None:
        """Set avatar for the agent if an avatar file exists."""
        if not self.client:
            return

        entity_type = "teams" if self.agent_name in self.config.teams else "agents"
        avatar_path = Path(__file__).parent.parent.parent / "avatars" / entity_type / f"{self.agent_name}.png"

        if avatar_path.exists():
            try:
                success = await check_and_set_avatar(self.client, avatar_path)
                if success:
                    self.logger.info(f"Successfully set avatar for {self.agent_name}")
                else:
                    self.logger.warning(f"Failed to set avatar for {self.agent_name}")
            except Exception as e:
                self.logger.warning(f"Failed to set avatar: {e}")

    async def _set_presence_with_model_info(self) -> None:
        """Set presence status with model information."""
        if self.client is None:
            return

        status_msg = build_agent_status_message(self.agent_name, self.config)
        await set_presence_status(self.client, status_msg)

    async def ensure_rooms(self) -> None:
        """Ensure agent is in the correct rooms based on configuration.

        This consolidates room management into a single method that:
        1. Joins configured rooms
        2. Leaves unconfigured rooms
        """
        await self.join_configured_rooms()
        await self.leave_unconfigured_rooms()

    async def start(self) -> None:
        """Start the agent bot with user account setup (but don't join rooms yet)."""
        await self.ensure_user_account()
        self.client = await login_agent_user(MATRIX_HOMESERVER, self.agent_user)
        await self._set_avatar_if_available()
        await self._set_presence_with_model_info()

        # Register event callbacks - wrap them to run as background tasks
        # This ensures the sync loop is never blocked, allowing stop reactions to work
        self.client.add_event_callback(_create_task_wrapper(self._on_invite), nio.InviteEvent)
        self.client.add_event_callback(_create_task_wrapper(self._on_message), nio.RoomMessageText)
        self.client.add_event_callback(_create_task_wrapper(self._on_reaction), nio.ReactionEvent)

        # Register voice message callbacks (only for router agent to avoid duplicates)
        if self.agent_name == ROUTER_AGENT_NAME:
            self.client.add_event_callback(_create_task_wrapper(self._on_voice_message), nio.RoomMessageAudio)
            self.client.add_event_callback(_create_task_wrapper(self._on_voice_message), nio.RoomEncryptedAudio)

        self.running = True

        # Router bot has additional responsibilities
        if self.agent_name == ROUTER_AGENT_NAME:
            try:
                await cleanup_all_orphaned_bots(self.client, self.config)
            except Exception as e:
                self.logger.warning(f"Could not cleanup orphaned bots (non-critical): {e}")

        # Note: Room joining is deferred until after invitations are handled
        self.logger.info(f"Agent setup complete: {self.agent_user.user_id}")

    async def try_start(self) -> bool:
        """Try to start the agent bot with smart retry logic.

        Uses tenacity to retry transient failures (network, timeouts) but not
        permanent ones (auth failures).

        Returns:
            True if the bot started successfully, False otherwise.

        """

        def should_retry_error(retry_state: RetryCallState) -> bool:
            """Determine if we should retry based on the exception.

            Don't retry on auth failures (M_FORBIDDEN, M_USER_DEACTIVATED, etc)
            which come as ValueError with those strings in the message.
            """
            if retry_state.outcome is None:
                return True
            exception = retry_state.outcome.exception()
            if exception is None:
                return False

            # Don't retry auth failures
            if isinstance(exception, ValueError):
                error_msg = str(exception)
                # Matrix auth error codes that shouldn't be retried
                permanent_errors = ["M_FORBIDDEN", "M_USER_DEACTIVATED", "M_UNKNOWN_TOKEN", "M_INVALID_USERNAME"]
                return not any(err in error_msg for err in permanent_errors)

            # Retry other exceptions (network errors, timeouts, etc)
            return True

        @retry(
            stop=stop_after_attempt(3),
            wait=wait_exponential(multiplier=1, min=2, max=10),
            retry=should_retry_error,
            reraise=True,
        )
        async def _start_with_retry() -> None:
            await self.start()

        try:
            await _start_with_retry()
            return True  # noqa: TRY300
        except Exception:
            logger.exception(f"Failed to start agent {self.agent_name}")
            return False

    async def cleanup(self) -> None:
        """Clean up the agent by leaving all rooms and stopping.

        This method ensures clean shutdown when an agent is removed from config.
        """
        assert self.client is not None
        # Leave all rooms (preserving DM rooms)
        try:
            joined_rooms = await get_joined_rooms(self.client)
            if joined_rooms:
                await leave_non_dm_rooms(self.client, joined_rooms)
        except Exception:
            self.logger.exception("Error leaving rooms during cleanup")

        # Stop the bot
        await self.stop()

    async def stop(self) -> None:
        """Stop the agent bot."""
        self.running = False

        # Wait for any pending background tasks (like memory saves) to complete
        try:
            await wait_for_background_tasks(timeout=5.0)  # 5 second timeout
            self.logger.info("Background tasks completed")
        except Exception as e:
            self.logger.warning(f"Some background tasks did not complete: {e}")

        if self.client is not None:
            self.logger.warning("Client is not None in stop()")
            await self.client.close()
        self.logger.info("Stopped agent bot")

    async def _send_welcome_message_if_empty(self, room_id: str) -> None:
        """Send a welcome message if the room has no messages yet.

        Only called by the router agent when joining a room.
        """
        assert self.client is not None

        # Check if room has any messages
        response = await self.client.room_messages(
            room_id,
            limit=2,  # Get 2 messages to check if we already sent welcome
            message_filter={"types": ["m.room.message"]},
        )

        # nio returns error types on failure - this is necessary
        if not isinstance(response, nio.RoomMessagesResponse):
            self.logger.error("Failed to check room messages", room_id=room_id, error=str(response))
            return

        # Only send welcome message if room is empty or only has our own welcome message
        if not response.chunk:
            # Room is completely empty
            self.logger.info("Room is empty, sending welcome message", room_id=room_id)

            # Generate and send the welcome message
            welcome_msg = _generate_welcome_message(room_id, self.config)
            await self._send_response(
                room_id=room_id,
                reply_to_event_id=None,
                response_text=welcome_msg,
                thread_id=None,
                skip_mentions=True,
            )
            self.logger.info("Welcome message sent", room_id=room_id)
        elif len(response.chunk) == 1:
            # Check if the only message is our welcome message
            msg = response.chunk[0]
            if (
                hasattr(msg, "sender")
                and msg.sender == self.agent_user.user_id
                and hasattr(msg, "body")
                and "Welcome to MindRoom" in msg.body
            ):
                self.logger.debug("Welcome message already sent", room_id=room_id)
                return
            # Otherwise, room has a different message, don't send welcome
        # Room has other messages, don't send welcome

    async def sync_forever(self) -> None:
        """Run the sync loop for this agent."""
        assert self.client is not None
        await self.client.sync_forever(timeout=SYNC_TIMEOUT_MS, full_state=True)

    async def _on_invite(self, room: nio.MatrixRoom, event: nio.InviteEvent) -> None:
        assert self.client is not None
        self.logger.info("Received invite", room_id=room.room_id, sender=event.sender)
        if await join_room(self.client, room.room_id):
            self.logger.info("Joined room", room_id=room.room_id)
            # If this is the router agent and the room is empty, send a welcome message
            if self.agent_name == ROUTER_AGENT_NAME:
                await self._send_welcome_message_if_empty(room.room_id)
        else:
            self.logger.error("Failed to join room", room_id=room.room_id)

    async def _on_message(self, room: nio.MatrixRoom, event: nio.RoomMessageText) -> None:  # noqa: C901, PLR0911, PLR0912
        self.logger.info("Received message", event_id=event.event_id, room_id=room.room_id, sender=event.sender)
        assert self.client is not None
        if event.body.endswith(IN_PROGRESS_MARKER):
            return

        # Skip our own messages (unless voice transcription from router)
        if event.sender == self.matrix_id.full_id and not event.body.startswith(VOICE_PREFIX):
            return

        event_info = EventInfo.from_event(event.source)

        # Check if sender is authorized to interact with agents
        is_authorized = is_authorized_sender(event.sender, self.config, room.room_id)
        self.logger.debug(
            f"Authorization check for {event.sender}: authorized={is_authorized}, room={room.room_id}",
        )
        if not is_authorized:
            # Mark as seen even though we're not responding (prevents reprocessing after permission changes)
            # Only mark non-edit events as responded
            if not event_info.is_edit:
                self.response_tracker.mark_responded(event.event_id)
            self.logger.debug(f"Ignoring message from unauthorized sender: {event.sender}")
            return

        # Handle edit events
        if event_info.is_edit:
            await self._handle_message_edit(room, event, event_info)
            return

        # Check if we've already seen this message (prevents reprocessing after restart)
        if self.response_tracker.has_responded(event.event_id):
            self.logger.debug("Already seen message", event_id=event.event_id)
            return

        # We only receive events from rooms we're in - no need to check access
        _is_dm_room = await is_dm_room(self.client, room.room_id)

        await interactive.handle_text_response(self.client, room, event, self.agent_name)

        # Router handles commands exclusively
        command = command_parser.parse(event.body)
        if command:
            if self.agent_name == ROUTER_AGENT_NAME:
                # Router always handles commands, even in single-agent rooms
                # Commands like !schedule, !help, etc. need to work regardless
                await self._handle_command(room, event, command)
            return

        context = await self._extract_message_context(room, event)

        # Check if the sender is an agent
        sender_agent_name = extract_agent_name(event.sender, self.config)

        # Skip messages from agents unless:
        # 1. We're mentioned
        # 2. It's a voice transcription from router (treated as user message)
        is_router_voice = sender_agent_name == ROUTER_AGENT_NAME and event.body.startswith(VOICE_PREFIX)
        if sender_agent_name and not context.am_i_mentioned and not is_router_voice:
            self.logger.debug("Ignoring message from other agent (not mentioned)")
            return

        # Get agents in thread (excludes router)
        agents_in_thread = get_agents_in_thread(context.thread_history, self.config)

        # Router: Route when no specific agent mentioned and no agents in thread
        if self.agent_name == ROUTER_AGENT_NAME:
            # Only perform AI routing when:
            # 1. No specific agent is mentioned
            # 2. No agents are already in the thread
            # 3. There's more than one agent available (routing makes sense)
            if not context.mentioned_agents and not agents_in_thread:
                available_agents = get_available_agents_in_room(room, self.config)
                if len(available_agents) == 1:
                    # Skip routing in single-agent rooms - the agent will handle it directly
                    self.logger.info("Skipping routing: only one agent present")
                else:
                    # Multiple agents available - perform AI routing
                    await self._handle_ai_routing(room, event, context.thread_history)
            # Router's job is done after routing/command handling/voice transcription
            return

        # Check for team formation
        all_mentioned_in_thread = get_all_mentioned_agents_in_thread(context.thread_history, self.config)
        form_team = await decide_team_formation(
            self.matrix_id,
            context.mentioned_agents,
            agents_in_thread,
            all_mentioned_in_thread,
            room=room,
            message=event.body,
            config=self.config,
            is_dm_room=_is_dm_room,
            is_thread=context.is_thread,
        )

        # Handle team formation (only first agent alphabetically)
        if form_team.should_form_team and self.matrix_id in form_team.agents:
            # Determine if this agent should lead the team response
            # Use the same ordering as decide_team_formation (by full_id)
            first_agent = min(form_team.agents, key=lambda x: x.full_id)
            if self.matrix_id != first_agent:
                return

            # Use the shared team response helper
            response_event_id = await self._generate_team_response_helper(
                room_id=room.room_id,
                reply_to_event_id=event.event_id,
                thread_id=context.thread_id,
                message=event.body,
                team_agents=form_team.agents,
                team_mode=form_team.mode,
                thread_history=context.thread_history,
                requester_user_id=event.sender,
                existing_event_id=None,
            )

            self.response_tracker.mark_responded(event.event_id, response_event_id)
            return

        # Check if we should respond individually
        should_respond = should_agent_respond(
            agent_name=self.agent_name,
            am_i_mentioned=context.am_i_mentioned,
            is_thread=context.is_thread,
            room=room,
            thread_history=context.thread_history,
            config=self.config,
            mentioned_agents=context.mentioned_agents,
        )

        if not should_respond:
            return

        # Log if responding without mention
        if not context.am_i_mentioned:
            self.logger.info("Will respond: only agent in thread")

        # Generate and send response
        self.logger.info("Processing", event_id=event.event_id)
        response_event_id = await self._generate_response(
            room_id=room.room_id,
            prompt=event.body,
            reply_to_event_id=event.event_id,
            thread_id=context.thread_id,
            thread_history=context.thread_history,
            user_id=event.sender,
        )
        self.response_tracker.mark_responded(event.event_id, response_event_id)

    async def _on_reaction(self, room: nio.MatrixRoom, event: nio.ReactionEvent) -> None:
        """Handle reaction events for interactive questions, stop functionality, and config confirmations."""
        assert self.client is not None

        # Check if sender is authorized to interact with agents
        if not is_authorized_sender(event.sender, self.config, room.room_id):
            self.logger.debug(f"Ignoring reaction from unauthorized sender: {event.sender}")
            return

        # Check if this is a stop button reaction for a message currently being generated
        # Only process stop functionality if:
        # 1. The reaction is ðŸ›‘
        # 2. The sender is not an agent (users only)
        # 3. The message is currently being generated by this agent
        if event.key == "ðŸ›‘":
            # Check if this is from a bot/agent
            sender_agent_name = extract_agent_name(event.sender, self.config)
            # Only handle stop from users, not agents, and only if tracking this message
            if not sender_agent_name and await self.stop_manager.handle_stop_reaction(event.reacts_to):
                self.logger.info(
                    "Stopped generation for message",
                    message_id=event.reacts_to,
                    stopped_by=event.sender,
                )
                # Remove the stop button immediately for user feedback
                await self.stop_manager.remove_stop_button(self.client, event.reacts_to)
                # Send a confirmation message
                await self._send_response(room.room_id, event.reacts_to, "âœ… Generation stopped", None)
                return
            # Message is not being generated - let the reaction be handled for other purposes
            # (e.g., interactive questions). Don't return here so it can fall through!
            # Agent reactions with ðŸ›‘ also fall through to other handlers

        # Then check if this is a config confirmation reaction
        pending_change = config_confirmation.get_pending_change(event.reacts_to)

        if pending_change and self.agent_name == ROUTER_AGENT_NAME:
            # Only router handles config confirmations
            await config_confirmation.handle_confirmation_reaction(self, room, event, pending_change)
            return

        # Otherwise handle as interactive question
        result = await interactive.handle_reaction(self.client, event, self.agent_name, self.config)

        if result:
            selected_value, thread_id = result
            # User selected an option from an interactive question

            # Check if we should process this reaction
            thread_history = []
            if thread_id:
                thread_history = await fetch_thread_history(self.client, room.room_id, thread_id)
                if has_user_responded_after_message(thread_history, event.reacts_to, self.matrix_id):
                    self.logger.info(
                        "Ignoring reaction - agent already responded after this question",
                        reacted_to=event.reacts_to,
                    )
                    return

            # Send immediate acknowledgment
            ack_text = f"You selected: {event.key} {selected_value}\n\nProcessing your response..."
            # Matrix doesn't allow reply relations to events that already have relations (reactions)
            # In threads, omit reply_to_event_id; the thread_id ensures correct placement
            ack_event_id = await self._send_response(
                room.room_id,
                None if thread_id else event.reacts_to,
                ack_text,
                thread_id,
            )

            if not ack_event_id:
                self.logger.error("Failed to send acknowledgment for reaction")
                return

            # Generate the response, editing the acknowledgment message
            # Note: existing_event_id is only used for interactive questions to edit the acknowledgment
            prompt = f"The user selected: {selected_value}"
            response_event_id = await self._generate_response(
                room_id=room.room_id,
                prompt=prompt,
                reply_to_event_id=event.reacts_to,
                thread_id=thread_id,
                thread_history=thread_history,
                existing_event_id=ack_event_id,  # Edit the acknowledgment instead of creating new message
                user_id=event.sender,
            )
            # Mark the original interactive question as responded
            self.response_tracker.mark_responded(event.reacts_to, response_event_id)

    async def _on_voice_message(
        self,
        room: nio.MatrixRoom,
        event: nio.RoomMessageAudio | nio.RoomEncryptedAudio,
    ) -> None:
        """Handle voice message events for transcription and processing."""
        # Only process if voice handler is enabled
        if not self.config.voice.enabled:
            return

        # Don't process our own voice messages
        if event.sender == self.matrix_id.full_id:
            return

        # Check if we've already seen this voice message (prevents reprocessing after restart)
        if self.response_tracker.has_responded(event.event_id):
            self.logger.debug("Already processed voice message", event_id=event.event_id)
            return

        # Check if sender is authorized to interact with agents
        if not is_authorized_sender(event.sender, self.config, room.room_id):
            # Mark as seen even though we're not responding
            self.response_tracker.mark_responded(event.event_id)
            self.logger.debug(f"Ignoring voice message from unauthorized sender: {event.sender}")
            return

        self.logger.info("Processing voice message", event_id=event.event_id, sender=event.sender)

        transcribed_message = await voice_handler.handle_voice_message(self.client, room, event, self.config)

        if transcribed_message:
            event_info = EventInfo.from_event(event.source)
            response_event_id = await self._send_response(
                room_id=room.room_id,
                reply_to_event_id=event.event_id,
                response_text=transcribed_message,
                thread_id=event_info.thread_id,
            )
            self.response_tracker.mark_responded(event.event_id, response_event_id)
        else:
            # Mark as responded to avoid reprocessing
            self.response_tracker.mark_responded(event.event_id)

    async def _extract_message_context(self, room: nio.MatrixRoom, event: nio.RoomMessageText) -> MessageContext:
        assert self.client is not None

        # Check if mentions should be ignored for this message
        skip_mentions = _should_skip_mentions(event.source)

        if skip_mentions:
            # Don't detect mentions if the message has skip_mentions metadata
            mentioned_agents: list[MatrixID] = []
            am_i_mentioned = False
        else:
            mentioned_agents, am_i_mentioned = check_agent_mentioned(event.source, self.matrix_id, self.config)

        if am_i_mentioned:
            self.logger.info("Mentioned", event_id=event.event_id, room_name=room.name)

        event_info = EventInfo.from_event(event.source)

        thread_history = []
        if event_info.thread_id:
            thread_history = await fetch_thread_history(self.client, room.room_id, event_info.thread_id)

        return MessageContext(
            am_i_mentioned=am_i_mentioned,
            is_thread=event_info.is_thread,
            thread_id=event_info.thread_id,
            thread_history=thread_history,
            mentioned_agents=mentioned_agents,
        )

    async def _generate_team_response_helper(
        self,
        room_id: str,
        reply_to_event_id: str,
        thread_id: str | None,
        message: str,
        team_agents: list[MatrixID],
        team_mode: str,
        thread_history: list[dict],
        requester_user_id: str,
        existing_event_id: str | None = None,
    ) -> str | None:
        """Generate a team response (shared between preformed teams and TeamBot).

        Returns the initial message ID if created, None otherwise.
        """
        assert self.client is not None

        # Get the appropriate model for this team and room
        model_name = select_model_for_team(self.agent_name, room_id, self.config)

        # Decide streaming based on presence
        use_streaming = self.enable_streaming and await should_use_streaming(
            self.client,
            room_id,
            requester_user_id=requester_user_id,
        )

        # Convert mode string to TeamMode enum
        mode = TeamMode.COORDINATE if team_mode == "coordinate" else TeamMode.COLLABORATE

        # Convert MatrixID list to agent names for non-streaming APIs
        agent_names = [mid.agent_name(self.config) or mid.username for mid in team_agents]
        orchestrator = self.orchestrator
        if orchestrator is None:
            msg = "Orchestrator is not set"
            raise RuntimeError(msg)

        # Create async function for team response generation that takes message_id as parameter
        async def generate_team_response(message_id: str | None) -> None:
            if use_streaming and not existing_event_id:
                # Show typing indicator while team generates streaming response
                async with typing_indicator(self.client, room_id):
                    response_stream = team_response_stream(
                        agent_ids=team_agents,
                        message=message,
                        orchestrator=orchestrator,
                        mode=mode,
                        thread_history=thread_history,
                        model_name=model_name,
                    )

                    event_id, accumulated = await send_streaming_response(
                        self.client,
                        room_id,
                        reply_to_event_id,
                        thread_id,
                        self.matrix_id.domain,
                        self.config,
                        response_stream,
                        streaming_cls=ReplacementStreamingResponse,
                        header=None,
                        existing_event_id=message_id,
                    )

                # Handle interactive questions in team responses
                await self._handle_interactive_question(
                    event_id,
                    accumulated,
                    room_id,
                    thread_id,
                    reply_to_event_id,
                    agent_name="team",
                )
            else:
                # Show typing indicator while team generates non-streaming response
                async with typing_indicator(self.client, room_id):
                    response_text = await team_response(
                        agent_names=agent_names,
                        mode=mode,
                        message=message,
                        orchestrator=orchestrator,
                        thread_history=thread_history,
                        model_name=model_name,
                    )

                # Either edit the thinking message or send new
                if message_id:
                    await self._edit_message(room_id, message_id, response_text, thread_id)
                else:
                    assert self.client is not None
                    event_id = await self._send_response(
                        room_id,
                        reply_to_event_id,
                        response_text,
                        thread_id,
                    )
                    # Handle interactive questions in non-streaming team responses
                    if event_id:
                        await self._handle_interactive_question(
                            event_id,
                            response_text,
                            room_id,
                            thread_id,
                            reply_to_event_id,
                            agent_name="team",
                        )

        # Use unified handler for cancellation support
        # Always send thinking message unless we're editing an existing message
        thinking_msg = None
        if not existing_event_id:
            thinking_msg = "ðŸ¤ Team Response: Thinking..."

        return await self._run_cancellable_response(
            room_id=room_id,
            reply_to_event_id=reply_to_event_id,
            thread_id=thread_id,
            response_function=generate_team_response,
            thinking_message=thinking_msg,
            existing_event_id=existing_event_id,
            user_id=requester_user_id,
        )

    async def _run_cancellable_response(
        self,
        room_id: str,
        reply_to_event_id: str,
        thread_id: str | None,
        response_function: object,  # Function that generates the response (takes message_id)
        thinking_message: str | None = None,  # None means don't send thinking message
        existing_event_id: str | None = None,
        user_id: str | None = None,  # User ID for presence check
    ) -> str | None:
        """Run a response generation function with cancellation support.

        This unified handler provides:
        - Optional "Thinking..." message
        - Task cancellation via stop button (when user is online)
        - Proper cleanup on completion or cancellation

        Args:
            room_id: The room to send to
            reply_to_event_id: Event to reply to
            thread_id: Thread ID if in thread
            response_function: Async function that generates the response (takes message_id parameter)
            thinking_message: Thinking message to show (only used when existing_event_id is None)
            existing_event_id: ID of existing message to edit (for interactive questions)
            user_id: User ID for checking if they're online (for stop button decision)

        Returns:
            The initial message ID if created, None otherwise

        Note: In practice, either thinking_message or existing_event_id is provided, never both.

        """
        assert self.client is not None

        # Validate the mutual exclusivity constraint
        assert not (thinking_message and existing_event_id), (
            "thinking_message and existing_event_id are mutually exclusive"
        )

        # Send initial thinking message if not editing an existing message
        initial_message_id = None
        if thinking_message:
            assert not existing_event_id  # Redundant but makes the logic clear
            initial_message_id = await self._send_response(
                room_id,
                reply_to_event_id,
                f"{thinking_message} {IN_PROGRESS_MARKER}",
                thread_id,
            )

        # Determine which message ID to use
        message_id = existing_event_id or initial_message_id

        # Create cancellable task by calling the function with the message ID
        task: asyncio.Task[None] = asyncio.create_task(response_function(message_id))  # type: ignore[operator]

        # Track for stop button (only if we have a message to track)
        message_to_track = existing_event_id or initial_message_id
        show_stop_button = False  # Default to not showing

        if message_to_track:
            self.stop_manager.set_current(message_to_track, room_id, task, None)

            # Add stop button if configured AND user is online
            # This uses the same logic as streaming to determine if user is online
            show_stop_button = self.config.defaults.show_stop_button
            if show_stop_button and user_id:
                # Check if user is online - same logic as streaming decision
                user_is_online = await is_user_online(self.client, user_id)
                show_stop_button = user_is_online
                self.logger.info(
                    "Stop button decision",
                    message_id=message_to_track,
                    user_online=user_is_online,
                    show_button=show_stop_button,
                )

            if show_stop_button:
                self.logger.info("Adding stop button", message_id=message_to_track)
                await self.stop_manager.add_stop_button(self.client, room_id, message_to_track)

        try:
            await task
        except asyncio.CancelledError:
            self.logger.info("Response cancelled by user", message_id=message_to_track)
        except Exception as e:
            self.logger.exception("Error during response generation", error=str(e))
            raise
        finally:
            if message_to_track:
                tracked = self.stop_manager.tracked_messages.get(message_to_track)
                button_already_removed = tracked is None or tracked.reaction_event_id is None

                self.stop_manager.clear_message(
                    message_to_track,
                    client=self.client,
                    remove_button=show_stop_button and not button_already_removed,
                )

        return initial_message_id

    async def _process_and_respond(
        self,
        room_id: str,
        prompt: str,
        reply_to_event_id: str,
        thread_id: str | None,
        thread_history: list[dict],
        existing_event_id: str | None = None,
        user_id: str | None = None,
    ) -> str | None:
        """Process a message and send a response (non-streaming)."""
        if not prompt.strip():
            return None

        session_id = create_session_id(room_id, thread_id)
        knowledge = self._knowledge_for_agent(self.agent_name)

        try:
            # Show typing indicator while generating response
            async with typing_indicator(self.client, room_id):
                response_text = await ai_response(
                    agent_name=self.agent_name,
                    prompt=prompt,
                    session_id=session_id,
                    storage_path=self.storage_path,
                    config=self.config,
                    thread_history=thread_history,
                    room_id=room_id,
                    knowledge=knowledge,
                    user_id=user_id,
                )
        except asyncio.CancelledError:
            # Handle cancellation - send a message showing it was stopped
            self.logger.info("Non-streaming response cancelled by user", message_id=existing_event_id)
            if existing_event_id:
                cancelled_text = "**[Response cancelled by user]**"
                await self._edit_message(room_id, existing_event_id, cancelled_text, thread_id)
            raise
        except Exception as e:
            self.logger.exception("Error in non-streaming response", error=str(e))
            raise

        if existing_event_id:
            # Edit the existing message
            await self._edit_message(room_id, existing_event_id, response_text, thread_id)
            return existing_event_id

        response = interactive.parse_and_format_interactive(response_text, extract_mapping=True)
        event_id = await self._send_response(room_id, reply_to_event_id, response.formatted_text, thread_id)
        if event_id and response.option_map and response.options_list:
            # For interactive questions, use the same thread root that _send_response uses:
            # - If already in a thread, use that thread_id
            # - If not in a thread, use reply_to_event_id (the user's message) as thread root
            # This ensures consistency with how the bot creates threads
            thread_root_for_registration = thread_id if thread_id else reply_to_event_id
            interactive.register_interactive_question(
                event_id,
                room_id,
                thread_root_for_registration,
                response.option_map,
                self.agent_name,
            )
            await interactive.add_reaction_buttons(self.client, room_id, event_id, response.options_list)

        return event_id

    async def _send_skill_command_response(
        self,
        *,
        room_id: str,
        reply_to_event_id: str,
        thread_id: str | None,
        thread_history: list[dict],
        prompt: str,
        agent_name: str,
        user_id: str | None,
        reply_to_event: nio.RoomMessageText | None = None,
    ) -> str | None:
        """Send a skill command response using a specific agent."""
        assert self.client is not None
        if not prompt.strip():
            return None

        session_id = create_session_id(room_id, thread_id)
        knowledge = self._knowledge_for_agent(agent_name)

        async with typing_indicator(self.client, room_id):
            response_text = await ai_response(
                agent_name=agent_name,
                prompt=prompt,
                session_id=session_id,
                storage_path=self.storage_path,
                config=self.config,
                thread_history=thread_history,
                room_id=room_id,
                knowledge=knowledge,
            )

        response = interactive.parse_and_format_interactive(response_text, extract_mapping=True)
        event_id = await self._send_response(
            room_id,
            reply_to_event_id,
            response.formatted_text,
            thread_id,
            reply_to_event=reply_to_event,
            skip_mentions=True,
        )

        if event_id and response.option_map and response.options_list:
            thread_root_for_registration = thread_id if thread_id else reply_to_event_id
            interactive.register_interactive_question(
                event_id,
                room_id,
                thread_root_for_registration,
                response.option_map,
                agent_name,
            )
            await interactive.add_reaction_buttons(
                self.client,
                room_id,
                event_id,
                response.options_list,
            )

        try:
            create_background_task(
                store_conversation_memory(
                    prompt,
                    agent_name,
                    self.storage_path,
                    session_id,
                    self.config,
                    room_id,
                    thread_history,
                    user_id,
                ),
                name=f"memory_save_{agent_name}_{session_id}",
            )
        except Exception:  # pragma: no cover
            self.logger.debug("Skipping memory storage due to configuration error")

        return event_id

    async def _handle_interactive_question(
        self,
        event_id: str | None,
        content: str,
        room_id: str,
        thread_id: str | None,
        reply_to_event_id: str,
        agent_name: str | None = None,
    ) -> None:
        """Handle interactive question registration and reactions if present.

        Args:
            event_id: The message event ID
            content: The message content to check for interactive questions
            room_id: The Matrix room ID
            thread_id: Thread ID if in a thread
            reply_to_event_id: Event being replied to
            agent_name: Name of agent (for registration)

        """
        if not event_id or not self.client:
            return

        if interactive.should_create_interactive_question(content):
            response = interactive.parse_and_format_interactive(content, extract_mapping=True)
            if response.option_map and response.options_list:
                thread_root_for_registration = thread_id if thread_id else reply_to_event_id
                interactive.register_interactive_question(
                    event_id,
                    room_id,
                    thread_root_for_registration,
                    response.option_map,
                    agent_name or self.agent_name,
                )
                await interactive.add_reaction_buttons(
                    self.client,
                    room_id,
                    event_id,
                    response.options_list,
                )

    async def _process_and_respond_streaming(
        self,
        room_id: str,
        prompt: str,
        reply_to_event_id: str,
        thread_id: str | None,
        thread_history: list[dict],
        existing_event_id: str | None = None,
        user_id: str | None = None,
    ) -> str | None:
        """Process a message and send a response (streaming)."""
        assert self.client is not None
        if not prompt.strip():
            return None

        session_id = create_session_id(room_id, thread_id)
        knowledge = self._knowledge_for_agent(self.agent_name)

        try:
            # Show typing indicator while generating response
            async with typing_indicator(self.client, room_id):
                response_stream = stream_agent_response(
                    agent_name=self.agent_name,
                    prompt=prompt,
                    session_id=session_id,
                    storage_path=self.storage_path,
                    config=self.config,
                    thread_history=thread_history,
                    room_id=room_id,
                    knowledge=knowledge,
                    user_id=user_id,
                )

                event_id, accumulated = await send_streaming_response(
                    self.client,
                    room_id,
                    reply_to_event_id,
                    thread_id,
                    self.matrix_id.domain,
                    self.config,
                    response_stream,
                    streaming_cls=StreamingResponse,
                    existing_event_id=existing_event_id,
                )

            # Handle interactive questions if present
            await self._handle_interactive_question(
                event_id,
                accumulated,
                room_id,
                thread_id,
                reply_to_event_id,
            )

        except asyncio.CancelledError:
            # Handle cancellation - send a message showing it was stopped
            self.logger.info("Streaming cancelled by user", message_id=existing_event_id)
            if existing_event_id:
                cancelled_text = "**[Response cancelled by user]**"
                await self._edit_message(room_id, existing_event_id, cancelled_text, thread_id)
            raise
        except Exception as e:
            self.logger.exception("Error in streaming response", error=str(e))
            # Don't mark as responded if streaming failed
            return None
        else:
            return event_id

    async def _generate_response(
        self,
        room_id: str,
        prompt: str,
        reply_to_event_id: str,
        thread_id: str | None,
        thread_history: list[dict],
        existing_event_id: str | None = None,
        user_id: str | None = None,
    ) -> str | None:
        """Generate and send/edit a response using AI.

        Args:
            room_id: The room to send the response to
            prompt: The prompt to send to the AI
            reply_to_event_id: The event to reply to
            thread_id: Thread ID if in a thread
            thread_history: Thread history for context
            existing_event_id: If provided, edit this message instead of sending a new one
                             (only used for interactive question responses)
            user_id: User ID of the sender for identifying user messages in history

        Returns:
            Event ID of the response message, or None if failed

        """
        assert self.client is not None

        # Prepare session id for memory storage (store after sending response)
        session_id = create_session_id(room_id, thread_id)

        # Dynamically determine whether to use streaming based on user presence
        # Only check presence if streaming is globally enabled
        use_streaming = self.enable_streaming
        if use_streaming:
            # Check if the user is online to decide whether to stream
            use_streaming = await should_use_streaming(self.client, room_id, requester_user_id=user_id)

        # Create async function for generation that takes message_id as parameter
        async def generate(message_id: str | None) -> None:
            if use_streaming:
                await self._process_and_respond_streaming(
                    room_id,
                    prompt,
                    reply_to_event_id,
                    thread_id,
                    thread_history,
                    message_id,  # Edit the thinking message or existing
                    user_id=user_id,
                )
            else:
                await self._process_and_respond(
                    room_id,
                    prompt,
                    reply_to_event_id,
                    thread_id,
                    thread_history,
                    message_id,  # Edit the thinking message or existing
                    user_id=user_id,
                )

        # Use unified handler for cancellation support
        # Always send "Thinking..." message unless we're editing an existing message
        thinking_msg = None
        if not existing_event_id:
            thinking_msg = "Thinking..."

        event_id = await self._run_cancellable_response(
            room_id=room_id,
            reply_to_event_id=reply_to_event_id,
            thread_id=thread_id,
            response_function=generate,
            thinking_message=thinking_msg,
            existing_event_id=existing_event_id,
            user_id=user_id,
        )

        # Store memory after response generation; ignore errors in tests/mocks
        # TODO: Remove try-except and fix tests
        try:
            create_background_task(
                store_conversation_memory(
                    prompt,
                    self.agent_name,
                    self.storage_path,
                    session_id,
                    self.config,
                    room_id,
                    thread_history,
                    user_id,
                ),
                name=f"memory_save_{self.agent_name}_{session_id}",
            )
        except Exception:  # pragma: no cover
            self.logger.debug("Skipping memory storage due to configuration error")

        return event_id

    async def _send_response(
        self,
        room_id: str,
        reply_to_event_id: str | None,
        response_text: str,
        thread_id: str | None,
        reply_to_event: nio.RoomMessageText | None = None,
        skip_mentions: bool = False,
    ) -> str | None:
        """Send a response message to a room.

        Args:
            room_id: The room id to send to
            reply_to_event_id: The event ID to reply to (can be None when in a thread)
            response_text: The text to send
            thread_id: The thread ID if already in a thread
            reply_to_event: Optional event object for the message we're replying to (used to check for safe thread root)
            skip_mentions: If True, add metadata to indicate mentions should not trigger responses

        Returns:
            Event ID if message was sent successfully, None otherwise.

        """
        sender_id = self.matrix_id
        sender_domain = sender_id.domain

        # Always ensure we have a thread_id - use the original message as thread root if needed
        # This ensures agents always respond in threads, even when mentioned in main room
        event_info = EventInfo.from_event(reply_to_event.source if reply_to_event else None)
        effective_thread_id = thread_id or event_info.safe_thread_root or reply_to_event_id

        # Get the latest message in thread for MSC3440 fallback compatibility
        latest_thread_event_id = await get_latest_thread_event_id_if_needed(
            self.client,
            room_id,
            effective_thread_id,
            reply_to_event_id,
        )

        content = format_message_with_mentions(
            self.config,
            response_text,
            sender_domain=sender_domain,
            thread_event_id=effective_thread_id,
            reply_to_event_id=reply_to_event_id,
            latest_thread_event_id=latest_thread_event_id,
        )

        # Add metadata to indicate mentions should be ignored for responses
        if skip_mentions:
            content["com.mindroom.skip_mentions"] = True

        assert self.client is not None
        event_id = await send_message(self.client, room_id, content)
        if event_id:
            self.logger.info("Sent response", event_id=event_id, room_id=room_id)
            return event_id
        self.logger.error("Failed to send response to room", room_id=room_id)
        return None

    async def _edit_message(self, room_id: str, event_id: str, new_text: str, thread_id: str | None) -> bool:
        """Edit an existing message.

        Returns:
            True if edit was successful, False otherwise.

        """
        sender_id = self.matrix_id
        sender_domain = sender_id.domain

        # For edits in threads, we need to get the latest thread event ID for MSC3440 compliance
        # When editing, we still need the latest thread event for the fallback behavior
        # So we fetch it directly rather than using get_latest_thread_event_id_if_needed
        latest_thread_event_id = None
        if thread_id:
            assert self.client is not None
            # For edits, we always need the latest thread event ID
            # We can use the event being edited as the fallback if we can't get the latest
            latest_thread_event_id = await _latest_thread_event_id(self.client, room_id, thread_id)
            # If we couldn't get the latest, use the event being edited as fallback
            if latest_thread_event_id is None:
                latest_thread_event_id = event_id

        content = format_message_with_mentions(
            self.config,
            new_text,
            sender_domain=sender_domain,
            thread_event_id=thread_id,
            latest_thread_event_id=latest_thread_event_id,
        )

        assert self.client is not None
        response = await edit_message(self.client, room_id, event_id, content, new_text)

        if isinstance(response, nio.RoomSendResponse):
            self.logger.info("Edited message", event_id=event_id)
            return True
        self.logger.error("Failed to edit message", event_id=event_id, error=str(response))
        return False

    async def _handle_ai_routing(
        self,
        room: nio.MatrixRoom,
        event: nio.RoomMessageText,
        thread_history: list[dict],
    ) -> None:
        # Only router agent should handle routing
        assert self.agent_name == ROUTER_AGENT_NAME

        # Use configured agents only - router should not suggest random agents
        available_agents = get_configured_agents_for_room(room.room_id, self.config)
        if not available_agents:
            self.logger.debug("No configured agents to route to in this room")
            return

        self.logger.info("Handling AI routing", event_id=event.event_id)

        event_info = EventInfo.from_event(event.source)
        suggested_agent = await suggest_agent_for_message(
            event.body,
            available_agents,
            self.config,
            thread_history,
        )

        if not suggested_agent:
            # Send error message when routing fails
            response_text = "âš ï¸ I couldn't determine which agent should help with this. Please try mentioning an agent directly with @ or rephrase your request."
            self.logger.warning("Router failed to determine agent")
        else:
            # Router mentions the suggested agent and asks them to help
            response_text = f"@{suggested_agent} could you help with this?"
        sender_id = self.matrix_id
        sender_domain = sender_id.domain

        # If no thread exists, create one with the original message as root
        thread_event_id = event_info.thread_id
        if not thread_event_id:
            # Check if the current event can be a thread root
            thread_event_id = event_info.safe_thread_root or event.event_id

        # Get latest thread event for MSC3440 compliance when no specific reply
        # Note: We use event.event_id as reply_to for routing suggestions
        latest_thread_event_id = await get_latest_thread_event_id_if_needed(
            self.client,
            room.room_id,
            thread_event_id,
            event.event_id,
        )

        content = format_message_with_mentions(
            self.config,
            response_text,
            sender_domain=sender_domain,
            thread_event_id=thread_event_id,
            reply_to_event_id=event.event_id,
            latest_thread_event_id=latest_thread_event_id,
        )

        assert self.client is not None
        event_id = await send_message(self.client, room.room_id, content)
        if event_id:
            self.logger.info("Routed to agent", suggested_agent=suggested_agent)
            self.response_tracker.mark_responded(event.event_id)
        else:
            self.logger.error("Failed to route to agent", agent=suggested_agent)

    async def _handle_message_edit(
        self,
        room: nio.MatrixRoom,
        event: nio.RoomMessageText,
        event_info: EventInfo,
    ) -> None:
        """Handle an edited message by regenerating the agent's response.

        Args:
            room: The Matrix room
            event: The edited message event
            event_info: Information about the edit event

        """
        if not event_info.original_event_id:
            self.logger.debug("Edit event has no original event ID")
            return

        # Skip edits from other agents
        sender_agent_name = extract_agent_name(event.sender, self.config)
        if sender_agent_name:
            self.logger.debug(f"Ignoring edit from other agent: {sender_agent_name}")
            return

        response_event_id = self.response_tracker.get_response_event_id(event_info.original_event_id)
        if not response_event_id:
            self.logger.debug(f"No previous response found for edited message {event_info.original_event_id}")
            return

        self.logger.info(
            "Regenerating response for edited message",
            original_event_id=event_info.original_event_id,
            response_event_id=response_event_id,
        )

        context = await self._extract_message_context(room, event)

        # Check if we should respond to the edited message
        # KNOWN LIMITATION: This doesn't work correctly for the router suggestion case.
        # When: User asks question â†’ Router suggests agent â†’ Agent responds â†’ User edits
        # The agent won't regenerate because it's not mentioned in the edited message.
        # Proper fix would require tracking response chains (user â†’ router â†’ agent).
        should_respond = should_agent_respond(
            agent_name=self.agent_name,
            am_i_mentioned=context.am_i_mentioned,
            is_thread=context.is_thread,
            room=room,
            thread_history=context.thread_history,
            config=self.config,
            mentioned_agents=context.mentioned_agents,
        )

        if not should_respond:
            self.logger.debug("Agent should not respond to edited message")
            return

        # These keys must be present according to MSC2676
        # https://github.com/matrix-org/matrix-spec-proposals/blob/main/proposals/2676-message-editing.md
        edited_content = event.source["content"]["m.new_content"]["body"]

        # Generate new response
        await self._generate_response(
            room_id=room.room_id,
            prompt=edited_content,
            reply_to_event_id=event_info.original_event_id,
            thread_id=context.thread_id,
            thread_history=context.thread_history,
            existing_event_id=response_event_id,
            user_id=event.sender,
        )

        # Update the response tracker
        self.response_tracker.mark_responded(event_info.original_event_id, response_event_id)
        self.logger.info("Successfully regenerated response for edited message")

    async def _handle_command(self, room: nio.MatrixRoom, event: nio.RoomMessageText, command: Command) -> None:  # noqa: C901, PLR0912, PLR0915
        self.logger.info("Handling command", command_type=command.type.value)

        event_info = EventInfo.from_event(event.source)

        # Widget command modifies room state, so it doesn't need a thread
        if command.type == CommandType.WIDGET:
            assert self.client is not None
            url = command.args.get("url")
            response_text = await handle_widget_command(client=self.client, room_id=room.room_id, url=url)
            # Send response in thread if in thread, otherwise in main room
            await self._send_response(room.room_id, event.event_id, response_text, event_info.thread_id)
            return

        # For commands that need thread context, use the existing thread or the event will start a new one
        # The _send_response method will automatically create a thread if needed
        effective_thread_id = event_info.thread_id or event_info.safe_thread_root or event.event_id

        response_text = ""

        if command.type == CommandType.HELP:
            topic = command.args.get("topic")
            response_text = get_command_help(topic)

        elif command.type == CommandType.HI:
            # Generate the welcome message for this room
            response_text = _generate_welcome_message(room.room_id, self.config)

        elif command.type == CommandType.SCHEDULE:
            full_text = command.args["full_text"]

            # Get mentioned agents from the command text
            mentioned_agents, _ = check_agent_mentioned(event.source, None, self.config)

            assert self.client is not None
            task_id, response_text = await schedule_task(
                client=self.client,
                room_id=room.room_id,
                thread_id=effective_thread_id,
                scheduled_by=event.sender,
                full_text=full_text,
                config=self.config,
                room=room,
                mentioned_agents=mentioned_agents,
            )

        elif command.type == CommandType.LIST_SCHEDULES:
            assert self.client is not None
            response_text = await list_scheduled_tasks(
                client=self.client,
                room_id=room.room_id,
                thread_id=effective_thread_id,
                config=self.config,
            )

        elif command.type == CommandType.CANCEL_SCHEDULE:
            assert self.client is not None
            cancel_all = command.args.get("cancel_all", False)

            if cancel_all:
                # Cancel all scheduled tasks
                response_text = await cancel_all_scheduled_tasks(
                    client=self.client,
                    room_id=room.room_id,
                )
            else:
                # Cancel specific task
                task_id = command.args["task_id"]
                response_text = await cancel_scheduled_task(
                    client=self.client,
                    room_id=room.room_id,
                    task_id=task_id,
                )

        elif command.type == CommandType.CONFIG:
            # Handle config command
            args_text = command.args.get("args_text", "")
            response_text, change_info = await handle_config_command(args_text)

            # If we have change_info, this is a config set that needs confirmation
            if change_info:
                # Send the preview message
                event_id = await self._send_response(
                    room.room_id,
                    event.event_id,
                    response_text,
                    event_info.thread_id,
                    reply_to_event=event,
                    skip_mentions=True,
                )

                if event_id:
                    # Register the pending change
                    config_confirmation.register_pending_change(
                        event_id=event_id,
                        room_id=room.room_id,
                        thread_id=event_info.thread_id,
                        config_path=change_info["config_path"],
                        old_value=change_info["old_value"],
                        new_value=change_info["new_value"],
                        requester=event.sender,
                    )

                    # Get the pending change we just registered
                    pending_change = config_confirmation.get_pending_change(event_id)

                    # Store in Matrix state for persistence
                    if pending_change:
                        await config_confirmation.store_pending_change_in_matrix(
                            self.client,
                            event_id,
                            pending_change,
                        )

                    # Add reaction buttons
                    await config_confirmation.add_confirmation_reactions(self.client, room.room_id, event_id)

                self.response_tracker.mark_responded(event.event_id)
                return  # Exit early since we've handled the response

        elif command.type == CommandType.SKILL:
            skill_name = command.args.get("skill_name")
            args_text = command.args.get("args_text", "")
            if not skill_name:
                response_text = "Usage: !skill <name> [args]"
            else:
                mentioned_agents, _ = check_agent_mentioned(event.source, None, self.config)
                target_agent, error = _resolve_skill_command_agent(
                    skill_name,
                    config=self.config,
                    room=room,
                    mentioned_agents=mentioned_agents,
                )
                if error:
                    response_text = error
                else:
                    assert target_agent is not None
                    spec = resolve_skill_command_spec(skill_name, self.config, target_agent)
                    if spec is None:
                        response_text = f"âŒ Skill '{skill_name}' not found or not enabled for agent '{target_agent}'."
                    elif not spec.user_invocable:
                        response_text = f"âŒ Skill '{spec.name}' is not user-invocable."
                    elif spec.dispatch and spec.dispatch.kind == "tool":
                        response_text = await _run_skill_command_tool(
                            config=self.config,
                            agent_name=target_agent,
                            command_tool=spec.dispatch.tool_name,
                            skill_name=spec.name,
                            args_text=args_text,
                        )
                    elif spec.disable_model_invocation:
                        response_text = (
                            f"âŒ Skill '{spec.name}' is configured to skip model invocation and has no tool dispatch."
                        )
                    else:
                        thread_history = []
                        if event_info.thread_id:
                            thread_history = await fetch_thread_history(
                                self.client,
                                room.room_id,
                                event_info.thread_id,
                            )
                        prompt = _build_skill_command_prompt(spec.name, args_text)
                        event_id = await self._send_skill_command_response(
                            room_id=room.room_id,
                            reply_to_event_id=event.event_id,
                            thread_id=effective_thread_id,
                            thread_history=thread_history,
                            prompt=prompt,
                            agent_name=target_agent,
                            user_id=event.sender,
                            reply_to_event=event,
                        )
                        if event_id:
                            self.response_tracker.mark_responded(event.event_id, event_id)
                        return

        elif command.type == CommandType.UNKNOWN:
            # Handle unknown commands
            response_text = "âŒ Unknown command. Try !help for available commands."

        if response_text:
            await self._send_response(
                room.room_id,
                event.event_id,
                response_text,
                event_info.thread_id,
                reply_to_event=event,
                skip_mentions=True,
            )
            self.response_tracker.mark_responded(event.event_id)


@dataclass
class TeamBot(AgentBot):
    """A bot that represents a team of agents working together."""

    team_agents: list[MatrixID] = field(default_factory=list)
    team_mode: str = field(default="coordinate")
    team_model: str | None = field(default=None)

    @cached_property
    def agent(self) -> Agent | None:  # type: ignore[override]
        """Teams don't have individual agents, return None."""
        return None

    async def _generate_response(
        self,
        room_id: str,
        prompt: str,
        reply_to_event_id: str,
        thread_id: str | None,
        thread_history: list[dict],
        existing_event_id: str | None = None,
        user_id: str | None = None,
    ) -> None:
        """Generate a team response instead of individual agent response."""
        if not prompt.strip():
            return

        assert self.client is not None

        # Store memory once for the entire team (avoids duplicate LLM processing)
        session_id = create_session_id(room_id, thread_id)
        # Convert MatrixID list to agent names for memory storage
        agent_names = [mid.agent_name(self.config) or mid.username for mid in self.team_agents]
        create_background_task(
            store_conversation_memory(
                prompt,
                agent_names,  # Pass list of agent names for team storage
                self.storage_path,
                session_id,
                self.config,
                room_id,
                thread_history,
                user_id,
            ),
            name=f"memory_save_team_{session_id}",
        )
        self.logger.info(f"Storing memory for team: {agent_names}")

        # Use the shared team response helper
        await self._generate_team_response_helper(
            room_id=room_id,
            reply_to_event_id=reply_to_event_id,
            thread_id=thread_id,
            message=prompt,
            team_agents=self.team_agents,
            team_mode=self.team_mode,
            thread_history=thread_history,
            requester_user_id=user_id or "",
            existing_event_id=existing_event_id,
        )


@dataclass
class MultiAgentOrchestrator:
    """Orchestrates multiple agent bots."""

    storage_path: Path
    agent_bots: dict[str, AgentBot | TeamBot] = field(default_factory=dict, init=False)
    running: bool = field(default=False, init=False)
    config: Config | None = field(default=None, init=False)
    _sync_tasks: dict[str, asyncio.Task] = field(default_factory=dict, init=False)
    knowledge_managers: dict[str, KnowledgeManager] = field(default_factory=dict, init=False)

    async def _ensure_user_account(self) -> None:
        """Ensure a user account exists, creating one if necessary.

        This reuses the same create_agent_user function that agents use,
        treating the user as a special "agent" named "user".
        """
        # The user account is just another "agent" from the perspective of account management
        user_account = await create_agent_user(
            MATRIX_HOMESERVER,
            "user",  # Special agent name for the human user
            "Mindroom User",  # Display name
        )
        logger.info(f"User account ready: {user_account.user_id}")

    async def _configure_knowledge(self, config: Config, *, start_watcher: bool) -> None:
        """Initialize or reconfigure knowledge managers for the current config."""
        self.knowledge_managers = await initialize_knowledge_managers(
            config=config,
            storage_path=self.storage_path,
            start_watchers=start_watcher,
        )

    async def initialize(self) -> None:
        """Initialize all agent bots with self-management.

        Each agent is now responsible for ensuring its own user account and rooms.
        """
        logger.info("Initializing multi-agent system...")

        # Ensure user account exists first
        await self._ensure_user_account()

        config = Config.from_yaml()
        load_plugins(config)
        self.config = config
        await self._configure_knowledge(config, start_watcher=False)

        # Create bots for all configured entities
        # Make Router the first so that it can manage room invitations
        all_entities = [ROUTER_AGENT_NAME, *list(config.agents.keys()), *list(config.teams.keys())]

        for entity_name in all_entities:
            # Create a temporary agent user object (will be updated by ensure_user_account)
            if entity_name == ROUTER_AGENT_NAME:
                temp_user = AgentMatrixUser(
                    agent_name=ROUTER_AGENT_NAME,
                    user_id="",  # Will be set by ensure_user_account
                    display_name="RouterAgent",
                    password="",  # Will be set by ensure_user_account
                )
            elif entity_name in config.agents:
                temp_user = AgentMatrixUser(
                    agent_name=entity_name,
                    user_id="",
                    display_name=config.agents[entity_name].display_name,
                    password="",
                )
            elif entity_name in config.teams:
                temp_user = AgentMatrixUser(
                    agent_name=entity_name,
                    user_id="",
                    display_name=config.teams[entity_name].display_name,
                    password="",
                )
            else:
                continue

            bot = create_bot_for_entity(entity_name, temp_user, config, self.storage_path)
            if bot is None:
                logger.warning(f"Could not create bot for {entity_name}")
                continue

            bot.orchestrator = self
            self.agent_bots[entity_name] = bot

        logger.info("Initialized agent bots", count=len(self.agent_bots))

    async def start(self) -> None:
        """Start all agent bots."""
        if not self.agent_bots:
            await self.initialize()

        # Start each agent bot (this registers callbacks and logs in, but doesn't join rooms)
        start_tasks = [bot.try_start() for bot in self.agent_bots.values()]
        results = await asyncio.gather(*start_tasks)

        # Check for failures
        failed_agents = [bot.agent_name for bot, success in zip(self.agent_bots.values(), results) if not success]

        if len(failed_agents) == len(self.agent_bots):
            msg = "All agents failed to start - cannot proceed"
            raise RuntimeError(msg)
        if failed_agents:
            logger.warning(
                f"System starting in degraded mode. "
                f"Failed agents: {', '.join(failed_agents)} "
                f"({len(self.agent_bots) - len(failed_agents)}/{len(self.agent_bots)} operational)",
            )
        else:
            logger.info("All agent bots started successfully")

        self.running = True
        config = self.config
        if config is None:
            msg = "Configuration not loaded"
            raise RuntimeError(msg)
        await self._configure_knowledge(config, start_watcher=True)

        # Setup rooms and have all bots join them
        await self._setup_rooms_and_memberships(list(self.agent_bots.values()))

        # Create sync tasks for each bot with automatic restart on failure
        for entity_name, bot in self.agent_bots.items():
            # Create a task for each bot's sync loop with restart wrapper
            sync_task = asyncio.create_task(_sync_forever_with_restart(bot))
            # Store the task reference for later cancellation
            self._sync_tasks[entity_name] = sync_task

        # Run all sync tasks
        await asyncio.gather(*tuple(self._sync_tasks.values()))

    async def update_config(self) -> bool:  # noqa: C901, PLR0912, PLR0915
        """Update configuration with simplified self-managing agents.

        Each agent handles its own user account creation and room management.

        Returns:
            True if any agents were updated, False otherwise.

        """
        new_config = Config.from_yaml()
        load_plugins(new_config)

        if not self.config:
            self.config = new_config
            await self._configure_knowledge(new_config, start_watcher=self.running)
            return False

        # Identify what changed - we can keep using the existing helper functions
        entities_to_restart = await _identify_entities_to_restart(self.config, new_config, self.agent_bots)

        # Also check for new entities that didn't exist before
        all_new_entities = set(new_config.agents.keys()) | set(new_config.teams.keys()) | {ROUTER_AGENT_NAME}
        existing_entities = set(self.agent_bots.keys())
        new_entities = all_new_entities - existing_entities - entities_to_restart

        # Always update the orchestrator's config first
        self.config = new_config
        await self._configure_knowledge(new_config, start_watcher=self.running)

        # Always update config for ALL existing bots (even those being restarted will get new config when recreated)
        logger.info(
            f"Updating config. New authorization: {new_config.authorization.global_users}",
        )
        for entity_name, bot in self.agent_bots.items():
            if entity_name not in entities_to_restart:
                bot.config = new_config
                await bot._set_presence_with_model_info()
                logger.debug(f"Updated config for {entity_name}")

        if not entities_to_restart and not new_entities:
            # No entities to restart or create, we're done
            return False

        # Stop entities that need restarting
        if entities_to_restart:
            await _stop_entities(entities_to_restart, self.agent_bots, self._sync_tasks)

        # Recreate entities that need restarting using self-management
        for entity_name in entities_to_restart:
            if entity_name in all_new_entities:
                # Create temporary user object (will be updated by ensure_user_account)
                temp_user = _create_temp_user(entity_name, new_config)
                bot = create_bot_for_entity(entity_name, temp_user, new_config, self.storage_path)  # type: ignore[assignment]
                if bot:
                    bot.orchestrator = self
                    self.agent_bots[entity_name] = bot
                    # Agent handles its own setup (but doesn't join rooms yet)
                    if await bot.try_start():
                        # Start sync loop with automatic restart
                        sync_task = asyncio.create_task(_sync_forever_with_restart(bot))
                        self._sync_tasks[entity_name] = sync_task
                    else:
                        # Remove the failed bot from our registry
                        del self.agent_bots[entity_name]
            # Entity was removed from config
            elif entity_name in self.agent_bots:
                del self.agent_bots[entity_name]

        # Create new entities
        for entity_name in new_entities:
            temp_user = _create_temp_user(entity_name, new_config)
            bot = create_bot_for_entity(entity_name, temp_user, new_config, self.storage_path)  # type: ignore[assignment]
            if bot:
                bot.orchestrator = self
                self.agent_bots[entity_name] = bot
                if await bot.try_start():
                    sync_task = asyncio.create_task(_sync_forever_with_restart(bot))
                    self._sync_tasks[entity_name] = sync_task
                else:
                    # Remove the failed bot from our registry
                    del self.agent_bots[entity_name]

        # Handle removed entities (cleanup)
        removed_entities = existing_entities - all_new_entities
        for entity_name in removed_entities:
            # Cancel sync task first
            await _cancel_sync_task(entity_name, self._sync_tasks)

            if entity_name in self.agent_bots:
                bot = self.agent_bots[entity_name]
                await bot.cleanup()  # Agent handles its own cleanup
                del self.agent_bots[entity_name]

        # Setup rooms and have new/restarted bots join them
        bots_to_setup = [
            self.agent_bots[entity_name]
            for entity_name in entities_to_restart | new_entities
            if entity_name in self.agent_bots
        ]

        if bots_to_setup:
            await self._setup_rooms_and_memberships(bots_to_setup)

        logger.info(f"Configuration update complete: {len(entities_to_restart) + len(new_entities)} bots affected")
        return True

    async def stop(self) -> None:
        """Stop all agent bots."""
        self.running = False
        await shutdown_knowledge_managers()
        self.knowledge_managers = {}

        # First cancel all sync tasks
        for entity_name in list(self._sync_tasks.keys()):
            await _cancel_sync_task(entity_name, self._sync_tasks)

        # Signal all bots to stop their sync loops
        for bot in self.agent_bots.values():
            bot.running = False

        # Now stop all bots
        stop_tasks = [bot.stop() for bot in self.agent_bots.values()]
        await asyncio.gather(*stop_tasks)
        logger.info("All agent bots stopped")

    async def _setup_rooms_and_memberships(self, bots: list[AgentBot | TeamBot]) -> None:
        """Setup rooms and ensure all bots have correct memberships.

        This shared method handles the common room setup flow for both
        initial startup and configuration updates.

        Args:
            bots: Collection of bots to setup room memberships for

        """
        # Ensure all configured rooms exist (router creates them if needed)
        await self._ensure_rooms_exist()

        # After rooms exist, update each bot's room list to use room IDs instead of aliases
        config = self.config
        if config is None:
            msg = "Configuration not loaded"
            raise RuntimeError(msg)
        for bot in bots:
            # Get the room aliases for this entity from config and resolve to IDs
            room_aliases = get_rooms_for_entity(bot.agent_name, config)
            bot.rooms = resolve_room_aliases(room_aliases)

        # After rooms exist, ensure room invitations are up to date
        await self._ensure_room_invitations()

        # Ensure user joins all rooms after being invited
        # Get all room IDs (not just newly created ones)
        all_rooms = load_rooms()
        all_room_ids = {room_key: room.room_id for room_key, room in all_rooms.items()}
        if all_room_ids:
            await ensure_user_in_rooms(MATRIX_HOMESERVER, all_room_ids)

        # Now have bots join their configured rooms
        join_tasks = [bot.ensure_rooms() for bot in bots]
        await asyncio.gather(*join_tasks)
        logger.info("All agents have joined their configured rooms")

    async def _ensure_rooms_exist(self) -> None:
        """Ensure all configured rooms exist, creating them if necessary.

        This uses the router bot's client to create rooms since it has the necessary permissions.
        """
        if ROUTER_AGENT_NAME not in self.agent_bots:
            logger.warning("Router not available, cannot ensure rooms exist")
            return

        router_bot = self.agent_bots[ROUTER_AGENT_NAME]
        if router_bot.client is None:
            logger.warning("Router client not available, cannot ensure rooms exist")
            return

        # Directly create rooms using the router's client
        config = self.config
        if config is None:
            msg = "Configuration not loaded"
            raise RuntimeError(msg)
        room_ids = await ensure_all_rooms_exist(router_bot.client, config)
        logger.info(f"Ensured existence of {len(room_ids)} rooms")

    async def _ensure_room_invitations(self) -> None:  # noqa: C901, PLR0912
        """Ensure all agents and the user are invited to their configured rooms.

        This uses the router bot's client to manage room invitations,
        as the router has admin privileges in all rooms.
        """
        if ROUTER_AGENT_NAME not in self.agent_bots:
            logger.warning("Router not available, cannot ensure room invitations")
            return

        router_bot = self.agent_bots[ROUTER_AGENT_NAME]
        if router_bot.client is None:
            logger.warning("Router client not available, cannot ensure room invitations")
            return

        # Get the current configuration
        config = self.config
        if not config:
            logger.warning("No configuration available, cannot ensure room invitations")
            return

        # Get all rooms the router is in
        joined_rooms = await get_joined_rooms(router_bot.client)
        if not joined_rooms:
            return

        server_name = extract_server_name_from_homeserver(MATRIX_HOMESERVER)

        # First, invite the user account to all rooms
        state = MatrixState.load()
        user_account = state.get_account("agent_user")  # User is stored as "agent_user"
        if user_account:
            user_id = MatrixID.from_username(user_account.username, server_name).full_id
            for room_id in joined_rooms:
                room_members = await get_room_members(router_bot.client, room_id)
                if user_id not in room_members:
                    success = await invite_to_room(router_bot.client, room_id, user_id)
                    if success:
                        logger.info(f"Invited user {user_id} to room {room_id}")
                    else:
                        logger.warning(f"Failed to invite user {user_id} to room {room_id}")

        for room_id in joined_rooms:
            # Get who should be in this room based on configuration
            configured_bots = config.get_configured_bots_for_room(room_id)

            if not configured_bots:
                continue

            # Get current members of the room
            current_members = await get_room_members(router_bot.client, room_id)

            # Invite missing bots
            for bot_username in configured_bots:
                bot_user_id = MatrixID.from_username(bot_username, server_name).full_id

                if bot_user_id not in current_members:
                    # Bot should be in room but isn't - invite them
                    success = await invite_to_room(router_bot.client, room_id, bot_user_id)
                    if success:
                        logger.info(f"Invited {bot_username} to room {room_id}")
                    else:
                        logger.warning(f"Failed to invite {bot_username} to room {room_id}")

        logger.info("Ensured room invitations for all configured agents")


async def _identify_entities_to_restart(
    config: Config | None,
    new_config: Config,
    agent_bots: dict[str, Any],
) -> set[str]:
    """Identify entities that need restarting due to config changes."""
    agents_to_restart = _get_changed_agents(config, new_config, agent_bots)
    teams_to_restart = _get_changed_teams(config, new_config, agent_bots)

    entities_to_restart = agents_to_restart | teams_to_restart

    if _router_needs_restart(config, new_config):
        entities_to_restart.add(ROUTER_AGENT_NAME)

    return entities_to_restart


def _get_changed_agents(config: Config | None, new_config: Config, agent_bots: dict[str, Any]) -> set[str]:
    if not config:
        return set()

    changed = set()
    all_agents = set(config.agents.keys()) | set(new_config.agents.keys())

    for agent_name in all_agents:
        old_agent = config.agents.get(agent_name)
        new_agent = new_config.agents.get(agent_name)

        # Compare agents using model_dump with exclude_none=True to match how configs are saved
        # This prevents false positives when None values are involved
        if old_agent and new_agent:
            # Both exist - compare their non-None values (matching save_to_yaml behavior)
            old_dict = old_agent.model_dump(exclude_none=True)
            new_dict = new_agent.model_dump(exclude_none=True)
            agents_differ = old_dict != new_dict
        else:
            # One is None - they definitely differ
            agents_differ = old_agent != new_agent

        # Only restart if this specific agent's configuration has changed
        # (not just global config changes like authorization)
        if agents_differ and (agent_name in agent_bots or new_agent is not None):
            if old_agent and new_agent:
                logger.debug(f"Agent {agent_name} configuration changed, will restart")
            elif new_agent:
                logger.info(f"Agent {agent_name} is new, will start")
            else:
                logger.info(f"Agent {agent_name} was removed, will stop")
            changed.add(agent_name)

    return changed


def _get_changed_teams(config: Config | None, new_config: Config, agent_bots: dict[str, Any]) -> set[str]:
    if not config:
        return set()

    changed = set()
    all_teams = set(config.teams.keys()) | set(new_config.teams.keys())

    for team_name in all_teams:
        old_team = config.teams.get(team_name)
        new_team = new_config.teams.get(team_name)

        # Compare teams using model_dump with exclude_none=True to match how configs are saved
        if old_team and new_team:
            old_dict = old_team.model_dump(exclude_none=True)
            new_dict = new_team.model_dump(exclude_none=True)
            teams_differ = old_dict != new_dict
        else:
            teams_differ = old_team != new_team

        if teams_differ and (team_name in agent_bots or new_team is not None):
            changed.add(team_name)

    return changed


def _router_needs_restart(config: Config | None, new_config: Config) -> bool:
    """Check if router needs restart due to room changes."""
    if not config:
        return False

    old_rooms = config.get_all_configured_rooms()
    new_rooms = new_config.get_all_configured_rooms()
    return old_rooms != new_rooms


def _create_temp_user(entity_name: str, config: Config) -> AgentMatrixUser:
    """Create a temporary user object that will be updated by ensure_user_account."""
    if entity_name == ROUTER_AGENT_NAME:
        display_name = "RouterAgent"
    elif entity_name in config.agents:
        display_name = config.agents[entity_name].display_name
    elif entity_name in config.teams:
        display_name = config.teams[entity_name].display_name
    else:
        display_name = entity_name

    return AgentMatrixUser(
        agent_name=entity_name,
        user_id="",  # Will be set by ensure_user_account
        display_name=display_name,
        password="",  # Will be set by ensure_user_account
    )


async def _cancel_sync_task(entity_name: str, sync_tasks: dict[str, asyncio.Task]) -> None:
    """Cancel and remove a sync task for an entity."""
    if entity_name in sync_tasks:
        task = sync_tasks[entity_name]
        task.cancel()
        with suppress(asyncio.CancelledError):
            await task
        del sync_tasks[entity_name]


async def _stop_entities(
    entities_to_restart: set[str],
    agent_bots: dict[str, Any],
    sync_tasks: dict[str, asyncio.Task],
) -> None:
    # Cancel sync tasks to prevent duplicates
    for entity_name in entities_to_restart:
        await _cancel_sync_task(entity_name, sync_tasks)

    stop_tasks = []
    for entity_name in entities_to_restart:
        if entity_name in agent_bots:
            bot = agent_bots[entity_name]
            stop_tasks.append(bot.stop())

    if stop_tasks:
        await asyncio.gather(*stop_tasks)

    for entity_name in entities_to_restart:
        agent_bots.pop(entity_name, None)


async def _sync_forever_with_restart(bot: AgentBot | TeamBot, max_retries: int = -1) -> None:
    """Run sync_forever with automatic restart on failure.

    Args:
        bot: The bot to run sync for
        max_retries: Maximum number of retries (-1 for infinite)

    """
    retry_count = 0
    while bot.running and (max_retries < 0 or retry_count < max_retries):
        try:
            logger.info(f"Starting sync loop for {bot.agent_name}")
            await bot.sync_forever()
            # If sync_forever returns normally, the bot was stopped intentionally
            break
        except asyncio.CancelledError:
            # Task was cancelled, exit gracefully
            logger.info(f"Sync task for {bot.agent_name} was cancelled")
            break
        except Exception:
            retry_count += 1
            logger.exception(f"Sync loop failed for {bot.agent_name} (retry {retry_count})")

            if not bot.running:
                # Bot was stopped, don't restart
                break

            if max_retries >= 0 and retry_count >= max_retries:
                logger.exception(f"Max retries ({max_retries}) reached for {bot.agent_name}, giving up")
                break

            # Wait a bit before restarting to avoid rapid restarts
            wait_time = min(60, 5 * retry_count)  # Exponential backoff, max 60 seconds
            logger.info(f"Restarting sync loop for {bot.agent_name} in {wait_time} seconds...")
            await asyncio.sleep(wait_time)


async def _handle_config_change(orchestrator: MultiAgentOrchestrator, stop_watching: asyncio.Event) -> None:
    """Handle configuration file changes."""
    logger.info("Configuration file changed, checking for updates...")
    if orchestrator.running:
        updated = await orchestrator.update_config()
        if updated:
            logger.info("Configuration update applied to affected agents")
        else:
            logger.info("No agent changes detected in configuration update")
    if not orchestrator.running:
        stop_watching.set()


async def _watch_config_task(config_path: Path, orchestrator: MultiAgentOrchestrator) -> None:
    """Watch config file for changes."""
    stop_watching = asyncio.Event()

    async def on_config_change() -> None:
        await _handle_config_change(orchestrator, stop_watching)

    await watch_file(config_path, on_config_change, stop_watching)


async def _watch_skills_task(orchestrator: MultiAgentOrchestrator) -> None:
    """Watch skill roots for changes and clear cached skills."""
    # Wait for orchestrator to start before watching
    while not orchestrator.running:  # noqa: ASYNC110
        await asyncio.sleep(0.1)
    last_snapshot = get_skill_snapshot()
    while orchestrator.running:
        await asyncio.sleep(1.0)
        snapshot = get_skill_snapshot()
        if snapshot != last_snapshot:
            last_snapshot = snapshot
            clear_skill_cache()
            logger.info("Skills changed; cache cleared")


async def main(log_level: str, storage_path: Path) -> None:
    """Main entry point for the multi-agent bot system.

    Args:
        log_level: The logging level to use (DEBUG, INFO, WARNING, ERROR)
        storage_path: The base directory for storing agent data

    """
    # Set up logging with the specified level
    setup_logging(level=log_level)

    # Sync API keys from environment to CredentialsManager
    logger.info("Syncing API keys from environment to CredentialsManager...")
    sync_env_to_credentials()

    # Create storage directory if it doesn't exist
    storage_path.mkdir(parents=True, exist_ok=True)

    # Get config file path
    config_path = Path("config.yaml")

    # Create and start orchestrator
    logger.info("Starting orchestrator...")
    orchestrator = MultiAgentOrchestrator(storage_path=storage_path)

    try:
        # Create task to run the orchestrator
        orchestrator_task = asyncio.create_task(orchestrator.start())

        # Create task to watch config file for changes
        watcher_task = asyncio.create_task(_watch_config_task(config_path, orchestrator))

        # Create task to watch skills for changes
        skills_watcher_task = asyncio.create_task(_watch_skills_task(orchestrator))

        # Wait for either orchestrator or watcher to complete
        done, pending = await asyncio.wait(
            {orchestrator_task, watcher_task, skills_watcher_task},
            return_when=asyncio.FIRST_COMPLETED,
        )

        # Check if any completed task had an exception
        for task in done:
            try:
                task.result()  # This will raise if the task had an exception
            except asyncio.CancelledError:
                logger.info("Task was cancelled")
            except Exception:
                logger.exception("Task failed with exception")
                # Don't re-raise - let cleanup happen gracefully

        # Cancel any pending tasks
        for task in pending:
            task.cancel()
            with suppress(asyncio.CancelledError):
                await task

    except KeyboardInterrupt:
        logger.info("Multi-agent bot system stopped by user")
    except Exception:
        logger.exception("Error in orchestrator")
    finally:
        # Final cleanup
        if orchestrator is not None:
            await orchestrator.stop()

================================================================================
# FILE: src/mindroom/cli.py
================================================================================

"""Mindroom CLI - Simplified multi-agent Matrix bot system."""

from __future__ import annotations

import asyncio
import sys
from pathlib import Path

import typer
from pydantic import ValidationError
from rich.console import Console

from mindroom import __version__
from mindroom.bot import main as bot_main
from mindroom.config import Config
from mindroom.constants import DEFAULT_AGENTS_CONFIG, STORAGE_PATH

app = typer.Typer(
    help="Mindroom: Multi-agent Matrix bot system",
    pretty_exceptions_enable=True,
    # Disable showing locals which can be very large (also see `setup_logging`)
    pretty_exceptions_show_locals=False,
)
console = Console()


@app.command()
def version() -> None:
    """Show the current version of Mindroom."""
    console.print(f"Mindroom version: [bold]{__version__}[/bold]")
    console.print("Multi-agent Matrix bot system")


@app.command()
def validate(
    config_path: Path = typer.Option(  # noqa: B008
        Path(DEFAULT_AGENTS_CONFIG),
        "--config",
        "-c",
        help="Path to the configuration file to validate",
    ),
) -> None:
    """Validate the configuration file.

    Parses the YAML configuration using Pydantic and reports any errors.
    """
    console.print(f"Validating configuration: [bold]{config_path}[/bold]")

    if not config_path.exists():
        console.print(f"[red]Error:[/red] Configuration file not found: {config_path}")
        raise typer.Exit(1)

    try:
        config = Config.from_yaml(config_path)
        console.print("[green]âœ“[/green] Configuration is valid!")
        console.print(f"  â€¢ {len(config.agents)} agent(s): {', '.join(config.agents.keys()) or 'none'}")
        console.print(f"  â€¢ {len(config.teams)} team(s): {', '.join(config.teams.keys()) or 'none'}")
        console.print(f"  â€¢ {len(config.models)} model(s): {', '.join(config.models.keys()) or 'none'}")
        rooms = config.get_all_configured_rooms()
        console.print(f"  â€¢ {len(rooms)} room(s): {', '.join(sorted(rooms)) or 'none'}")
    except ValidationError as e:
        console.print("[red]âœ—[/red] Configuration validation failed:")
        for error in e.errors():
            loc = " â†’ ".join(str(x) for x in error["loc"])
            console.print(f"  [red]â€¢[/red] {loc}: {error['msg']}")
        raise typer.Exit(1) from None
    except Exception as e:
        console.print(f"[red]âœ—[/red] Error loading configuration: {e}")
        raise typer.Exit(1) from None


@app.command()
def run(
    log_level: str = typer.Option(
        "INFO",
        "--log-level",
        "-l",
        help="Set the logging level (DEBUG, INFO, WARNING, ERROR)",
        case_sensitive=False,
    ),
    storage_path: Path = typer.Option(  # noqa: B008
        Path(STORAGE_PATH),
        "--storage-path",
        "-s",
        help="Base directory for persistent MindRoom data (state, sessions, tracking)",
    ),
) -> None:
    """Run the mindroom multi-agent system.

    This command starts the multi-agent bot system which automatically:
    - Creates all necessary user and agent accounts
    - Creates all rooms defined in config.yaml
    - Manages agent room memberships
    """
    asyncio.run(_run(log_level=log_level.upper(), storage_path=storage_path))


async def _run(log_level: str, storage_path: Path) -> None:
    """Run the multi-agent system."""
    console.print(f"ðŸš€ Starting Mindroom multi-agent system (log level: {log_level})...")
    console.print("Press Ctrl+C to stop\n")

    try:
        await bot_main(log_level=log_level, storage_path=storage_path)
    except KeyboardInterrupt:
        console.print("\nâœ‹ Stopped")


def main() -> None:
    """Main entry point that shows help by default."""
    # Handle -h flag by replacing with --help
    for i, arg in enumerate(sys.argv):
        if arg == "-h":
            sys.argv[i] = "--help"
            break

    # If no arguments provided, show help
    if len(sys.argv) == 1:
        # Show help by appending --help to argv
        sys.argv.append("--help")

    app()


if __name__ == "__main__":
    main()

================================================================================
# FILE: src/mindroom/commands.py
================================================================================

"""Command parsing and handling for user commands."""

from __future__ import annotations

import re
from dataclasses import dataclass
from enum import Enum
from typing import Any

import nio

from .constants import VOICE_PREFIX
from .logging_config import get_logger

logger = get_logger(__name__)


class CommandType(Enum):
    """Types of commands supported."""

    HELP = "help"
    SCHEDULE = "schedule"
    LIST_SCHEDULES = "list_schedules"
    CANCEL_SCHEDULE = "cancel_schedule"
    WIDGET = "widget"
    CONFIG = "config"  # Configuration command
    HI = "hi"  # Welcome message command
    SKILL = "skill"  # Skill command
    UNKNOWN = "unknown"  # Special type for unrecognized commands


# Command documentation for each command type
COMMAND_DOCS = {
    CommandType.SCHEDULE: ("!schedule <task>", "Schedule a task"),
    CommandType.LIST_SCHEDULES: ("!list_schedules", "List scheduled tasks"),
    CommandType.CANCEL_SCHEDULE: ("!cancel_schedule <id>", "Cancel a scheduled task"),
    CommandType.HELP: ("!help [topic]", "Get help"),
    CommandType.WIDGET: ("!widget [url]", "Add configuration widget"),
    CommandType.CONFIG: ("!config <operation>", "Manage configuration"),
    CommandType.HI: ("!hi", "Show welcome message"),
    CommandType.SKILL: ("!skill <name> [args]", "Run a skill by name"),
}


def _get_command_entries(format_code: bool = False) -> list[str]:
    """Get command entries as a list of formatted strings.

    Args:
        format_code: If True, wrap commands in backticks for markdown

    Returns:
        List of formatted command strings

    """
    entries = []
    for cmd_type in CommandType:
        if cmd_type in COMMAND_DOCS and cmd_type != CommandType.UNKNOWN:
            syntax, description = COMMAND_DOCS[cmd_type]
            if format_code:
                entries.append(f"- `{syntax}` - {description}")
            else:
                entries.append(f"- {syntax} - {description}")
    return entries


def get_command_list() -> str:
    """Get a formatted list of all available commands.

    Returns:
        Formatted string with all commands and their descriptions

    """
    lines = ["Available commands:", *_get_command_entries(format_code=False)]
    return "\n".join(lines)


@dataclass
class Command:
    """Parsed command with arguments."""

    type: CommandType
    args: dict[str, Any]
    raw_text: str


class CommandParser:
    """Parser for user commands in messages."""

    # Command patterns
    HELP_PATTERN = re.compile(r"^!help(?:\s+(.+))?$", re.IGNORECASE)
    SCHEDULE_PATTERN = re.compile(r"^!schedule\s+(.+)$", re.IGNORECASE | re.DOTALL)
    LIST_SCHEDULES_PATTERN = re.compile(r"^!list[_-]?schedules?$", re.IGNORECASE)
    CANCEL_SCHEDULE_PATTERN = re.compile(r"^!cancel[_-]?schedule\s+(.+)$", re.IGNORECASE)
    WIDGET_PATTERN = re.compile(r"^!widget(?:\s+(.+))?$", re.IGNORECASE)
    CONFIG_PATTERN = re.compile(r"^!config(?:\s+(.+))?$", re.IGNORECASE)
    HI_PATTERN = re.compile(r"^!hi$", re.IGNORECASE)
    SKILL_PATTERN = re.compile(r"^!skill(?:\s+(.+))?$", re.IGNORECASE)

    def parse(self, message: str) -> Command | None:  # noqa: C901, PLR0911
        """Parse a message for commands.

        Args:
            message: The message text to parse

        Returns:
            Parsed command or None if no command found

        """
        message = message.strip()

        # Handle voice emoji prefixe (e.g., "ðŸŽ¤ !schedule ...")
        message = message.removeprefix(VOICE_PREFIX)
        if not message.startswith("!"):
            return None

        # Try to match each command pattern

        # !hi command (check this early as it's simple)
        if self.HI_PATTERN.match(message):
            return Command(
                type=CommandType.HI,
                args={},
                raw_text=message,
            )

        # !help command
        match = self.HELP_PATTERN.match(message)
        if match:
            topic = match.group(1)
            return Command(
                type=CommandType.HELP,
                args={"topic": topic},
                raw_text=message,
            )

        # !schedule command
        match = self.SCHEDULE_PATTERN.match(message)
        if match:
            full_text = match.group(1).strip()
            # Pass the entire text to AI - it will parse both time and message
            return Command(
                type=CommandType.SCHEDULE,
                args={"full_text": full_text},
                raw_text=message,
            )

        # !list_schedules command
        if self.LIST_SCHEDULES_PATTERN.match(message):
            return Command(
                type=CommandType.LIST_SCHEDULES,
                args={},
                raw_text=message,
            )

        # !cancel_schedule command
        match = self.CANCEL_SCHEDULE_PATTERN.match(message)
        if match:
            task_id = match.group(1).strip()
            # Check if user wants to cancel all tasks
            cancel_all = task_id.lower() == "all"
            return Command(
                type=CommandType.CANCEL_SCHEDULE,
                args={"task_id": task_id, "cancel_all": cancel_all},
                raw_text=message,
            )

        # !widget command
        match = self.WIDGET_PATTERN.match(message)
        if match:
            url = match.group(1).strip() if match.group(1) else None
            return Command(
                type=CommandType.WIDGET,
                args={"url": url},
                raw_text=message,
            )

        # !config command
        match = self.CONFIG_PATTERN.match(message)
        if match:
            args_text = match.group(1).strip() if match.group(1) else ""
            return Command(
                type=CommandType.CONFIG,
                args={"args_text": args_text},
                raw_text=message,
            )

        # !skill command
        match = self.SKILL_PATTERN.match(message)
        if match:
            payload = match.group(1).strip() if match.group(1) else ""
            if not payload:
                return Command(
                    type=CommandType.SKILL,
                    args={"skill_name": None, "args_text": ""},
                    raw_text=message,
                )
            parts = payload.split(maxsplit=1)
            skill_name = parts[0].strip()
            args_text = parts[1].strip() if len(parts) > 1 else ""
            return Command(
                type=CommandType.SKILL,
                args={"skill_name": skill_name, "args_text": args_text},
                raw_text=message,
            )

        # Unknown command - return a special Command indicating it's unknown
        logger.debug(f"Unknown command: {message}")
        return Command(
            type=CommandType.UNKNOWN,
            args={"raw_command": message},
            raw_text=message,
        )


def get_command_help(topic: str | None = None) -> str:  # noqa: PLR0911
    """Get help text for commands.

    Args:
        topic: Specific topic to get help for (optional)

    Returns:
        Help text

    """
    if topic == "schedule":
        return """**Schedule Command**

Usage: `!schedule <time> <message>` - Schedule tasks, reminders, or agent workflows

**Simple Reminders:**
- `!schedule in 5 minutes Check the deployment`
- `!schedule tomorrow at 3pm Send the weekly report`
- `!schedule later Ping me about the meeting`
- `ping me tomorrow about the meeting`
- `remind me in 2 hours to review PRs`

**Event-Driven Workflows (New!):**
- `!schedule If I get an email about "urgent", @phone_agent call me`
- `!schedule When Bitcoin drops below $40k, @crypto_agent notify me`
- `!schedule If server CPU > 80%, @ops_agent scale up`
- `!schedule When someone mentions our product on Reddit, @analyst summarize it`
- `!schedule Whenever I get email from boss, @notification_agent alert me immediately`

**Agent Workflows:**
- `!schedule Daily at 9am, @finance give me a market analysis`
- `!schedule Every Monday, @research AI news and @email_assistant send me a summary`
- `!schedule tomorrow at 2pm, @email_assistant check my Gmail`

**Recurring Tasks (Cron-style):**
- `!schedule Every hour, @shell check server status`
- `!schedule Daily at 9am, @finance market report`
- `!schedule Weekly on Friday, @analyst prepare weekly summary`

How it works:
- **Time-based**: Executes at specific times or intervals
- **Event-based**: Automatically converts to smart polling (e.g., "if email" â†’ check every 1-2 min)
- Agents receive clear instructions about conditions to check
- Multiple agents collaborate when mentioned together
- Automated tasks are clearly marked so agents don't wait for follow-up"""

    if topic == "skill":
        return """**Skill Command**

Usage: `!skill <name> [args]` - Run a user-invocable skill by name

Examples:
- `!skill repo-quick-audit`
- `!skill summarize Release notes for v2.3`

Notes:
- Skills must be enabled on the target agent and marked `user-invocable: true`.
- When a skill uses `command-dispatch: tool`, the tool runs directly with raw args."""

    if topic == "list_schedules":
        return """**List Schedules Command**

Usage: `!list_schedules`

Alternative syntax: `!listschedules`, `!list-schedules`, `!list_schedule`, `!listschedule`, `!list-schedule`

Shows pending scheduled tasks. When used in a thread, shows tasks for that thread. When used in the main room, shows all tasks in the room."""

    if topic in {"cancel", "cancel_schedule"}:
        return """**Cancel Schedule Command**

Usage: `!cancel_schedule <id>` - Cancel a scheduled task
       `!cancel_schedule all` - Cancel ALL scheduled tasks in this room

Alternative syntax: `!cancelschedule`, `!cancel-schedule`

Examples:
- `!cancel_schedule abc123` - Cancel the task with ID abc123
- `!cancel_schedule all` - Cancel all scheduled tasks

Use `!list_schedules` to see task IDs."""

    if topic == "config":
        return """**Config Command**

Usage: `!config <operation>` - View and modify MindRoom configuration

**Viewing Configuration:**
- `!config show` - Show entire configuration
- `!config get <path>` - Get a specific configuration value
- `!config get agents` - Show all agents
- `!config get models.default` - Show default model
- `!config get agents.analyst.display_name` - Show analyst's display name

**Modifying Configuration:**
- `!config set <path> <value>` - Set a configuration value
- `!config set agents.analyst.display_name "Research Expert"` - Change display name
- `!config set models.default.id gpt-4` - Change default model
- `!config set defaults.markdown false` - Disable markdown by default
- `!config set timezone America/New_York` - Set timezone

**Path Syntax:**
- Use dot notation to navigate nested config (e.g., `agents.analyst.role`)
- Arrays use indexes (e.g., `agents.analyst.tools.0` for first tool)
- String values with spaces must be quoted

**Note:** Configuration changes are immediately saved to config.yaml and affect all new agent interactions."""

    if topic == "widget":
        return """**Widget Command**

Usage: `!widget [url]` - Add the MindRoom configuration widget to this room

Examples:
- `!widget` - Add widget using default URL (http://localhost:3003)
- `!widget https://config.mindroom.ai` - Add widget from custom URL

The widget provides a visual interface for configuring MindRoom agents and settings.
Pin it to keep it visible in the room.

Note: Widget support requires Element Desktop or self-hosted Element Web."""

    # General help - dynamically generated from COMMAND_DOCS
    commands_text = "\n".join(_get_command_entries(format_code=True))

    return f"""**Available Commands**

{commands_text}

**Scheduling Features:**
- Time-based and event-driven workflows
- Recurring tasks with cron-style scheduling (daily, weekly, hourly)
- Agent workflows - mention agents to have them collaborate on scheduled tasks
- Natural language time parsing - "tomorrow", "in 5 minutes", "every Monday"

Note: All commands only work within threads, not in main room messages
(except !widget which works in the main room).

For detailed help on a command, use: `!help <command>`"""


async def handle_widget_command(
    client: nio.AsyncClient,
    room_id: str,
    url: str | None = None,
) -> str:
    """Handle the widget command to add configuration widget to room.

    Args:
        client: The Matrix client
        room_id: The room ID to add widget to
        url: Optional custom widget URL

    Returns:
        Response text for the user

    """
    # Default URL for local development
    default_url = "http://localhost:3003/matrix-widget.html"
    widget_url = url if url else default_url

    # Create the widget state event content
    widget_content = {
        "type": "custom",
        "url": widget_url,
        "name": "MindRoom Configuration",
        "data": {"title": "MindRoom Configuration", "curl": widget_url.replace("/matrix-widget.html", "")},
        "creatorUserId": client.user_id,
        "id": "mindroom_config",
    }

    try:
        # Send the state event to add the widget
        response = await client.room_put_state(
            room_id=room_id,
            event_type="im.vector.modular.widgets",
            state_key="mindroom_config",
            content=widget_content,
        )

        if isinstance(response, nio.RoomPutStateError):
            logger.error(f"Failed to add widget to room {room_id}: {response.message}")
            return f"âŒ Failed to add widget: {response.message}"

        logger.info(f"Successfully added widget to room {room_id}")
    except Exception as e:
        logger.exception("Error adding widget to room", room_id=room_id)
        return f"âŒ Error adding widget: {e!s}"
    else:
        return (
            "âœ… **MindRoom Configuration widget added!**\n\n"
            "â€¢ Pin the widget to keep it visible\n"
            "â€¢ All room members can access the configuration\n"
            "â€¢ Changes sync in real-time with config.yaml\n\n"
            f"Widget URL: {widget_url}\n\n"
            "**Note:** Widgets require Element Desktop or self-hosted Element Web.\n"
            "Alternatively, you can use: `/addwidget {url}` in Element."
        )


# Global parser instance
command_parser = CommandParser()

================================================================================
# FILE: src/mindroom/config.py
================================================================================

"""Pydantic models for configuration."""

from __future__ import annotations

from functools import cached_property
from pathlib import Path
from typing import TYPE_CHECKING, Any, Literal

import yaml
from pydantic import BaseModel, Field, model_validator

from .constants import DEFAULT_AGENTS_CONFIG, MATRIX_HOMESERVER, ROUTER_AGENT_NAME, safe_replace
from .logging_config import get_logger

if TYPE_CHECKING:
    from .matrix.identity import MatrixID

logger = get_logger(__name__)

AgentLearningMode = Literal["always", "agentic"]


class AgentConfig(BaseModel):
    """Configuration for a single agent."""

    display_name: str = Field(description="Human-readable name for the agent")
    role: str = Field(default="", description="Description of the agent's purpose")
    tools: list[str] = Field(default_factory=list, description="List of tool names")
    skills: list[str] = Field(default_factory=list, description="List of skill names")
    instructions: list[str] = Field(default_factory=list, description="Agent instructions")
    rooms: list[str] = Field(default_factory=list, description="List of room IDs or names to auto-join")
    markdown: bool | None = Field(default=None, description="Whether to use markdown formatting")
    learning: bool | None = Field(default=None, description="Enable Agno Learning (defaults to true when omitted)")
    learning_mode: AgentLearningMode | None = Field(
        default=None,
        description="Learning mode for Agno Learning: always (automatic) or agentic (tool-driven)",
    )
    model: str = Field(default="default", description="Model name")
    knowledge_base: str | None = Field(
        default=None,
        description="Knowledge base ID assigned to this agent",
    )


class DefaultsConfig(BaseModel):
    """Default configuration values for agents."""

    markdown: bool = Field(default=True, description="Default markdown setting")
    show_stop_button: bool = Field(default=False, description="Whether to automatically show stop button on messages")
    learning: bool = Field(default=True, description="Default Agno Learning setting")
    learning_mode: AgentLearningMode = Field(default="always", description="Default Agno Learning mode")


class EmbedderConfig(BaseModel):
    """Configuration for memory embedder."""

    model: str = Field(default="text-embedding-3-small", description="Model name for embeddings")
    api_key: str | None = Field(default=None, description="API key (usually from environment variable)")
    host: str | None = Field(default=None, description="Host URL for self-hosted models (Ollama, llama.cpp, etc.)")


class MemoryEmbedderConfig(BaseModel):
    """Memory embedder configuration."""

    provider: str = Field(default="openai", description="Embedder provider (openai, huggingface, etc)")
    config: EmbedderConfig = Field(default_factory=EmbedderConfig, description="Provider-specific config")


class MemoryLLMConfig(BaseModel):
    """Memory LLM configuration."""

    provider: str = Field(default="ollama", description="LLM provider (ollama, openai, anthropic)")
    config: dict[str, Any] = Field(default_factory=dict, description="Provider-specific LLM config")


class MemoryConfig(BaseModel):
    """Memory system configuration."""

    embedder: MemoryEmbedderConfig = Field(
        default_factory=MemoryEmbedderConfig,
        description="Embedder configuration for memory",
    )
    llm: MemoryLLMConfig | None = Field(default=None, description="LLM configuration for memory")


class KnowledgeBaseConfig(BaseModel):
    """Knowledge base configuration."""

    path: str = Field(default="./knowledge_docs", description="Path to knowledge documents folder")
    watch: bool = Field(default=True, description="Watch folder for changes")


class ModelConfig(BaseModel):
    """Configuration for an AI model."""

    provider: str = Field(description="Model provider (openai, anthropic, ollama, etc)")
    id: str = Field(description="Model ID specific to the provider")
    host: str | None = Field(default=None, description="Optional host URL (e.g., for Ollama)")
    api_key: str | None = Field(default=None, description="Optional API key (usually from env vars)")
    extra_kwargs: dict[str, Any] | None = Field(
        default=None,
        description="Additional provider-specific parameters passed directly to the model",
    )


class RouterConfig(BaseModel):
    """Configuration for the router system."""

    model: str = Field(default="default", description="Model to use for routing decisions")


class TeamConfig(BaseModel):
    """Configuration for a team of agents."""

    display_name: str = Field(description="Human-readable name for the team")
    role: str = Field(description="Description of the team's purpose")
    agents: list[str] = Field(description="List of agent names that compose this team")
    rooms: list[str] = Field(default_factory=list, description="List of room IDs or names to auto-join")
    model: str | None = Field(default="default", description="Default model for this team (optional)")
    mode: str = Field(default="coordinate", description="Team collaboration mode: coordinate or collaborate")


class VoiceSTTConfig(BaseModel):
    """Configuration for voice speech-to-text."""

    provider: str = Field(default="openai", description="STT provider (openai or compatible)")
    model: str = Field(default="whisper-1", description="STT model name")
    api_key: str | None = Field(default=None, description="API key for STT service")
    host: str | None = Field(default=None, description="Host URL for self-hosted STT")


class VoiceLLMConfig(BaseModel):
    """Configuration for voice command intelligence."""

    model: str = Field(default="default", description="Model for command recognition")


class VoiceConfig(BaseModel):
    """Configuration for voice message handling."""

    enabled: bool = Field(default=False, description="Enable voice message processing")
    stt: VoiceSTTConfig = Field(default_factory=VoiceSTTConfig, description="STT configuration")
    intelligence: VoiceLLMConfig = Field(
        default_factory=VoiceLLMConfig,
        description="Command intelligence configuration",
    )


class AuthorizationConfig(BaseModel):
    """Authorization configuration with fine-grained permissions."""

    global_users: list[str] = Field(
        default_factory=list,
        description="Users with access to all rooms (e.g., '@user:example.com')",
    )
    room_permissions: dict[str, list[str]] = Field(
        default_factory=dict,
        description="Room-specific user permissions. Keys are room IDs, values are lists of authorized user IDs",
    )
    default_room_access: bool = Field(
        default=False,
        description="Default permission for rooms not explicitly configured",
    )


class Config(BaseModel):
    """Complete configuration from YAML."""

    agents: dict[str, AgentConfig] = Field(default_factory=dict, description="Agent configurations")
    teams: dict[str, TeamConfig] = Field(default_factory=dict, description="Team configurations")
    room_models: dict[str, str] = Field(default_factory=dict, description="Room-specific model overrides")
    plugins: list[str] = Field(default_factory=list, description="Plugin paths")
    defaults: DefaultsConfig = Field(default_factory=DefaultsConfig, description="Default values")
    memory: MemoryConfig = Field(default_factory=MemoryConfig, description="Memory configuration")
    knowledge_bases: dict[str, KnowledgeBaseConfig] = Field(
        default_factory=dict,
        description="Knowledge base configurations keyed by base ID",
    )
    models: dict[str, ModelConfig] = Field(default_factory=dict, description="Model configurations")
    router: RouterConfig = Field(default_factory=RouterConfig, description="Router configuration")
    voice: VoiceConfig = Field(default_factory=VoiceConfig, description="Voice configuration")
    timezone: str = Field(
        default="UTC",
        description="Timezone for displaying scheduled tasks (e.g., 'America/New_York')",
    )
    authorization: AuthorizationConfig = Field(
        default_factory=AuthorizationConfig,
        description="Authorization configuration with fine-grained permissions",
    )

    @model_validator(mode="after")
    def validate_knowledge_base_assignments(self) -> Config:
        """Ensure agents only reference configured knowledge base IDs."""
        invalid_assignments = [
            (agent_name, agent_config.knowledge_base)
            for agent_name, agent_config in self.agents.items()
            if agent_config.knowledge_base is not None and agent_config.knowledge_base not in self.knowledge_bases
        ]
        if invalid_assignments:
            formatted = ", ".join(
                f"{agent_name} -> {knowledge_base}"
                for agent_name, knowledge_base in sorted(invalid_assignments, key=lambda item: item[0])
            )
            msg = f"Agents reference unknown knowledge bases: {formatted}"
            raise ValueError(msg)
        return self

    @cached_property
    def domain(self) -> str:
        """Extract the domain from the MATRIX_HOMESERVER."""
        from .matrix.identity import extract_server_name_from_homeserver  # noqa: PLC0415

        return extract_server_name_from_homeserver(MATRIX_HOMESERVER)

    @cached_property
    def ids(self) -> dict[str, MatrixID]:
        """Get MatrixID objects for all agents and teams.

        Returns:
            Dictionary mapping agent/team names to their MatrixID objects.

        """
        from .matrix.identity import MatrixID  # noqa: PLC0415

        mapping: dict[str, MatrixID] = {}

        # Add all agents
        for agent_name in self.agents:
            mapping[agent_name] = MatrixID.from_agent(agent_name, self.domain)

        # Add router agent separately (it's not in config.agents)
        mapping[ROUTER_AGENT_NAME] = MatrixID.from_agent(ROUTER_AGENT_NAME, self.domain)

        # Add all teams
        for team_name in self.teams:
            mapping[team_name] = MatrixID.from_agent(team_name, self.domain)
        return mapping

    @classmethod
    def from_yaml(cls, config_path: Path | None = None) -> Config:
        """Create a Config instance from YAML data."""
        path = config_path or DEFAULT_AGENTS_CONFIG

        if not path.exists():
            msg = f"Agent configuration file not found: {path}"
            raise FileNotFoundError(msg)

        with path.open() as f:
            data = yaml.safe_load(f)

        # Handle None values for optional dictionaries
        if data.get("teams") is None:
            data["teams"] = {}
        if data.get("room_models") is None:
            data["room_models"] = {}
        if data.get("plugins") is None:
            data["plugins"] = []
        if data.get("knowledge_bases") is None:
            data["knowledge_bases"] = {}

        config = cls(**data)
        logger.info(f"Loaded agent configuration from {path}")
        logger.info(f"Found {len(config.agents)} agent configurations")
        return config

    def get_agent(self, agent_name: str) -> AgentConfig:
        """Get an agent configuration by name.

        Args:
            agent_name: Name of the agent

        Returns:
            Agent configuration

        Raises:
            ValueError: If agent not found

        """
        if agent_name not in self.agents:
            available = ", ".join(sorted(self.agents.keys()))
            msg = f"Unknown agent: {agent_name}. Available agents: {available}"
            raise ValueError(msg)
        return self.agents[agent_name]

    def get_all_configured_rooms(self) -> set[str]:
        """Extract all room aliases configured for agents and teams.

        Returns:
            Set of all unique room aliases from agent and team configurations

        """
        all_room_aliases = set()
        for agent_config in self.agents.values():
            all_room_aliases.update(agent_config.rooms)
        for team_config in self.teams.values():
            all_room_aliases.update(team_config.rooms)
        return all_room_aliases

    def get_entity_model_name(self, entity_name: str) -> str:
        """Get the model name for an agent, team, or router.

        Args:
            entity_name: Name of the entity (agent, team, or router)

        Returns:
            Model name (e.g., "default", "gpt-4", etc.)

        Raises:
            ValueError: If entity_name is not found in configuration

        """
        # Router uses router model
        if entity_name == ROUTER_AGENT_NAME:
            return self.router.model
        # Teams use their configured model (required to have one)
        if entity_name in self.teams:
            model = self.teams[entity_name].model
            if model is None:
                msg = f"Team {entity_name} has no model configured"
                raise ValueError(msg)
            return model
        # Regular agents use their configured model
        if entity_name in self.agents:
            return self.agents[entity_name].model

        # Entity not found in any category
        available = sorted(set(self.agents.keys()) | set(self.teams.keys()) | {ROUTER_AGENT_NAME})
        msg = f"Unknown entity: {entity_name}. Available entities: {', '.join(available)}"
        raise ValueError(msg)

    def get_configured_bots_for_room(self, room_id: str) -> set[str]:
        """Get the set of bot usernames that should be in a specific room.

        Args:
            room_id: The Matrix room ID

        Returns:
            Set of bot usernames (without domain) that should be in this room

        """
        from .matrix.rooms import resolve_room_aliases  # noqa: PLC0415

        configured_bots = set()

        # Check which agents should be in this room
        for agent_name, agent_config in self.agents.items():
            resolved_rooms = set(resolve_room_aliases(agent_config.rooms))
            if room_id in resolved_rooms:
                configured_bots.add(f"mindroom_{agent_name}")

        # Check which teams should be in this room
        for team_name, team_config in self.teams.items():
            resolved_rooms = set(resolve_room_aliases(team_config.rooms))
            if room_id in resolved_rooms:
                configured_bots.add(f"mindroom_{team_name}")

        # Router should be in any room that has any configured agents/teams
        if configured_bots:  # If any bots are configured for this room
            configured_bots.add(f"mindroom_{ROUTER_AGENT_NAME}")

        return configured_bots

    def save_to_yaml(self, config_path: Path | None = None) -> None:
        """Save the config to a YAML file, excluding None values.

        Args:
            config_path: Path to save the config to. If None, uses DEFAULT_AGENTS_CONFIG.

        """
        path = config_path or DEFAULT_AGENTS_CONFIG
        config_dict = self.model_dump(exclude_none=True)
        path_obj = Path(path)
        path_obj.parent.mkdir(parents=True, exist_ok=True)
        tmp_path = path_obj.with_suffix(path_obj.suffix + ".tmp")
        with tmp_path.open("w", encoding="utf-8") as f:
            yaml.dump(
                config_dict,
                f,
                default_flow_style=False,
                sort_keys=True,
                allow_unicode=True,  # Preserve Unicode characters like Ã«
                width=120,  # Wider lines to reduce wrapping
            )
        safe_replace(tmp_path, path_obj)
        logger.info(f"Saved configuration to {path}")

================================================================================
# FILE: src/mindroom/config_commands.py
================================================================================

"""Configuration command handling for user-driven config changes."""

from __future__ import annotations

import shlex
from pathlib import Path  # noqa: TC003
from typing import Any

import yaml
from pydantic import ValidationError

from .config import Config
from .constants import DEFAULT_AGENTS_CONFIG
from .logging_config import get_logger

logger = get_logger(__name__)


def parse_config_args(args_text: str) -> tuple[str, list[str]]:
    """Parse config command arguments.

    Args:
        args_text: Raw argument text from command

    Returns:
        Tuple of (operation, arguments)

    """
    if not args_text:
        return "show", []

    # Use shlex to handle quoted strings properly
    try:
        parts = shlex.split(args_text)
    except ValueError as e:
        # Handle parsing errors (e.g., unmatched quotes)
        # Return a special operation that will trigger an error message
        return "parse_error", [str(e)]

    if not parts:
        return "show", []

    operation = parts[0].lower()
    args = parts[1:] if len(parts) > 1 else []
    return operation, args


def get_nested_value(data: Any, path: str) -> Any:  # noqa: ANN401
    """Get a value from nested dict using dot notation.

    Args:
        data: The dictionary to search
        path: Dot-separated path (e.g., "agents.analyst.display_name")

    Returns:
        The value at the path

    Raises:
        KeyError: If path doesn't exist

    """
    keys = path.split(".")
    current = data

    for key in keys:
        # Handle array indexing
        if key.isdigit():  # noqa: SIM108
            current = current[int(key)]
        else:
            current = current[key]

    return current


def set_nested_value(data: Any, path: str, value: Any) -> None:  # noqa: ANN401
    """Set a value in nested dict using dot notation.

    Args:
        data: The dictionary to modify
        path: Dot-separated path (e.g., "agents.analyst.display_name")
        value: Value to set

    Raises:
        KeyError: If parent path doesn't exist

    """
    keys = path.split(".")
    current = data

    # Navigate to the parent of the target
    for key in keys[:-1]:
        if key.isdigit():
            current = current[int(key)]
        elif key not in current:
            # Auto-create missing intermediate dicts
            current[key] = {}
            current = current[key]
        else:
            current = current[key]

    # Set the final value
    final_key = keys[-1]
    if final_key.isdigit():
        current[int(final_key)] = value
    else:
        current[final_key] = value


def parse_value(value_str: str) -> Any:  # noqa: ANN401
    """Parse a string value into appropriate Python type.

    Args:
        value_str: String representation of value

    Returns:
        Parsed value (str, int, float, bool, list, or dict)

    """
    # Try to parse as YAML first (handles unquoted strings in arrays/dicts)
    # YAML is a superset of JSON, so this handles both formats
    # Examples that work:
    #   [item1, item2]          -> ['item1', 'item2']
    #   ["item1", "item2"]      -> ['item1', 'item2']
    #   {key: value}            -> {'key': 'value'}
    #   {"key": "value"}        -> {'key': 'value'}
    try:
        return yaml.safe_load(value_str)
    except yaml.YAMLError:
        pass

    # If YAML parsing fails, return as string
    # This handles cases where the string itself contains special YAML characters
    return value_str


def format_value(value: Any) -> str:  # noqa: ANN401
    """Format a value for display as YAML.

    Args:
        value: Value to format

    Returns:
        YAML formatted string representation

    """
    # Use yaml.dump for consistent formatting
    yaml_str = yaml.dump(value, default_flow_style=False, sort_keys=False, allow_unicode=True)
    # Remove trailing newline and document end marker that yaml.dump adds
    yaml_str = yaml_str.rstrip()
    if yaml_str.endswith("..."):
        yaml_str = yaml_str[:-3].rstrip()
    return yaml_str


async def handle_config_command(args_text: str, config_path: Path | None = None) -> tuple[str, dict[str, Any] | None]:  # noqa: C901, PLR0911, PLR0912
    """Handle config command execution.

    Args:
        args_text: The command arguments
        config_path: Optional path to config file

    Returns:
        Tuple of (response message, config change dict or None)
        The config change dict contains info needed for confirmation

    """
    operation, args = parse_config_args(args_text)
    path = config_path or DEFAULT_AGENTS_CONFIG

    # Load current config
    config = Config.from_yaml(path)
    config_dict = config.model_dump(exclude_none=True)

    if operation == "show":
        # Show entire config
        yaml_str = yaml.dump(config_dict, default_flow_style=False, sort_keys=False, allow_unicode=True)
        return f"**Current Configuration:**\n```yaml\n{yaml_str}```", None

    if operation == "get":
        if not args:
            return (
                "âŒ Please specify a configuration path to get\nExample: `!config get agents.analyst.display_name`",
                None,
            )

        config_path_str = args[0]
        try:
            value = get_nested_value(config_dict, config_path_str)
        except (KeyError, IndexError) as e:
            return f"âŒ Configuration path not found: `{config_path_str}`\nError: {e}", None
        else:
            formatted = format_value(value)
            return f"**Configuration value for `{config_path_str}`:**\n```yaml\n{formatted}\n```", None

    elif operation == "set":
        if len(args) < 2:
            return (
                'âŒ Please specify a path and value\nExample: `!config set agents.analyst.display_name "New Name"`',
                None,
            )

        config_path_str = args[0]
        # Join remaining args as the value (handles unquoted strings with spaces)
        value_str = " ".join(args[1:])

        # Parse the value - YAML parsing handles both quoted and unquoted formats
        value = parse_value(value_str)

        # Get the current value for comparison
        try:
            old_value = get_nested_value(config_dict, config_path_str)
        except (KeyError, IndexError):
            old_value = None  # Path doesn't exist yet

        # Create a copy to test the change
        test_config_dict = config_dict.copy()

        try:
            # Verify the path exists or can be created
            set_nested_value(test_config_dict, config_path_str, value)

            # Validate the modified config
            Config(**test_config_dict)  # This will raise ValidationError if invalid
        except (KeyError, IndexError) as e:
            return f"âŒ Configuration path error: `{config_path_str}`\nError: {e}", None
        except ValidationError as e:
            # Validation failed - explain why
            errors = []
            for error in e.errors():
                location = " â†’ ".join(str(loc) for loc in error["loc"])
                errors.append(f"â€¢ {location}: {error['msg']}")
            error_msg = "\n".join(errors)
            return f"âŒ Invalid configuration:\n{error_msg}\n\nChanges were NOT applied.", None
        else:
            # Format the preview message
            formatted_old = format_value(old_value) if old_value is not None else "Not set"
            formatted_new = format_value(value)

            preview_msg = (
                f"**Configuration Change Preview**\n\n"
                f"ðŸ“ **Path:** `{config_path_str}`\n\n"
                f"**Current value:**\n```yaml\n{formatted_old}\n```\n"
                f"**New value:**\n```yaml\n{formatted_new}\n```\n\n"
                f"React with âœ… to confirm or âŒ to cancel this change."
            )

            # Return the preview and the change info for confirmation
            change_info = {
                "config_path": config_path_str,
                "old_value": old_value,
                "new_value": value,
                "path": str(path),
            }

            return preview_msg, change_info

    elif operation == "parse_error":
        # Handle parsing errors (e.g., unmatched quotes)
        error_msg = args[0] if args else "Unknown parsing error"
        return (
            f"âŒ **Command parsing error:**\n{error_msg}\n\n"
            "**Common issues:**\n"
            "â€¢ Unmatched quotes: Make sure quotes are properly paired\n"
            'â€¢ For JSON arrays/objects, use matching quotes: `["item1", "item2"]`\n'
            "â€¢ Or use single quotes consistently: `['item1', 'item2']`\n\n"
            "**Example:**\n"
            '`!config set agents.analyst.tools ["tool1", "tool2"]`'
        ), None

    else:
        available_ops = ["show", "get", "set"]
        return (
            f"âŒ Unknown operation: '{operation}'\n"
            f"Available operations: {', '.join(available_ops)}\n\n"
            "Try `!help config` for usage examples."
        ), None


async def apply_config_change(
    config_path_str: str,
    new_value: Any,  # noqa: ANN401
    config_file_path: Path | None = None,
) -> str:
    """Apply a confirmed configuration change.

    Args:
        config_path_str: The configuration path (e.g., "agents.analyst.role")
        new_value: The new value to set
        config_file_path: Optional path to config file

    Returns:
        Success or error message

    """
    path = config_file_path or DEFAULT_AGENTS_CONFIG

    try:
        # Load the current configuration
        config = Config.from_yaml(path)
        config_dict = config.model_dump()

        # Apply the specific change
        set_nested_value(config_dict, config_path_str, new_value)

        # Validate the modified config
        try:
            new_config = Config(**config_dict)
        except ValidationError as ve:
            errors = ["âŒ Configuration validation failed:"]
            for error in ve.errors():
                location = " â†’ ".join(str(loc) for loc in error["loc"])
                errors.append(f"â€¢ {location}: {error['msg']}")
            error_msg = "\n".join(errors)
            return f"{error_msg}\n\nChanges were NOT applied."

        # Save to file
        new_config.save_to_yaml(path)
        return (  # noqa: TRY300
            f"âœ… **Configuration updated successfully!**\n\n"
            f"Changes saved to {path} and will affect new agent interactions."
        )
    except Exception as e:
        logger.exception("Failed to apply config change")
        return f"âŒ Failed to apply configuration change: {e}"

================================================================================
# FILE: src/mindroom/config_confirmation.py
================================================================================

"""Configuration change confirmation system using Matrix reactions with persistence."""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import UTC, datetime
from typing import TYPE_CHECKING, Any

import nio

from .logging_config import get_logger

if TYPE_CHECKING:
    from .bot import AgentBot

logger = get_logger(__name__)

# Event type for pending config changes in Matrix state
PENDING_CONFIG_EVENT_TYPE = "com.mindroom.pending.config"

# Maximum age for pending confirmations (24 hours)
MAX_PENDING_AGE_HOURS = 24


@dataclass
class PendingConfigChange:
    """Represents a pending configuration change awaiting confirmation."""

    room_id: str
    thread_id: str | None
    config_path: str
    old_value: Any
    new_value: Any
    requester: str  # User who requested the change
    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))

    def is_expired(self) -> bool:
        """Check if this pending change has expired."""
        age = datetime.now(UTC) - self.created_at
        return age.total_seconds() > MAX_PENDING_AGE_HOURS * 3600

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for Matrix state storage."""
        return {
            "room_id": self.room_id,
            "thread_id": self.thread_id,
            "config_path": self.config_path,
            "old_value": self.old_value,
            "new_value": self.new_value,
            "requester": self.requester,
            "created_at": self.created_at.isoformat(),
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> PendingConfigChange:
        """Create from dictionary retrieved from Matrix state."""
        # Parse the ISO format datetime
        created_at = datetime.fromisoformat(data["created_at"])

        return cls(
            room_id=data["room_id"],
            thread_id=data.get("thread_id"),
            config_path=data["config_path"],
            old_value=data["old_value"],
            new_value=data["new_value"],
            requester=data["requester"],
            created_at=created_at,
        )


# Track pending configuration changes by event_id
_pending_changes: dict[str, PendingConfigChange] = {}


def register_pending_change(
    event_id: str,
    room_id: str,
    thread_id: str | None,
    config_path: str,
    old_value: Any,  # noqa: ANN401
    new_value: Any,  # noqa: ANN401
    requester: str,
) -> None:
    """Register a pending configuration change for confirmation.

    Args:
        event_id: The event ID of the confirmation message
        room_id: The room ID
        thread_id: Thread ID if in a thread
        config_path: The configuration path being changed
        old_value: The current value
        new_value: The proposed new value
        requester: User ID who requested the change

    """
    _pending_changes[event_id] = PendingConfigChange(
        room_id=room_id,
        thread_id=thread_id,
        config_path=config_path,
        old_value=old_value,
        new_value=new_value,
        requester=requester,
    )
    logger.info(
        "Registered pending config change",
        event_id=event_id,
        path=config_path,
        requester=requester,
    )


def get_pending_change(event_id: str) -> PendingConfigChange | None:
    """Get a pending configuration change by event ID.

    Args:
        event_id: The event ID of the confirmation message

    Returns:
        The pending change or None if not found

    """
    return _pending_changes.get(event_id)


def remove_pending_change(event_id: str) -> PendingConfigChange | None:
    """Remove and return a pending configuration change.

    Args:
        event_id: The event ID of the confirmation message

    Returns:
        The removed pending change or None if not found

    """
    return _pending_changes.pop(event_id, None)


async def store_pending_change_in_matrix(
    client: nio.AsyncClient,
    event_id: str,
    pending_change: PendingConfigChange,
) -> None:
    """Store pending config change in Matrix room state for persistence.

    Args:
        client: The Matrix client
        event_id: The event ID of the confirmation message
        pending_change: The pending configuration change

    """
    try:
        response = await client.room_put_state(
            room_id=pending_change.room_id,
            event_type=PENDING_CONFIG_EVENT_TYPE,
            content=pending_change.to_dict(),
            state_key=event_id,
        )

        if isinstance(response, nio.RoomPutStateResponse):
            logger.info(
                "Stored pending config change in Matrix state",
                event_id=event_id,
                room_id=pending_change.room_id,
                config_path=pending_change.config_path,
            )
        else:
            logger.error(
                "Failed to store pending config change in Matrix state",
                event_id=event_id,
                error=str(response),
            )
    except Exception:
        logger.exception("Error storing pending config change in Matrix state")


async def remove_pending_change_from_matrix(
    client: nio.AsyncClient,
    room_id: str,
    event_id: str,
) -> None:
    """Remove pending config change from Matrix room state.

    Args:
        client: The Matrix client
        room_id: The room ID
        event_id: The event ID of the confirmation message

    """
    try:
        # To remove a state event, set it with empty content
        response = await client.room_put_state(
            room_id=room_id,
            event_type=PENDING_CONFIG_EVENT_TYPE,
            content={},
            state_key=event_id,
        )

        if isinstance(response, nio.RoomPutStateResponse):
            logger.info(
                "Removed pending config change from Matrix state",
                event_id=event_id,
                room_id=room_id,
            )
        else:
            logger.error(
                "Failed to remove pending config change from Matrix state",
                event_id=event_id,
                error=str(response),
            )
    except Exception:
        logger.exception("Error removing pending config change from Matrix state")


async def restore_pending_changes(client: nio.AsyncClient, room_id: str) -> int:
    """Restore pending config changes from Matrix state after bot restart.

    Args:
        client: The Matrix client
        room_id: The room ID to restore from

    Returns:
        Number of pending changes restored

    """
    try:
        response = await client.room_get_state(room_id)
        if not isinstance(response, nio.RoomGetStateResponse):
            logger.warning(
                "Failed to get room state for pending config changes",
                room_id=room_id,
                error=str(response),
            )
            return 0

        restored_count = 0
        expired_count = 0

        for event in response.events:
            if event.get("type") != PENDING_CONFIG_EVENT_TYPE:
                continue

            state_key = event.get("state_key")
            content = event.get("content", {})

            # Skip empty content (deleted state events)
            if not content:
                continue

            try:
                pending_change = PendingConfigChange.from_dict(content)

                # Check if expired
                if pending_change.is_expired():
                    logger.info(
                        "Skipping expired pending config change",
                        event_id=state_key,
                        created_at=pending_change.created_at,
                    )
                    # Remove from Matrix state
                    await remove_pending_change_from_matrix(client, room_id, state_key)
                    expired_count += 1
                else:
                    # Restore to memory
                    _pending_changes[state_key] = pending_change
                    restored_count += 1
                    logger.info(
                        "Restored pending config change",
                        event_id=state_key,
                        config_path=pending_change.config_path,
                        requester=pending_change.requester,
                    )
            except Exception:
                logger.exception(
                    "Error restoring pending config change",
                    event_id=state_key,
                )

        if restored_count > 0 or expired_count > 0:
            logger.info(
                "Completed restoration of pending config changes",
                room_id=room_id,
                restored=restored_count,
                expired=expired_count,
            )

        return restored_count  # noqa: TRY300

    except Exception:
        logger.exception("Error restoring pending config changes from Matrix state")
        return 0


def cleanup() -> None:
    """Clean up when shutting down."""
    _pending_changes.clear()


async def add_confirmation_reactions(client: nio.AsyncClient, room_id: str, event_id: str) -> None:
    """Add confirmation reaction buttons to a config change message.

    Args:
        client: The Matrix client
        room_id: The room ID
        event_id: The event ID of the message to add reactions to

    """
    # Add âœ… reaction
    confirm_response = await client.room_send(
        room_id=room_id,
        message_type="m.reaction",
        content={
            "m.relates_to": {
                "rel_type": "m.annotation",
                "event_id": event_id,
                "key": "âœ…",
            },
        },
    )
    if not isinstance(confirm_response, nio.RoomSendResponse):
        logger.warning("Failed to add confirm reaction", error=str(confirm_response))

    # Add âŒ reaction
    cancel_response = await client.room_send(
        room_id=room_id,
        message_type="m.reaction",
        content={
            "m.relates_to": {
                "rel_type": "m.annotation",
                "event_id": event_id,
                "key": "âŒ",
            },
        },
    )
    if not isinstance(cancel_response, nio.RoomSendResponse):
        logger.warning("Failed to add cancel reaction", error=str(cancel_response))


async def handle_confirmation_reaction(
    bot: AgentBot,
    room: nio.MatrixRoom,
    event: nio.ReactionEvent,
    pending_change: PendingConfigChange,
) -> None:
    """Handle reactions to config confirmation messages.

    Args:
        bot: The agent bot instance
        room: The room the reaction occurred in
        event: The reaction event
        pending_change: The pending configuration change

    """
    # Only process reactions from the requester
    if event.sender != pending_change.requester:
        logger.debug(
            "Ignoring config reaction from non-requester",
            sender=event.sender,
            requester=pending_change.requester,
        )
        return

    # Don't process our own reactions
    assert bot.client is not None
    if event.sender == bot.client.user_id:
        return

    reaction_key = event.key

    # Only handle âœ… and âŒ reactions
    if reaction_key not in ["âœ…", "âŒ"]:
        return

    # Remove the pending change from memory and Matrix state
    remove_pending_change(event.reacts_to)
    await remove_pending_change_from_matrix(
        bot.client,
        pending_change.room_id,
        event.reacts_to,
    )

    if reaction_key == "âœ…":
        # User confirmed - apply the change
        from .config_commands import apply_config_change  # noqa: PLC0415

        response_text = await apply_config_change(
            pending_change.config_path,
            pending_change.new_value,
        )

        logger.info(
            "Config change confirmed",
            path=pending_change.config_path,
            requester=event.sender,
        )
    else:
        # User cancelled
        response_text = "âŒ Configuration change cancelled."
        logger.info(
            "Config change cancelled",
            path=pending_change.config_path,
            requester=event.sender,
        )

    # Send the response
    await bot._send_response(
        room.room_id,
        event.reacts_to,  # Reply to the confirmation message
        response_text,
        pending_change.thread_id,
        skip_mentions=True,
    )

================================================================================
# FILE: src/mindroom/constants.py
================================================================================

"""Shared constants for the mindroom package.

This module contains constants that are used across multiple modules
to avoid circular imports. It does not import anything from the internal
codebase.
"""

import os
import shutil
from pathlib import Path

from dotenv import load_dotenv

load_dotenv()

# Agent names
ROUTER_AGENT_NAME = "router"

# Default path to agents configuration file. Allow overriding via environment
# variable so deployments can place the writable configuration file on a
# persistent volume instead of the package directory (which may be read-only).
_CONFIG_PATH_ENV = os.getenv("MINDROOM_CONFIG_PATH") or os.getenv("CONFIG_PATH")
DEFAULT_AGENTS_CONFIG = (
    Path(_CONFIG_PATH_ENV).expanduser() if _CONFIG_PATH_ENV else Path(__file__).parent.parent.parent / "config.yaml"
)

# Optional template path used to seed the writable config file if it does not
# exist yet. Defaults to the same location as DEFAULT_AGENTS_CONFIG so the
# behaviour is unchanged when no overrides are provided.
_CONFIG_TEMPLATE_ENV = os.getenv("MINDROOM_CONFIG_TEMPLATE") or os.getenv("CONFIG_TEMPLATE_PATH")
DEFAULT_CONFIG_TEMPLATE = Path(_CONFIG_TEMPLATE_ENV).expanduser() if _CONFIG_TEMPLATE_ENV else DEFAULT_AGENTS_CONFIG

STORAGE_PATH = os.getenv("STORAGE_PATH", "mindroom_data")
STORAGE_PATH_OBJ = Path(STORAGE_PATH)

# Specific files and directories
MATRIX_STATE_FILE = STORAGE_PATH_OBJ / "matrix_state.yaml"
TRACKING_DIR = STORAGE_PATH_OBJ / "tracking"
MEMORY_DIR = STORAGE_PATH_OBJ / "memory"
CREDENTIALS_DIR = STORAGE_PATH_OBJ / "credentials"
ENCRYPTION_KEYS_DIR = STORAGE_PATH_OBJ / "encryption_keys"

# Other constants
VOICE_PREFIX = "ðŸŽ¤ "
ENABLE_STREAMING = os.getenv("MINDROOM_ENABLE_STREAMING", "true").lower() != "false"
ENABLE_AI_CACHE = os.getenv("ENABLE_AI_CACHE", "true").lower() != "false"

# Matrix
MATRIX_HOMESERVER = os.getenv("MATRIX_HOMESERVER", "http://localhost:8008")
# (for federation setups where hostname != server_name)
MATRIX_SERVER_NAME = os.getenv("MATRIX_SERVER_NAME", None)
MATRIX_SSL_VERIFY = os.getenv("MATRIX_SSL_VERIFY", "true").lower() != "false"


def safe_replace(tmp_path: Path, target_path: Path) -> None:
    """Replace *target_path* with *tmp_path*, with a fallback for bind mounts.

    ``Path.replace`` performs an atomic rename which fails on some filesystems
    (e.g. Docker bind mounts) with ``OSError: [Errno 16] Device or resource
    busy``.  When that happens we fall back to a non-atomic copy.
    """
    try:
        tmp_path.replace(target_path)
    except OSError:
        shutil.copy2(tmp_path, target_path)
        tmp_path.unlink(missing_ok=True)

================================================================================
# FILE: src/mindroom/credentials.py
================================================================================

"""Unified credentials management for MindRoom.

This module provides centralized credential storage and retrieval for all integrations,
used by both agents and the widget interface.
"""

import json
from pathlib import Path
from typing import Any

from .constants import CREDENTIALS_DIR


class CredentialsManager:
    """Centralized credentials storage and retrieval for MindRoom."""

    def __init__(self, base_path: Path | None = None) -> None:
        """Initialize the credentials manager.

        Args:
            base_path: Base directory for storing credentials.
                      Defaults to STORAGE_PATH/credentials (usually mindroom_data/credentials)

        """
        if base_path is None:
            self.base_path = CREDENTIALS_DIR
        else:
            self.base_path = Path(base_path)

        # Ensure the directory exists
        self.base_path.mkdir(parents=True, exist_ok=True)

    def get_credentials_path(self, service: str) -> Path:
        """Get the path for a service's credentials file.

        Args:
            service: Name of the service (e.g., 'google', 'homeassistant')

        Returns:
            Path to the credentials file

        """
        return self.base_path / f"{service}_credentials.json"

    def load_credentials(self, service: str) -> dict[str, Any] | None:
        """Load credentials for a service.

        Args:
            service: Name of the service

        Returns:
            Credentials dictionary or None if not found

        """
        credentials_path = self.get_credentials_path(service)
        if credentials_path.exists():
            try:
                with credentials_path.open() as f:
                    data: dict[str, Any] = json.load(f)
                    return data
            except Exception:
                return None
        return None

    def save_credentials(self, service: str, credentials: dict[str, Any]) -> None:
        """Save credentials for a service.

        Args:
            service: Name of the service
            credentials: Credentials dictionary to save

        """
        credentials_path = self.get_credentials_path(service)
        with credentials_path.open("w") as f:
            json.dump(credentials, f, indent=2)

    def delete_credentials(self, service: str) -> None:
        """Delete credentials for a service.

        Args:
            service: Name of the service

        """
        credentials_path = self.get_credentials_path(service)
        if credentials_path.exists():
            credentials_path.unlink()

    def list_services(self) -> list[str]:
        """List all services with stored credentials.

        Returns:
            List of service names

        """
        services = []
        if self.base_path.exists():
            for path in self.base_path.glob("*_credentials.json"):
                service = path.stem.replace("_credentials", "")
                services.append(service)
        return sorted(services)

    def get_api_key(self, service: str, key_name: str = "api_key") -> str | None:
        """Get an API key for a service.

        Args:
            service: Name of the service (e.g., 'openai', 'anthropic')
            key_name: Name of the key field (default: 'api_key')

        Returns:
            API key string or None if not found

        """
        credentials = self.load_credentials(service)
        if credentials:
            return credentials.get(key_name)
        return None

    def set_api_key(self, service: str, api_key: str, key_name: str = "api_key") -> None:
        """Set an API key for a service.

        Args:
            service: Name of the service
            api_key: The API key to store
            key_name: Name of the key field (default: 'api_key')

        """
        credentials = self.load_credentials(service) or {}
        credentials[key_name] = api_key
        self.save_credentials(service, credentials)


# Global instance for convenience (lazy initialization)
_credentials_manager: CredentialsManager | None = None


def get_credentials_manager() -> CredentialsManager:
    """Get the global credentials manager instance.

    Returns:
        The global CredentialsManager instance

    """
    global _credentials_manager
    if _credentials_manager is None:
        _credentials_manager = CredentialsManager()
    return _credentials_manager

================================================================================
# FILE: src/mindroom/credentials_sync.py
================================================================================

"""Sync API keys from environment variables to CredentialsManager.

On first run, API keys from .env are seeded into the CredentialsManager.
On subsequent runs, env-sourced credentials (_source=env) are updated from
.env, but UI-sourced credentials (_source=ui) are never overwritten.
This lets users change keys via the UI without losing them on restart,
while still picking up .env changes for keys that were never manually set.
"""

import os
from pathlib import Path

from .credentials import get_credentials_manager
from .logging_config import get_logger

logger = get_logger(__name__)

# Mapping of environment variables to service names
ENV_TO_SERVICE_MAP = {
    "OPENAI_API_KEY": "openai",
    "ANTHROPIC_API_KEY": "anthropic",
    "GOOGLE_API_KEY": "google",  # Also used for Gemini
    "OPENROUTER_API_KEY": "openrouter",
    "DEEPSEEK_API_KEY": "deepseek",
    "CEREBRAS_API_KEY": "cerebras",
    "GROQ_API_KEY": "groq",
    "OLLAMA_HOST": "ollama",  # Special case: host instead of API key
}


def _get_secret(name: str) -> str | None:
    """Read a secret from NAME or NAME_FILE.

    If env var `NAME` is set, return it. Otherwise, if `NAME_FILE` points to
    a readable file, return its stripped contents. Else return None.
    """
    val = os.getenv(name)
    if val:
        return val
    file_var = f"{name}_FILE"
    file_path = os.getenv(file_var)
    if file_path and Path(file_path).exists():
        try:
            return Path(file_path).read_text(encoding="utf-8").strip()
        except Exception:
            # Avoid noisy logs here; callers can handle None gracefully
            return None
    return None


def sync_env_to_credentials() -> None:
    """Sync API keys from environment variables into CredentialsManager.

    - If no credential file exists for a service, seed it from .env.
    - If the existing credential has ``_source=env``, update it from .env
      (the user never touched it via UI, so .env should still win).
    - If the existing credential has ``_source=ui`` (or no ``_source``,
      for legacy files), skip it to protect the user's manual override.

    Environment variables are always exported to ``os.environ`` so that
    libraries like mem0 can pick them up regardless.
    """
    creds_manager = get_credentials_manager()
    synced_count = 0

    for env_var, service in ENV_TO_SERVICE_MAP.items():
        env_value = _get_secret(env_var)

        if not env_value:
            logger.debug(f"No value found for {env_var} or {env_var}_FILE")
            continue

        logger.debug(f"Found value for {env_var}: length={len(env_value)}")

        # Always export to os.environ so libraries (mem0, etc.) can use it
        if service != "ollama":
            os.environ[env_var] = env_value

        # Check existing credentials and their source
        existing = creds_manager.load_credentials(service)
        if existing is not None:
            source = existing.get("_source")
            if source != "env":
                # UI-set or legacy (no _source) â€” don't overwrite
                logger.debug(f"Credentials for {service} not env-sourced, skipping env sync")
                continue

        if service == "ollama":
            new_creds = {"host": env_value, "_source": "env"}
        else:
            new_creds = {"api_key": env_value, "_source": "env"}

        creds_manager.save_credentials(service, new_creds)
        if existing is None:
            logger.info(f"Seeded {service} credentials from environment")
        else:
            logger.info(f"Updated {service} credentials from environment")
        synced_count += 1

    if synced_count > 0:
        logger.info(f"Synced {synced_count} credentials from environment")
    else:
        logger.debug("No credentials to sync from environment")


def get_api_key_for_provider(provider: str) -> str | None:
    """Get API key for a provider, checking CredentialsManager first.

    Since we sync from .env to CredentialsManager on startup,
    CredentialsManager will always have the latest keys from .env.

    Args:
        provider: The provider name (e.g., 'openai', 'anthropic')

    Returns:
        The API key if found, None otherwise

    """
    creds_manager = get_credentials_manager()

    # Special case for Ollama - return None as it doesn't use API keys
    if provider == "ollama":
        return None

    # For Google/Gemini, both use the same key
    if provider == "gemini":
        provider = "google"

    return creds_manager.get_api_key(provider)


def get_ollama_host() -> str | None:
    """Get Ollama host configuration.

    Returns:
        The Ollama host URL if configured, None otherwise

    """
    creds_manager = get_credentials_manager()
    ollama_creds = creds_manager.load_credentials("ollama")
    if ollama_creds:
        return ollama_creds.get("host")
    return None

================================================================================
# FILE: src/mindroom/custom_tools/__init__.py
================================================================================

"""MindRoom custom tools package."""

from .gmail import GmailTools
from .google_calendar import GoogleCalendarTools
from .google_sheets import GoogleSheetsTools
from .homeassistant import HomeAssistantTools

__all__ = ["GmailTools", "GoogleCalendarTools", "GoogleSheetsTools", "HomeAssistantTools"]

================================================================================
# FILE: src/mindroom/custom_tools/config_manager.py
================================================================================

"""Consolidated ConfigManager tool for building and managing MindRoom agents."""

from __future__ import annotations

import re
from enum import Enum
from pathlib import Path
from typing import Literal

import yaml
from agno.tools import Toolkit

from mindroom.commands import get_command_help
from mindroom.config import AgentConfig, AgentLearningMode, Config, TeamConfig
from mindroom.constants import DEFAULT_AGENTS_CONFIG
from mindroom.logging_config import get_logger
from mindroom.tools_metadata import TOOL_METADATA, ToolCategory, ToolStatus

logger = get_logger(__name__)


class InfoType(str, Enum):
    """Types of information that can be retrieved."""

    MINDROOM_DOCS = "mindroom_docs"
    CONFIG_SCHEMA = "config_schema"
    AVAILABLE_MODELS = "available_models"
    AGENTS = "agents"
    TEAMS = "teams"
    AVAILABLE_TOOLS = "available_tools"
    TOOL_DETAILS = "tool_details"
    AGENT_CONFIG = "agent_config"
    AGENT_TEMPLATE = "agent_template"


class ConfigManagerTools(Toolkit):
    """Consolidated tools for managing MindRoom agent configurations.

    This toolkit provides comprehensive agent building capabilities with a minimal
    number of tools to reduce cognitive load on AI models.
    """

    def __init__(self, config_path: Path | None = None) -> None:
        """Initialize the ConfigManager toolkit.

        Args:
            config_path: Optional path to configuration file

        """
        self.config_path = config_path or DEFAULT_AGENTS_CONFIG
        self._mindroom_docs: str | None = None
        self._help_text: str | None = None

        # Register only the consolidated tools
        super().__init__(
            name="config_manager",
            tools=[
                self.get_info,
                self.manage_agent,
                self.manage_team,
            ],
        )

    def get_info(  # noqa: C901, PLR0911, PLR0912
        self,
        info_type: str,
        name: str | None = None,
    ) -> str:
        """Get various types of information about MindRoom, agents, tools, and configuration.

        Args:
            info_type: Type of information to retrieve. Options:
                - "mindroom_docs": MindRoom documentation and help
                - "config_schema": Configuration schema for agents and teams
                - "available_models": List of configured AI models
                - "agents": List all configured agents
                - "teams": List all configured teams
                - "available_tools": List all available tools by category
                - "tool_details": Get details about a specific tool (requires name)
                - "agent_config": Get configuration for a specific agent (requires name)
                - "agent_template": Generate template for agent type (requires name as type)
            name: Optional name/identifier for specific queries (tool name, agent name, or template type)

        Returns:
            Requested information as formatted string

        """
        try:
            if info_type == InfoType.MINDROOM_DOCS:
                return self._get_mindroom_info()
            if info_type == InfoType.CONFIG_SCHEMA:
                return self._get_config_schema()
            if info_type == InfoType.AVAILABLE_MODELS:
                return self._get_available_models()
            if info_type == InfoType.AGENTS:
                return self._list_agents()
            if info_type == InfoType.TEAMS:
                return self._list_teams()
            if info_type == InfoType.AVAILABLE_TOOLS:
                return self._list_available_tools()
            if info_type == InfoType.TOOL_DETAILS:
                if not name:
                    return "Error: tool_details requires 'name' parameter with the tool name"
                return self._get_tool_details(name)
            if info_type == InfoType.AGENT_CONFIG:
                if not name:
                    return "Error: agent_config requires 'name' parameter with the agent name"
                return self._get_agent_config(name)
            if info_type == InfoType.AGENT_TEMPLATE:
                if not name:
                    return "Error: agent_template requires 'name' parameter with the template type (researcher, developer, social, communicator, analyst, productivity)"
                return self._generate_agent_template(name)
            return f"Error: Unknown info_type '{info_type}'. Valid options: {', '.join([t.value for t in InfoType])}"
        except Exception as e:
            logger.exception(f"Failed to get info for type {info_type}")
            return f"Error getting {info_type}: {e}"

    def manage_agent(
        self,
        operation: Literal["create", "update", "validate"],
        agent_name: str,
        display_name: str | None = None,
        role: str | None = None,
        tools: list[str] | None = None,
        instructions: list[str] | None = None,
        model: str | None = None,
        rooms: list[str] | None = None,
        markdown: bool | None = None,
        learning: bool | None = None,
        learning_mode: AgentLearningMode | None = None,
    ) -> str:
        """Manage agent configurations - create, update, or validate agents.

        Args:
            operation: Operation to perform - "create", "update", or "validate"
            agent_name: Internal name for the agent (alphanumeric, lowercase)
            display_name: Human-readable display name (required for create)
            role: Description of the agent's purpose (required for create)
            tools: List of tool names to enable for the agent
            instructions: List of instructions for the agent
            model: Model to use (default: "default")
            rooms: List of room IDs or names to auto-join
            markdown: Whether to use markdown formatting
            learning: Whether to enable Agno Learning for this agent
            learning_mode: Learning mode for Agno Learning ("always" or "agentic")

        Returns:
            Success message or error details

        """
        if operation == "create":
            if not display_name:
                return "Error: display_name is required for create operation"
            if role is None:
                role = ""
            return self._create_agent_config(
                agent_name=agent_name,
                display_name=display_name,
                role=role,
                tools=tools or [],
                instructions=instructions or [],
                model=model or "default",
                rooms=rooms or [],
                markdown=markdown,
                learning=learning,
                learning_mode=learning_mode,
            )
        if operation == "update":
            return self._update_agent_config(
                agent_name=agent_name,
                display_name=display_name,
                role=role,
                tools=tools,
                instructions=instructions,
                model=model,
                rooms=rooms,
                markdown=markdown,
                learning=learning,
                learning_mode=learning_mode,
            )
        if operation == "validate":
            return self._validate_agent_config(agent_name)
        return f"Error: Unknown operation '{operation}'. Valid options: create, update, validate"

    def manage_team(
        self,
        team_name: str,
        display_name: str,
        role: str,
        agents: list[str],
        mode: str = "coordinate",
    ) -> str:
        """Create or manage team configurations.

        Args:
            team_name: Internal name for the team
            display_name: Human-readable display name
            role: Description of the team's purpose
            agents: List of agent names that compose this team
            mode: Team mode - "coordinate" or "collaborate"

        Returns:
            Success message or error details

        """
        return self._create_team_config(team_name, display_name, role, agents, mode)

    # ===== Internal helper methods (not exposed as tools) =====

    def _load_mindroom_docs(self) -> str:
        """Load MindRoom documentation once and cache it."""
        if self._mindroom_docs is None:
            readme_path = Path(__file__).parent.parent.parent.parent / "README.md"
            try:
                with readme_path.open() as f:
                    self._mindroom_docs = f.read()
            except Exception as e:
                logger.warning(f"Could not load README.md: {e}")
                self._mindroom_docs = "README.md not available"
        return self._mindroom_docs

    def _load_help_text(self) -> str:
        """Load help text once and cache it."""
        if self._help_text is None:
            self._help_text = get_command_help()
        return self._help_text

    def _get_available_models(self) -> str:
        """Get the list of configured models from the current configuration."""
        try:
            config = Config.from_yaml(self.config_path)

            output = ["# Available Models\n"]

            if not config.models:
                return "No models configured in the system."

            output.append("These models are currently configured and can be used:\n")

            for model_name, model_config in config.models.items():
                provider = model_config.provider
                model_id = model_config.id

                output.append(f"## `{model_name}`")
                output.append(f"- **Provider**: {provider}")
                output.append(f"- **Model ID**: {model_id}")

                if model_config.host:
                    output.append(f"- **Host**: {model_config.host}")

                if model_name == "default":
                    output.append("- **Note**: This is typically the system default model")

                output.append("")

            if config.router and config.router.model:
                output.append("## Router Configuration")
                output.append(f"The router uses model: `{config.router.model}`")
                output.append("")

            return "\n".join(output)
        except Exception as e:
            return f"Error loading model configuration: {e}"

    def _format_schema_field(self, field: str, info: dict, required_fields: list) -> list[str]:
        """Format a single schema field for display."""
        lines = []
        required = field in required_fields
        field_type = info.get("type", "unknown")
        description = info.get("description", "")
        default = info.get("default")

        if "enum" in info:
            field_type = f"enum: {info['enum']}"

        lines.append(f"{field}:  # {field_type}")
        if description:
            lines.append(f"  # {description}")
        if required:
            lines.append("  # REQUIRED")
        elif default is not None:
            lines.append(f"  # Default: {default}")
        lines.append("")
        return lines

    def _get_config_schema(self) -> str:
        """Get the JSON schema for MindRoom configuration."""
        output = ["# MindRoom Configuration Schema\n"]

        agent_schema = AgentConfig.model_json_schema()
        team_schema = TeamConfig.model_json_schema()

        output.append("## Agent Configuration Fields\n")
        output.append("```yaml")
        output.append("# Required fields:")
        for field, info in agent_schema.get("properties", {}).items():
            output.extend(self._format_schema_field(field, info, agent_schema.get("required", [])))
        output.append("```\n")

        output.append("## Team Configuration Fields\n")
        output.append("```yaml")
        for field, info in team_schema.get("properties", {}).items():
            output.extend(self._format_schema_field(field, info, team_schema.get("required", [])))
        output.append("```\n")

        output.append("## Team Modes")
        if "properties" in team_schema and "mode" in team_schema["properties"]:
            mode_info = team_schema["properties"]["mode"]
            if "enum" in mode_info:
                output.extend(f"- `{mode}`: {mode.title()} mode" for mode in mode_info["enum"])

        return "\n".join(output)

    def _get_mindroom_info(self) -> str:
        """Get comprehensive information about MindRoom."""
        docs = self._load_mindroom_docs()
        help_text = self._load_help_text()

        return f"""# MindRoom Documentation

## README Content:
{docs}

## Available Commands:
{help_text}

## Key Concepts:
- **Agents**: AI assistants with specific roles and tools
- **Teams**: Groups of agents that collaborate
- **Tools**: Integrations that give agents capabilities (80+ available)
- **Memory**: Persistent conversation memory across sessions
- **Threading**: Agents respond in threads for organized conversations
- **Routing**: Smart agent selection based on message content
- **Commands**: Special !commands for configuration and control
"""

    def _list_agents(self) -> str:
        """List all configured agents and their details."""
        try:
            config = Config.from_yaml(self.config_path)
            agents_info = []

            for name, agent in config.agents.items():
                tools_str = ", ".join(agent.tools) if agent.tools else "No tools"
                role_line = f"  - Role: {agent.role[:100]}..." if len(agent.role) > 100 else f"  - Role: {agent.role}"
                agents_info.append(
                    f"**{name}** ({agent.display_name})\n"
                    f"{role_line}\n"
                    f"  - Tools: {tools_str}\n"
                    f"  - Model: {agent.model}\n",
                )

            if not agents_info:
                return "No agents configured yet."

            return "## Configured Agents:\n\n" + "\n".join(agents_info)
        except Exception as e:
            return f"Error loading agents: {e}"

    def _list_teams(self) -> str:
        """List all configured teams and their composition."""
        try:
            config = Config.from_yaml(self.config_path)
            teams_info = []

            for name, team in config.teams.items():
                agents_str = ", ".join(team.agents)
                teams_info.append(
                    f"**{name}** ({team.display_name})\n"
                    f"  - Role: {team.role}\n"
                    f"  - Agents: {agents_str}\n"
                    f"  - Mode: {team.mode}\n",
                )

            if not teams_info:
                return "No teams configured yet."

            return "## Configured Teams:\n\n" + "\n".join(teams_info)
        except Exception as e:
            return f"Error loading teams: {e}"

    def _list_available_tools(self) -> str:
        """List all available tools that can be used by agents."""
        tools_by_category: dict[str, list[tuple[str, str]]] = {}

        for tool_name in sorted(TOOL_METADATA.keys()):
            if tool_name in TOOL_METADATA:
                metadata = TOOL_METADATA[tool_name]
                category = metadata.category.value
                description = metadata.description

                if category not in tools_by_category:
                    tools_by_category[category] = []
                tools_by_category[category].append((tool_name, description))
            else:
                if "uncategorized" not in tools_by_category:
                    tools_by_category["uncategorized"] = []
                tools_by_category["uncategorized"].append((tool_name, "No description available"))

        output = ["## Available Tools by Category:\n"]
        for category in sorted(tools_by_category.keys()):
            output.append(f"\n### {category.title()}:")
            for tool_name, description in tools_by_category[category]:
                output.append(f"- **{tool_name}**: {description}")

        return "\n".join(output)

    def _get_tool_details(self, tool_name: str) -> str:
        """Get detailed information about a specific tool."""
        if tool_name not in TOOL_METADATA:
            available = ", ".join(sorted(TOOL_METADATA.keys()))
            return f"Unknown tool: {tool_name}\n\nAvailable tools: {available}"

        output = [f"## Tool: {tool_name}\n"]

        if tool_name in TOOL_METADATA:
            metadata = TOOL_METADATA[tool_name]
            output.append(f"**Display Name**: {metadata.display_name}")
            output.append(f"**Description**: {metadata.description}")
            output.append(f"**Category**: {metadata.category.value}")
            output.append(f"**Status**: {metadata.status.value}")
            output.append(f"**Setup Type**: {metadata.setup_type.value}")

            if metadata.config_fields:
                output.append("\n**Configuration Fields**:")
                for field in metadata.config_fields:
                    required = "Required" if field.required else "Optional"
                    output.append(f"- **{field.name}** ({field.type}, {required}): {field.description}")
                    if field.default is not None:
                        output.append(f"  Default: {field.default}")

            if metadata.dependencies:
                output.append(f"\n**Dependencies**: {', '.join(metadata.dependencies)}")

            if metadata.docs_url:
                output.append(f"\n**Documentation**: {metadata.docs_url}")
        else:
            output.append("No metadata available for this tool.")

        return "\n".join(output)

    def _create_agent_config(
        self,
        agent_name: str,
        display_name: str,
        role: str,
        tools: list[str],
        instructions: list[str],
        model: str,
        rooms: list[str],
        markdown: bool | None,
        learning: bool | None,
        learning_mode: AgentLearningMode | None,
    ) -> str:
        """Create a new agent configuration."""
        # Validate agent name
        if not re.match(r"^[a-z0-9_]+$", agent_name):
            return "Error: Agent name must be lowercase alphanumeric with underscores only"

        # Validate tools
        invalid_tools = [t for t in tools if t not in TOOL_METADATA]
        if invalid_tools:
            return f"Error: Unknown tools: {', '.join(invalid_tools)}\n\nUse get_info with info_type='available_tools' to see valid tools."

        try:
            config = Config.from_yaml(self.config_path)

            if agent_name in config.agents:
                return f"Error: Agent '{agent_name}' already exists. Use manage_agent with operation='update' to modify it."

            # Create new agent config
            new_agent = AgentConfig(
                display_name=display_name,
                role=role,
                tools=tools,
                instructions=instructions,
                model=model,
                rooms=rooms,
                markdown=markdown,
                learning=learning,
                learning_mode=learning_mode,
            )

            # Add to config
            config.agents[agent_name] = new_agent

            # Save config
            config.save_to_yaml(self.config_path)

            # Build success message
            tools_str = ", ".join(tools) if tools else "None"
            rooms_str = ", ".join(rooms) if rooms else "None"
            return (  # noqa: TRY300
                f"âœ… Successfully created agent '{agent_name}'!\n\n"
                f"**Configuration:**\n"
                f"- Display Name: {display_name}\n"
                f"- Role: {role}\n"
                f"- Tools: {tools_str}\n"
                f"- Model: {model}\n"
                f"- Rooms: {rooms_str}\n\n"
                f"The agent is now available and can be mentioned with @{agent_name}"
            )
        except Exception as e:
            logger.exception("Failed to create agent")
            return f"Error creating agent: {e}"

    def _update_agent_config(
        self,
        agent_name: str,
        display_name: str | None,
        role: str | None,
        tools: list[str] | None,
        instructions: list[str] | None,
        model: str | None,
        rooms: list[str] | None,
        markdown: bool | None,
        learning: bool | None,
        learning_mode: AgentLearningMode | None,
    ) -> str:
        """Update an existing agent configuration."""
        try:
            config = Config.from_yaml(self.config_path)

            if agent_name not in config.agents:
                return f"Error: Agent '{agent_name}' not found. Use manage_agent with operation='create' to create it."

            agent = config.agents[agent_name]

            # Validate tools if provided
            if tools is not None:
                invalid_tools = [t for t in tools if t not in TOOL_METADATA]
                if invalid_tools:
                    return f"Error: Unknown tools: {', '.join(invalid_tools)}"

            # Map of field names to (new_value, display_formatter)
            updates = {
                "display_name": (display_name, lambda v: v),
                "role": (role, lambda v: v),
                "tools": (tools, lambda v: ", ".join(v) if v else "(empty)"),
                "instructions": (instructions, lambda v: f"{len(v)} instructions" if v else "(empty)"),
                "model": (model, lambda v: v),
                "rooms": (rooms, lambda v: ", ".join(v) if v else "(empty)"),
                "markdown": (markdown, lambda v: str(v)),
                "learning": (learning, lambda v: str(v)),
                "learning_mode": (learning_mode, lambda v: str(v)),
            }

            # Apply updates and track changes
            changes = []
            for field_name, (new_value, formatter) in updates.items():
                if new_value is not None:
                    current_value = getattr(agent, field_name)
                    if new_value != current_value:
                        setattr(agent, field_name, new_value)
                        display_name = field_name.replace("_", " ").title()
                        changes.append(f"{display_name} -> {formatter(new_value)}")

            if not changes:
                return "No changes made. All provided values are the same as current configuration."

            # Save config
            config.save_to_yaml(self.config_path)

            return f"âœ… Successfully updated agent '{agent_name}'!\n\n**Changes:**\n" + "\n".join(
                f"- {c}" for c in changes
            )
        except Exception as e:
            logger.exception("Failed to update agent")
            return f"Error updating agent: {e}"

    def _create_team_config(
        self,
        team_name: str,
        display_name: str,
        role: str,
        agents: list[str],
        mode: str = "coordinate",
    ) -> str:
        """Create a new team configuration."""
        if mode not in ["coordinate", "collaborate"]:
            return "Error: Team mode must be 'coordinate' or 'collaborate'"

        try:
            config = Config.from_yaml(self.config_path)

            if team_name in config.teams:
                return f"Error: Team '{team_name}' already exists."

            # Validate agents exist
            invalid_agents = [a for a in agents if a not in config.agents]
            if invalid_agents:
                return f"Error: Unknown agents: {', '.join(invalid_agents)}"

            # Create new team config
            new_team = TeamConfig(
                display_name=display_name,
                role=role,
                agents=agents,
                mode=mode,
            )

            # Add to config
            config.teams[team_name] = new_team

            # Save config
            config.save_to_yaml(self.config_path)

            return (
                f"âœ… Successfully created team '{team_name}'!\n\n"
                f"**Configuration:**\n"
                f"- Display Name: {display_name}\n"
                f"- Role: {role}\n"
                f"- Agents: {', '.join(agents)}\n"
                f"- Mode: {mode}\n\n"
                f"The team can now be mentioned with @{team_name}"
            )
        except Exception as e:
            logger.exception("Failed to create team")
            return f"Error creating team: {e}"

    def _validate_agent_config(self, agent_name: str) -> str:  # noqa: C901, PLR0912
        """Validate an agent's configuration."""
        try:
            config = Config.from_yaml(self.config_path)

            if agent_name not in config.agents:
                return f"Error: Agent '{agent_name}' not found."

            agent = config.agents[agent_name]
            issues = []
            warnings = []

            # Check display name
            if not agent.display_name:
                issues.append("Missing display name")

            # Check role
            if not agent.role:
                warnings.append("No role description provided")
            elif len(agent.role) < 20:
                warnings.append("Role description is very short")

            # Check tools
            if not agent.tools:
                warnings.append("No tools configured")
            else:
                invalid_tools = [t for t in agent.tools if t not in TOOL_METADATA]
                if invalid_tools:
                    issues.append(f"Invalid tools: {', '.join(invalid_tools)}")

            # Check model
            available_models = list(config.models.keys()) if config.models else []
            if available_models and agent.model not in available_models:
                warnings.append(f"Model '{agent.model}' not in configured models: {', '.join(available_models)}")

            # Format results
            output = [f"## Validation Results for '{agent_name}':\n"]

            if not issues and not warnings:
                output.append("âœ… Configuration is valid!")
            else:
                if issues:
                    output.append("### âŒ Issues (must fix):")
                    output.extend(f"- {issue}" for issue in issues)

                if warnings:
                    output.append("\n### âš ï¸ Warnings (consider fixing):")
                    output.extend(f"- {warning}" for warning in warnings)

            # Add summary
            output.append("\n### Configuration Summary:")
            output.append(f"- Display Name: {agent.display_name}")
            output.append(f"- Role: {agent.role[:100]}..." if len(agent.role) > 100 else f"- Role: {agent.role}")
            output.append(f"- Tools: {', '.join(agent.tools) if agent.tools else 'None'}")
            output.append(f"- Model: {agent.model}")

            return "\n".join(output)
        except Exception as e:
            return f"Error validating agent: {e}"

    def _get_agent_config(self, agent_name: str) -> str:
        """Get the full configuration for a specific agent."""
        try:
            config = Config.from_yaml(self.config_path)

            if agent_name not in config.agents:
                return f"Error: Agent '{agent_name}' not found."

            agent = config.agents[agent_name]
            agent_dict = agent.model_dump(exclude_none=True)

            yaml_str = yaml.dump(agent_dict, default_flow_style=False, sort_keys=False)
        except Exception as e:
            return f"Error loading agent config: {e}"
        else:
            return f"## Configuration for '{agent_name}':\n\n```yaml\n{yaml_str}```"

    def _generate_agent_template(self, agent_type: str) -> str:
        """Generate a template configuration for common agent types."""
        # Map agent types to tool categories
        type_to_category = {
            "researcher": ToolCategory.RESEARCH,
            "developer": ToolCategory.DEVELOPMENT,
            "social": ToolCategory.SOCIAL,
            "communicator": ToolCategory.COMMUNICATION,
            "analyst": ToolCategory.INFORMATION,
            "productivity": ToolCategory.PRODUCTIVITY,
        }

        if agent_type not in type_to_category:
            available = ", ".join(type_to_category.keys())
            return f"Unknown template type: {agent_type}\n\nAvailable templates: {available}"

        category = type_to_category[agent_type]

        # Get tools from this category that are available
        tools = [
            name
            for name, metadata in TOOL_METADATA.items()
            if metadata.category == category and metadata.status == ToolStatus.AVAILABLE
        ][:5]  # Limit to 5 tools

        # Generate role based on category
        role_descriptions = {
            ToolCategory.RESEARCH: "Research specialist focused on finding and analyzing information",
            ToolCategory.DEVELOPMENT: "Software development expert for coding and technical tasks",
            ToolCategory.SOCIAL: "Social interaction specialist for community engagement",
            ToolCategory.COMMUNICATION: "Communication expert for messaging and collaboration",
            ToolCategory.INFORMATION: "Information analyst for data processing and insights",
            ToolCategory.PRODUCTIVITY: "Productivity specialist for task and workflow management",
        }

        role = role_descriptions.get(category, f"Specialist in {category.value} tasks")

        # Generic instructions
        instructions = [
            f"Focus on {category.value} tasks",
            "Provide clear and actionable responses",
            "Use available tools effectively",
        ]

        return f"""## Template for '{agent_type}' agent:

```yaml
display_name: "{agent_type.title()} Agent"
role: "{role}"
tools: {yaml.dump(tools, default_flow_style=True).strip() if tools else "[]"}
instructions: {yaml.dump(instructions, default_flow_style=False).strip()}
model: "default"
```

**Available tools in {category.value} category:**
{chr(10).join(f"- {name}: {metadata.description}" for name, metadata in TOOL_METADATA.items() if metadata.category == category)}

**To create this agent, use:**
```
manage_agent(
    operation="create",
    agent_name="{agent_type}_agent",
    display_name="{agent_type.title()} Agent",
    role="{role}",
    tools={tools},
    instructions={instructions},
)
```"""

================================================================================
# FILE: src/mindroom/custom_tools/gmail.py
================================================================================

"""Custom Gmail Tools wrapper for MindRoom.

This module provides a wrapper around Agno's GmailTools that properly handles
credentials stored in MindRoom's unified credentials location.
"""

from typing import Any

from agno.tools.gmail import GmailTools as AgnoGmailTools
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from loguru import logger

from mindroom.credentials import get_credentials_manager


class GmailTools(AgnoGmailTools):
    """Gmail tools wrapper that uses MindRoom's credential management."""

    def __init__(self, **kwargs: Any) -> None:  # noqa: ANN401
        """Initialize Gmail tools with MindRoom credentials.

        This wrapper automatically loads credentials from MindRoom's
        unified credential storage and passes them to the Agno GmailTools.
        """
        # Load credentials using the credentials manager
        creds_manager = get_credentials_manager()
        token_data = creds_manager.load_credentials("google")
        creds = None

        if token_data:
            try:
                # Create Google Credentials object from stored data
                creds = Credentials(
                    token=token_data.get("token"),
                    refresh_token=token_data.get("refresh_token"),
                    token_uri=token_data.get("token_uri"),
                    client_id=token_data.get("client_id"),
                    client_secret=token_data.get("client_secret"),
                    scopes=token_data.get("scopes", self.DEFAULT_SCOPES),
                )
                logger.info("Loaded Gmail credentials from MindRoom storage")
            except Exception as e:
                logger.error(f"Failed to load Gmail credentials: {e}")
                creds = None
        else:
            logger.warning("Gmail credentials not found in MindRoom storage")

        # Pass credentials to parent class
        super().__init__(creds=creds, **kwargs)

        # Store original auth method for fallback
        self._original_auth = super()._auth

    def _auth(self) -> None:
        """Custom auth method that uses MindRoom's credential storage."""
        # If we already have valid credentials, don't re-authenticate
        if self.creds and self.creds.valid:
            return

        # Reload credentials from MindRoom's storage in case they've been updated
        creds_manager = get_credentials_manager()
        token_data = creds_manager.load_credentials("google")

        if token_data:
            try:
                self.creds = Credentials(
                    token=token_data.get("token"),
                    refresh_token=token_data.get("refresh_token"),
                    token_uri=token_data.get("token_uri"),
                    client_id=token_data.get("client_id"),
                    client_secret=token_data.get("client_secret"),
                    scopes=token_data.get("scopes", self.DEFAULT_SCOPES),
                )

                # Refresh if expired
                if self.creds.expired and self.creds.refresh_token:
                    self.creds.refresh(Request())

                    # Save the refreshed credentials back
                    token_data["token"] = self.creds.token
                    creds_manager.save_credentials("google", token_data)

                logger.info("Gmail authentication successful")
            except Exception as e:
                logger.error(f"Failed to authenticate with Gmail: {e}")
                raise
        else:
            # If no credentials found, fall back to original auth method
            # This will prompt for OAuth flow
            logger.warning("No stored credentials found, initiating OAuth flow")
            self._original_auth()

================================================================================
# FILE: src/mindroom/custom_tools/google_calendar.py
================================================================================

"""Custom Google Calendar Tools wrapper for MindRoom.

This module provides a wrapper around Agno's GoogleCalendarTools that properly handles
credentials stored in MindRoom's unified credentials location.
"""

from typing import Any

from agno.tools.googlecalendar import GoogleCalendarTools as AgnoGoogleCalendarTools
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from loguru import logger

from mindroom.credentials import get_credentials_manager


class GoogleCalendarTools(AgnoGoogleCalendarTools):
    """Google Calendar tools wrapper that uses MindRoom's credential management."""

    def __init__(self, **kwargs: Any) -> None:  # noqa: ANN401
        """Initialize Google Calendar tools with MindRoom credentials.

        This wrapper automatically loads credentials from MindRoom's
        unified credential storage and passes them to the Agno GoogleCalendarTools.
        """
        # Load credentials using the credentials manager
        creds_manager = get_credentials_manager()
        token_data = creds_manager.load_credentials("google")
        creds = None

        if token_data:
            try:
                # Create Google Credentials object from stored data
                creds = Credentials(
                    token=token_data.get("token"),
                    refresh_token=token_data.get("refresh_token"),
                    token_uri=token_data.get("token_uri"),
                    client_id=token_data.get("client_id"),
                    client_secret=token_data.get("client_secret"),
                    scopes=token_data.get("scopes", self.DEFAULT_SCOPES),
                )
                logger.info("Loaded Google Calendar credentials from MindRoom storage")
            except Exception as e:
                logger.error(f"Failed to load Google Calendar credentials: {e}")
                creds = None
        else:
            logger.warning("Google Calendar credentials not found in MindRoom storage")

        # Pass credentials to parent class
        super().__init__(creds=creds, **kwargs)

        # Store original auth method for fallback
        self._original_auth = super()._auth

    def _auth(self) -> None:
        """Custom auth method that uses MindRoom's credential storage."""
        # If we already have valid credentials, don't re-authenticate
        if self.creds and self.creds.valid:
            return

        # Reload credentials from MindRoom's storage in case they've been updated
        creds_manager = get_credentials_manager()
        token_data = creds_manager.load_credentials("google")

        if token_data:
            try:
                self.creds = Credentials(
                    token=token_data.get("token"),
                    refresh_token=token_data.get("refresh_token"),
                    token_uri=token_data.get("token_uri"),
                    client_id=token_data.get("client_id"),
                    client_secret=token_data.get("client_secret"),
                    scopes=token_data.get("scopes", self.DEFAULT_SCOPES),
                )

                # Refresh if expired
                if self.creds.expired and self.creds.refresh_token:
                    self.creds.refresh(Request())

                    # Save the refreshed credentials back
                    token_data["token"] = self.creds.token
                    creds_manager.save_credentials("google", token_data)

                logger.info("Google Calendar authentication successful")
            except Exception as e:
                logger.error(f"Failed to authenticate with Google Calendar: {e}")
                raise
        else:
            # If no credentials found, fall back to original auth method
            # This will prompt for OAuth flow
            logger.warning("No stored credentials found, initiating OAuth flow")
            self._original_auth()

================================================================================
# FILE: src/mindroom/custom_tools/google_sheets.py
================================================================================

"""Custom Google Sheets Tools wrapper for MindRoom.

This module provides a wrapper around Agno's GoogleSheetsTools that properly handles
credentials stored in MindRoom's unified credentials location.
"""

from typing import Any

from agno.tools.googlesheets import GoogleSheetsTools as AgnoGoogleSheetsTools
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from loguru import logger

from mindroom.credentials import get_credentials_manager


class GoogleSheetsTools(AgnoGoogleSheetsTools):
    """Google Sheets tools wrapper that uses MindRoom's credential management."""

    def __init__(self, **kwargs: Any) -> None:  # noqa: ANN401
        """Initialize Google Sheets tools with MindRoom credentials.

        This wrapper automatically loads credentials from MindRoom's
        unified credential storage and passes them to the Agno GoogleSheetsTools.
        """
        # Load credentials using the credentials manager
        creds_manager = get_credentials_manager()
        token_data = creds_manager.load_credentials("google")
        creds = None

        if token_data:
            try:
                # Create Google Credentials object from stored data
                creds = Credentials(
                    token=token_data.get("token"),
                    refresh_token=token_data.get("refresh_token"),
                    token_uri=token_data.get("token_uri"),
                    client_id=token_data.get("client_id"),
                    client_secret=token_data.get("client_secret"),
                    scopes=token_data.get("scopes", self.DEFAULT_SCOPES),
                )
                logger.info("Loaded Google Sheets credentials from MindRoom storage")
            except Exception as e:
                logger.error(f"Failed to load Google Sheets credentials: {e}")
                creds = None
        else:
            logger.warning("Google Sheets credentials not found in MindRoom storage")

        # Pass credentials to parent class
        super().__init__(creds=creds, **kwargs)

        # Store original auth method for fallback
        self._original_auth = super()._auth

    def _auth(self) -> None:
        """Custom auth method that uses MindRoom's credential storage."""
        # If we already have valid credentials, don't re-authenticate
        if self.creds and self.creds.valid:
            return

        # Reload credentials from MindRoom's storage in case they've been updated
        creds_manager = get_credentials_manager()
        token_data = creds_manager.load_credentials("google")

        if token_data:
            try:
                self.creds = Credentials(
                    token=token_data.get("token"),
                    refresh_token=token_data.get("refresh_token"),
                    token_uri=token_data.get("token_uri"),
                    client_id=token_data.get("client_id"),
                    client_secret=token_data.get("client_secret"),
                    scopes=token_data.get("scopes", self.DEFAULT_SCOPES),
                )

                # Refresh if expired
                if self.creds.expired and self.creds.refresh_token:
                    self.creds.refresh(Request())

                    # Save the refreshed credentials back
                    token_data["token"] = self.creds.token
                    creds_manager.save_credentials("google", token_data)

                logger.info("Google Sheets authentication successful")
            except Exception as e:
                logger.error(f"Failed to authenticate with Google Sheets: {e}")
                raise
        else:
            # If no credentials found, fall back to original auth method
            # This will prompt for OAuth flow
            logger.warning("No stored credentials found, initiating OAuth flow")
            self._original_auth()

================================================================================
# FILE: src/mindroom/custom_tools/homeassistant.py
================================================================================

"""Home Assistant tools for MindRoom agents.

This module provides tools for interacting with Home Assistant,
allowing agents to control devices, query states, and execute automations.
"""

import json
from typing import Any
from urllib.parse import urljoin

import httpx
from agno.tools import Toolkit

from mindroom.credentials import get_credentials_manager


class HomeAssistantTools(Toolkit):
    """Tools for interacting with Home Assistant."""

    def __init__(self) -> None:
        """Initialize Home Assistant tools."""
        # Use the credentials manager
        self._creds_manager = get_credentials_manager()
        self._config: dict[str, Any] | None = None

        # Initialize the toolkit with all available methods
        super().__init__(
            name="homeassistant",
            tools=[
                self.get_entity_state,
                self.list_entities,
                self.turn_on,
                self.turn_off,
                self.toggle,
                self.set_brightness,
                self.set_color,
                self.set_temperature,
                self.activate_scene,
                self.trigger_automation,
                self.call_service,
            ],
        )

    def _load_config(self) -> dict[str, Any] | None:
        """Load Home Assistant configuration from unified location."""
        if self._config:
            return self._config

        # Load from credentials manager
        self._config = self._creds_manager.load_credentials("homeassistant")
        return self._config

    async def _api_request(  # noqa: PLR0911
        self,
        method: str,
        endpoint: str,
        json_data: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """Make an API request to Home Assistant."""
        config = self._load_config()
        if not config:
            return {"error": "Home Assistant is not configured. Please connect through the widget."}

        instance_url = config.get("instance_url")
        token = config.get("access_token") or config.get("long_lived_token")

        if not instance_url or not token:
            return {"error": "Missing Home Assistant credentials"}

        try:
            async with httpx.AsyncClient() as client:
                response = await client.request(
                    method=method,
                    url=urljoin(instance_url, endpoint),
                    headers={"Authorization": f"Bearer {token}"},
                    json=json_data,
                    timeout=10.0,
                )

                if response.status_code == 401:
                    return {"error": "Invalid authentication token. Please reconnect Home Assistant."}
                if response.status_code not in (200, 201):
                    return {"error": f"API error: {response.text}"}

                return response.json() if response.text else {"success": True}

        except (httpx.ConnectTimeout, httpx.ReadTimeout, httpx.WriteTimeout, httpx.PoolTimeout):
            return {"error": "Connection timeout - check if Home Assistant is accessible"}
        except httpx.RequestError as e:
            return {"error": f"Connection error: {e!s}"}
        except Exception as e:
            return {"error": f"Unexpected error: {e!s}"}

    async def get_entity_state(self, entity_id: str) -> str:
        """Get the current state of a Home Assistant entity.

        Args:
            entity_id: The entity ID (e.g., 'light.living_room', 'switch.bedroom_fan')

        Returns:
            JSON string with entity state information

        """
        result = await self._api_request("GET", f"/api/states/{entity_id}")

        if "error" in result:
            return json.dumps(result)

        return json.dumps(
            {
                "entity_id": result.get("entity_id"),
                "state": result.get("state"),
                "attributes": result.get("attributes", {}),
                "last_changed": result.get("last_changed"),
            },
        )

    async def list_entities(self, domain: str = "") -> str:
        """List all entities in Home Assistant, optionally filtered by domain.

        Args:
            domain: Optional domain to filter by (e.g., 'light', 'switch', 'sensor')

        Returns:
            JSON string with list of entities

        """
        result = await self._api_request("GET", "/api/states")

        if isinstance(result, dict) and "error" in result:
            return json.dumps(result)

        entities = result

        # Filter by domain if specified
        if domain and isinstance(entities, list):
            entities = [e for e in entities if e["entity_id"].startswith(f"{domain}.")]

        # Simplify the response
        entity_list: list[Any] = entities[:50] if isinstance(entities, list) else []
        simplified: list[dict[str, Any]] = [
            {
                "entity_id": e["entity_id"],
                "state": e["state"],
                "friendly_name": e.get("attributes", {}).get("friendly_name", e["entity_id"]),
            }
            for e in entity_list  # Limit to 50 entities to avoid huge responses
        ]

        return json.dumps(simplified)

    async def turn_on(self, entity_id: str) -> str:
        """Turn on a device (light, switch, etc.).

        Args:
            entity_id: The entity ID to turn on (e.g., 'light.living_room')

        Returns:
            JSON string with result

        """
        domain = entity_id.split(".")[0]
        result = await self._api_request(
            "POST",
            f"/api/services/{domain}/turn_on",
            {"entity_id": entity_id},
        )
        return json.dumps(result)

    async def turn_off(self, entity_id: str) -> str:
        """Turn off a device (light, switch, etc.).

        Args:
            entity_id: The entity ID to turn off (e.g., 'light.living_room')

        Returns:
            JSON string with result

        """
        domain = entity_id.split(".")[0]
        result = await self._api_request(
            "POST",
            f"/api/services/{domain}/turn_off",
            {"entity_id": entity_id},
        )
        return json.dumps(result)

    async def toggle(self, entity_id: str) -> str:
        """Toggle a device (if on, turn off; if off, turn on).

        Args:
            entity_id: The entity ID to toggle (e.g., 'switch.bedroom_fan')

        Returns:
            JSON string with result

        """
        domain = entity_id.split(".")[0]
        result = await self._api_request(
            "POST",
            f"/api/services/{domain}/toggle",
            {"entity_id": entity_id},
        )
        return json.dumps(result)

    async def set_brightness(self, entity_id: str, brightness: int) -> str:
        """Set the brightness of a light.

        Args:
            entity_id: The light entity ID (e.g., 'light.living_room')
            brightness: Brightness level (0-255, where 255 is 100%)

        Returns:
            JSON string with result

        """
        if not 0 <= brightness <= 255:
            return json.dumps({"error": "Brightness must be between 0 and 255"})

        result = await self._api_request(
            "POST",
            "/api/services/light/turn_on",
            {
                "entity_id": entity_id,
                "brightness": brightness,
            },
        )
        return json.dumps(result)

    async def set_color(self, entity_id: str, red: int, green: int, blue: int) -> str:
        """Set the color of a light using RGB values.

        Args:
            entity_id: The light entity ID (e.g., 'light.living_room')
            red: Red value (0-255)
            green: Green value (0-255)
            blue: Blue value (0-255)

        Returns:
            JSON string with result

        """
        if not all(0 <= v <= 255 for v in [red, green, blue]):
            return json.dumps({"error": "RGB values must be between 0 and 255"})

        result = await self._api_request(
            "POST",
            "/api/services/light/turn_on",
            {
                "entity_id": entity_id,
                "rgb_color": [red, green, blue],
            },
        )
        return json.dumps(result)

    async def set_temperature(self, entity_id: str, temperature: float) -> str:
        """Set the temperature of a climate device.

        Args:
            entity_id: The climate entity ID (e.g., 'climate.thermostat')
            temperature: Target temperature in the unit configured in Home Assistant

        Returns:
            JSON string with result

        """
        result = await self._api_request(
            "POST",
            "/api/services/climate/set_temperature",
            {
                "entity_id": entity_id,
                "temperature": temperature,
            },
        )
        return json.dumps(result)

    async def activate_scene(self, scene_id: str) -> str:
        """Activate a Home Assistant scene.

        Args:
            scene_id: The scene entity ID (e.g., 'scene.movie_time')

        Returns:
            JSON string with result

        """
        result = await self._api_request(
            "POST",
            "/api/services/scene/turn_on",
            {"entity_id": scene_id},
        )
        return json.dumps(result)

    async def trigger_automation(self, automation_id: str) -> str:
        """Trigger a Home Assistant automation.

        Args:
            automation_id: The automation entity ID (e.g., 'automation.morning_routine')

        Returns:
            JSON string with result

        """
        result = await self._api_request(
            "POST",
            "/api/services/automation/trigger",
            {"entity_id": automation_id},
        )
        return json.dumps(result)

    async def call_service(self, domain: str, service: str, entity_id: str = "", data: str = "") -> str:
        """Call a generic Home Assistant service.

        Args:
            domain: The service domain (e.g., 'light', 'switch', 'notify')
            service: The service name (e.g., 'turn_on', 'toggle', 'send_message')
            entity_id: The entity ID(s) to apply the service to (optional)
            data: Additional service data as JSON string (optional)

        Returns:
            JSON string with result

        """
        service_data = {}

        if entity_id:
            service_data["entity_id"] = entity_id

        if data:
            try:
                additional_data = json.loads(data)
                service_data.update(additional_data)
            except json.JSONDecodeError:
                return json.dumps({"error": "Invalid JSON in data parameter"})

        result = await self._api_request(
            "POST",
            f"/api/services/{domain}/{service}",
            service_data,
        )
        return json.dumps(result)

================================================================================
# FILE: src/mindroom/custom_tools/memory.py
================================================================================

"""Explicit memory tools for MindRoom agents.

Gives agents conscious control over their memory â€” they can deliberately
store and search facts on demand, complementing the automatic/unconscious
memory extraction that happens after every response.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from agno.tools import Toolkit

from mindroom.logging_config import get_logger
from mindroom.memory.functions import (
    add_agent_memory,
    delete_agent_memory,
    get_agent_memory,
    list_all_agent_memories,
    search_agent_memories,
    update_agent_memory,
)

if TYPE_CHECKING:
    from pathlib import Path

    from mindroom.config import Config

logger = get_logger(__name__)


class MemoryTools(Toolkit):
    """Tools that let an agent explicitly store and search its own memories."""

    def __init__(self, agent_name: str, storage_path: Path, config: Config) -> None:
        self._agent_name = agent_name
        self._storage_path = storage_path
        self._config = config

        super().__init__(
            name="memory",
            tools=[
                self.add_memory,
                self.search_memories,
                self.list_memories,
                self.get_memory,
                self.update_memory,
                self.delete_memory,
            ],
        )

    async def add_memory(self, content: str) -> str:
        """Store a specific fact or piece of information in your memory.

        Use this when explicitly asked to remember something, or when you
        encounter important information worth retaining for future conversations.

        Args:
            content: The fact or information to memorize.

        Returns:
            Confirmation message.

        """
        try:
            await add_agent_memory(
                content,
                self._agent_name,
                self._storage_path,
                self._config,
                metadata={"source": "explicit_tool"},
            )
        except Exception as e:
            logger.exception("Failed to add memory via tool", agent=self._agent_name, error=str(e))
            return f"Failed to store memory: {e}"
        else:
            return f"Memorized: {content}"

    async def search_memories(self, query: str, limit: int = 5) -> str:
        """Search your memories for information relevant to a query.

        Use this when you need to recall previously stored facts or context.

        Args:
            query: What to search for in your memories.
            limit: Maximum number of results to return (default 5).

        Returns:
            Formatted list of matching memories, or a message if none found.

        """
        try:
            results = await search_agent_memories(
                query,
                self._agent_name,
                self._storage_path,
                self._config,
                limit=limit,
            )
            if not results:
                return "No relevant memories found."

            lines = [f"Found {len(results)} memory(ies):"]
            for i, mem in enumerate(results, 1):
                mid = mem.get("id", "?")
                lines.append(f"{i}. [id={mid}] {mem.get('memory', '')}")
            return "\n".join(lines)
        except Exception as e:
            logger.exception("Failed to search memories via tool", agent=self._agent_name, error=str(e))
            return f"Failed to search memories: {e}"

    async def list_memories(self, limit: int = 50) -> str:
        """List all your stored memories.

        Use this when asked to show, list, or dump all memories.

        Args:
            limit: Maximum number of memories to return (default 50).

        Returns:
            Formatted list of all memories, or a message if none exist.

        """
        try:
            results = await list_all_agent_memories(
                self._agent_name,
                self._storage_path,
                self._config,
                limit=limit,
            )
            if not results:
                return "No memories stored yet."

            lines = [f"All memories ({len(results)}):"]
            for i, mem in enumerate(results, 1):
                mid = mem.get("id", "?")
                lines.append(f"{i}. [id={mid}] {mem.get('memory', '')}")
            return "\n".join(lines)
        except Exception as e:
            logger.exception("Failed to list memories via tool", agent=self._agent_name, error=str(e))
            return f"Failed to list memories: {e}"

    async def get_memory(self, memory_id: str) -> str:
        """Retrieve a single memory by its ID.

        Use this to inspect the full details of a specific memory.

        Args:
            memory_id: The ID of the memory to retrieve (shown in search/list results as [id=...]).

        Returns:
            The memory content, or an error message if not found.

        """
        try:
            result = await get_agent_memory(memory_id, self._agent_name, self._storage_path, self._config)
            if result is None:
                return f"No memory found with id={memory_id}"
            return f"[id={result.get('id', memory_id)}] {result.get('memory', '')}"
        except Exception as e:
            logger.exception("Failed to get memory via tool", agent=self._agent_name, memory_id=memory_id, error=str(e))
            return f"Failed to get memory: {e}"

    async def update_memory(self, memory_id: str, new_content: str) -> str:
        """Update the content of a specific memory by its ID.

        Use this to correct or refine a previously stored memory.

        Args:
            memory_id: The ID of the memory to update (shown in search/list results as [id=...]).
            new_content: The new content to replace the existing memory with.

        Returns:
            Confirmation message.

        """
        try:
            await update_agent_memory(memory_id, new_content, self._agent_name, self._storage_path, self._config)
        except Exception as e:
            logger.exception(
                "Failed to update memory via tool",
                agent=self._agent_name,
                memory_id=memory_id,
                error=str(e),
            )
            return f"Failed to update memory: {e}"
        else:
            return f"Updated memory [id={memory_id}]: {new_content}"

    async def delete_memory(self, memory_id: str) -> str:
        """Delete a single memory by its ID.

        Use this to remove a specific outdated or incorrect memory
        without affecting other memories.

        Args:
            memory_id: The ID of the memory to delete (shown in search/list results as [id=...]).

        Returns:
            Confirmation message.

        """
        try:
            await delete_agent_memory(memory_id, self._agent_name, self._storage_path, self._config)
        except Exception as e:
            logger.exception(
                "Failed to delete memory via tool",
                agent=self._agent_name,
                memory_id=memory_id,
                error=str(e),
            )
            return f"Failed to delete memory: {e}"
        else:
            return f"Deleted memory [id={memory_id}]"

================================================================================
# FILE: src/mindroom/error_handling.py
================================================================================

"""Simple error handling for MindRoom agents."""

from __future__ import annotations

from .logging_config import get_logger

logger = get_logger(__name__)


def _extract_provider_from_error(error: Exception) -> str | None:
    """Try to extract the provider name from the exception's module."""
    module = type(error).__module__ or ""
    # e.g. "openai" from "openai._exceptions", "anthropic" from "anthropic._exceptions"
    top_module = module.split(".")[0] if module else ""
    known_providers = {"openai", "anthropic", "google", "groq", "cerebras", "httpx"}
    if top_module in known_providers:
        return top_module
    return None


def get_user_friendly_error_message(error: Exception, agent_name: str | None = None) -> str:
    """Return a user-friendly error message.

    Args:
        error: The exception that occurred
        agent_name: Optional name of the agent that encountered the error

    Returns:
        A user-friendly error message

    """
    error_str = str(error).lower()
    agent_prefix = f"[{agent_name}] " if agent_name else ""

    # Log the full error for debugging
    logger.error(f"Error in {agent_name or 'agent'}: {error!r}")

    # Only distinguish the most important error types
    if any(x in error_str for x in ["401", "auth", "unauthorized", "api key", "api_key", "apikey"]):
        provider = _extract_provider_from_error(error)
        provider_hint = f" ({provider})" if provider else ""
        return f"{agent_prefix}âŒ Authentication failed{provider_hint}: {error}"
    if any(x in error_str for x in ["rate", "429", "quota"]):
        return f"{agent_prefix}â±ï¸ Rate limited. Please wait a moment and try again."
    if "timeout" in error_str:
        return f"{agent_prefix}â° Request timed out. Please try again."
    # Generic error with the actual error message for transparency
    return f"{agent_prefix}âš ï¸ Error: {error!s}"

================================================================================
# FILE: src/mindroom/file_watcher.py
================================================================================

"""Simple file watcher utility without external dependencies."""

from __future__ import annotations

import asyncio
from pathlib import Path
from typing import TYPE_CHECKING

import structlog

if TYPE_CHECKING:
    from collections.abc import Awaitable, Callable

logger = structlog.get_logger(__name__)


async def watch_file(
    file_path: Path | str,
    callback: Callable[[], Awaitable[None]],
    stop_event: asyncio.Event | None = None,
) -> None:
    """Watch a file for changes and call callback when modified.

    Args:
        file_path: Path to the file to watch
        callback: Async function to call when file changes
        stop_event: Optional event to signal when to stop watching

    """
    file_path = Path(file_path)
    last_mtime = file_path.stat().st_mtime if file_path.exists() else 0

    while stop_event is None or not stop_event.is_set():
        await asyncio.sleep(1.0)  # Check every second

        try:
            if file_path.exists():
                current_mtime = file_path.stat().st_mtime
                if current_mtime != last_mtime:
                    last_mtime = current_mtime
                    await callback()
        except (OSError, PermissionError):
            # File might have been deleted or become unreadable
            # Reset mtime so we detect when it comes back
            last_mtime = 0
        except Exception:
            # Don't let callback errors stop the watcher
            # The callback should handle its own errors
            logger.exception("Exception during file watcher callback - continuing to watch")

================================================================================
# FILE: src/mindroom/interactive.py
================================================================================

"""Interactive Q&A system using Matrix reactions as clickable buttons."""

from __future__ import annotations

import json
import re
from contextlib import suppress
from typing import TYPE_CHECKING, NamedTuple

import nio

from .logging_config import get_logger
from .matrix.event_info import EventInfo
from .matrix.identity import is_agent_id

if TYPE_CHECKING:
    from .config import Config

logger = get_logger(__name__)


class InteractiveQuestion(NamedTuple):
    """Represents an active interactive question."""

    room_id: str
    thread_id: str | None
    options: dict[str, str]  # emoji/number -> value mapping
    creator_agent: str


class InteractiveResponse(NamedTuple):
    """Result of parsing and formatting an interactive response."""

    formatted_text: str
    option_map: dict[str, str] | None
    options_list: list[dict[str, str]] | None


# Track active interactive questions by event_id
_active_questions: dict[str, InteractiveQuestion] = {}

# Constants
# Match interactive code blocks
INTERACTIVE_PATTERN = r"```(?:interactive\s*)?\n(?:interactive\s*\n)?(.*?)\n```"
MAX_OPTIONS = 5
DEFAULT_QUESTION = "Please choose an option:"
INSTRUCTION_TEXT = "React with an emoji or type the number to respond."


def should_create_interactive_question(response_text: str) -> bool:
    """Check if the response contains an interactive question in JSON format.

    Args:
        response_text: The AI's response text

    Returns:
        True if an interactive code block is found

    """
    return bool(re.search(INTERACTIVE_PATTERN, response_text, re.DOTALL))


async def handle_reaction(
    client: nio.AsyncClient,
    event: nio.ReactionEvent,
    agent_name: str,
    config: Config,
) -> tuple[str, str | None] | None:
    """Handle a reaction event that might be an answer to a question.

    Args:
        client: The Matrix client
        event: The reaction event
        agent_name: The name of the agent handling this
        config: Application configuration

    Returns:
        Tuple of (selected_value, thread_id) if this was a valid response, None otherwise

    """
    question = _active_questions.get(event.reacts_to)
    if not question:
        logger.debug(
            "Reaction to unknown message",
            reacts_to=event.reacts_to,
            sender=event.sender,
            reaction=event.key,
            active_questions=list(_active_questions.keys()),
        )
        return None

    # Only the agent who created the question should respond to reactions
    if agent_name != question.creator_agent:
        logger.debug(
            "Ignoring reaction to question created by another agent",
            reacting_agent=agent_name,
            question_creator=question.creator_agent,
            reaction=event.key,
        )
        return None

    reaction_key = event.key
    if reaction_key not in question.options:
        return None

    # Don't process our own reactions
    if event.sender == client.user_id:
        return None

    # Ignore reactions from other agents
    if is_agent_id(event.sender, config):
        logger.debug("Ignoring reaction from agent", sender=event.sender, reaction=reaction_key)
        return None

    selected_value = question.options[reaction_key]

    logger.info(
        "Received answer via reaction",
        user=event.sender,
        reaction=reaction_key,
        value=selected_value,
    )

    # Store the response for the agent to process
    # The agent will continue the conversation based on this selection
    # No confirmation message needed - the emoji reaction itself is the user's response

    with suppress(KeyError):
        del _active_questions[event.reacts_to]

    # Return the selected value and thread_id so the agent can respond
    return (selected_value, question.thread_id)


async def handle_text_response(
    client: nio.AsyncClient,
    room: nio.MatrixRoom,
    event: nio.RoomMessageText,
    agent_name: str,
) -> tuple[str, str | None] | None:
    """Handle text responses to interactive questions (e.g., "1", "2", "3").

    Args:
        client: The Matrix client
        room: The room the message occurred in
        event: The message event
        agent_name: The name of the agent handling this

    Returns:
        Tuple of (selected_value, thread_id) if this was a valid response, None otherwise

    """
    message_text = event.body.strip()

    # Look for numeric responses
    if not message_text.isdigit() or len(message_text) > 1:
        return None

    thread_info = EventInfo.from_event(event.source)
    thread_id = thread_info.thread_id

    # Find matching active questions in this room/thread
    for question_event_id, question in _active_questions.items():
        if question.room_id != room.room_id:
            continue
        if question.thread_id != thread_id:
            continue
        if message_text not in question.options:
            continue
        if event.sender == client.user_id:
            continue
        # Only respond if this agent created the question
        if agent_name != question.creator_agent:
            continue

        # Found a matching question
        selected_value = question.options[message_text]

        logger.info(
            "Received answer via text",
            user=event.sender,
            text=message_text,
            value=selected_value,
        )

        del _active_questions[question_event_id]

        return (selected_value, question.thread_id)

    return None


def parse_and_format_interactive(response_text: str, extract_mapping: bool = False) -> InteractiveResponse:
    """Parse and format interactive content from response text.

    Args:
        response_text: The response text containing interactive JSON
        extract_mapping: Whether to extract option mapping and return options list

    Returns:
        InteractiveResponse with formatted_text, option_map, and options_list

    """
    # Find the first interactive block for processing
    first_match = re.search(INTERACTIVE_PATTERN, response_text, re.DOTALL)

    if not first_match:
        return InteractiveResponse(response_text, None, None)

    try:
        interactive_data = json.loads(first_match.group(1))
    except json.JSONDecodeError:
        return InteractiveResponse(response_text, None, None)

    question = interactive_data.get("question", DEFAULT_QUESTION)
    options = interactive_data.get("options", [])

    if not options:
        return InteractiveResponse(response_text, None, None)

    options = options[:MAX_OPTIONS]
    clean_response = response_text.replace(first_match.group(0), "").strip()

    option_lines = []
    option_map: dict[str, str] | None = {} if extract_mapping else None

    for i, opt in enumerate(options, 1):
        emoji_char = opt.get("emoji", "â“")
        label = opt.get("label", "Option")
        option_lines.append(f"{i}. {emoji_char} {label}")

        if extract_mapping and option_map is not None:
            value = opt.get("value", label.lower())
            option_map[emoji_char] = value
            option_map[str(i)] = value

    # Combine everything into the final message
    message_parts = []
    if clean_response:
        message_parts.append(clean_response)
    message_parts.append("")  # Empty line
    message_parts.append(question)
    message_parts.append("")  # Empty line
    message_parts.extend(option_lines)
    message_parts.append("")  # Empty line
    message_parts.append(INSTRUCTION_TEXT)

    final_text = "\n".join(message_parts)

    return InteractiveResponse(final_text, option_map, options if extract_mapping else None)


def register_interactive_question(
    event_id: str,
    room_id: str,
    thread_id: str | None,
    option_map: dict[str, str],
    agent_name: str,
) -> None:
    """Register an interactive question for tracking.

    Args:
        event_id: The event ID of the message with the question
        room_id: The room ID
        thread_id: Thread ID if in a thread
        option_map: Mapping of emoji/number to values
        agent_name: The agent that created the question

    """
    _active_questions[event_id] = InteractiveQuestion(
        room_id=room_id,
        thread_id=thread_id,
        options=option_map,
        creator_agent=agent_name,
    )
    logger.info("Registered interactive question", event_id=event_id, options=len(option_map))


async def add_reaction_buttons(
    client: nio.AsyncClient,
    room_id: str,
    event_id: str,
    options: list[dict[str, str]],
) -> None:
    """Add reaction buttons to a message.

    Args:
        client: The Matrix client
        room_id: The room ID
        event_id: The event ID of the message to add reactions to
        options: List of option dictionaries with 'emoji' keys

    """
    for opt in options:
        emoji_char = opt.get("emoji", "â“")
        reaction_response = await client.room_send(
            room_id=room_id,
            message_type="m.reaction",
            content={
                "m.relates_to": {
                    "rel_type": "m.annotation",
                    "event_id": event_id,
                    "key": emoji_char,
                },
            },
        )
        if not isinstance(reaction_response, nio.RoomSendResponse):
            logger.warning("Failed to add reaction", emoji=emoji_char, error=str(reaction_response))


def cleanup() -> None:
    """Clean up when shutting down."""
    _active_questions.clear()

================================================================================
# FILE: src/mindroom/knowledge.py
================================================================================

"""Knowledge base management for file-backed RAG."""

from __future__ import annotations

import asyncio
import hashlib
from contextlib import suppress
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING, Any

from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.chroma import ChromaDb
from watchfiles import Change, awatch

from .credentials_sync import get_api_key_for_provider, get_ollama_host
from .logging_config import get_logger

if TYPE_CHECKING:
    from agno.knowledge.embedder.base import Embedder

    from .config import Config, KnowledgeBaseConfig

logger = get_logger(__name__)

_COLLECTION_PREFIX = "mindroom_knowledge"


def _resolve_knowledge_path(path: str) -> Path:
    return Path(path).expanduser().resolve()


def _safe_identifier(value: str) -> str:
    sanitized = "".join(char if char.isalnum() or char in {"_", "-"} else "_" for char in value)
    return sanitized or "default"


def _base_storage_key(base_id: str) -> str:
    digest = hashlib.sha256(base_id.encode("utf-8")).hexdigest()[:8]
    return f"{_safe_identifier(base_id)}_{digest}"


def _collection_name(base_id: str) -> str:
    return f"{_COLLECTION_PREFIX}_{_base_storage_key(base_id)}"


def _knowledge_base_config(config: Config, base_id: str) -> KnowledgeBaseConfig:
    if base_id not in config.knowledge_bases:
        msg = f"Knowledge base '{base_id}' is not configured"
        raise ValueError(msg)
    return config.knowledge_bases[base_id]


def _settings_key(config: Config, storage_path: Path, base_id: str) -> tuple[str, ...]:
    embedder_config = config.memory.embedder.config
    base_config = _knowledge_base_config(config, base_id)
    knowledge_path = _resolve_knowledge_path(base_config.path)
    return (
        base_id,
        str(storage_path.resolve()),
        str(knowledge_path),
        config.memory.embedder.provider,
        embedder_config.model,
        embedder_config.host or "",
        str(base_config.watch),
    )


def _create_embedder(config: Config) -> Embedder:
    provider = config.memory.embedder.provider
    embedder_config = config.memory.embedder.config

    if provider == "openai":
        return OpenAIEmbedder(
            id=embedder_config.model,
            api_key=get_api_key_for_provider("openai"),
            base_url=embedder_config.host,
        )

    if provider == "ollama":
        host = get_ollama_host() or embedder_config.host or "http://localhost:11434"
        return OllamaEmbedder(id=embedder_config.model, host=host)

    msg = f"Unsupported knowledge embedder provider: {provider}. Supported providers: openai, ollama"
    raise ValueError(msg)


@dataclass
class KnowledgeManager:
    """Manage indexing and watching for one knowledge base folder."""

    base_id: str
    config: Config
    storage_path: Path

    knowledge_path: Path = field(init=False)
    _settings: tuple[str, ...] = field(init=False)
    _knowledge: Knowledge = field(init=False)
    _indexed_files: set[str] = field(default_factory=set, init=False)
    _lock: asyncio.Lock = field(default_factory=asyncio.Lock, init=False)
    _watch_task: asyncio.Task[None] | None = field(default=None, init=False)
    _watch_stop_event: asyncio.Event = field(default_factory=asyncio.Event, init=False)

    def __post_init__(self) -> None:
        """Initialize filesystem paths and the underlying vector database."""
        base_config = _knowledge_base_config(self.config, self.base_id)
        self.knowledge_path = _resolve_knowledge_path(base_config.path)
        self.knowledge_path.mkdir(parents=True, exist_ok=True)
        self._settings = _settings_key(self.config, self.storage_path, self.base_id)

        vector_db = ChromaDb(
            collection=_collection_name(self.base_id),
            path=str((self.storage_path / "knowledge_db" / _base_storage_key(self.base_id)).resolve()),
            persistent_client=True,
            embedder=_create_embedder(self.config),
        )
        self._knowledge = Knowledge(vector_db=vector_db)

    def matches(self, config: Config, storage_path: Path) -> bool:
        """Return True when manager settings match the provided config."""
        return self._settings == _settings_key(config, storage_path, self.base_id)

    def get_knowledge(self) -> Knowledge:
        """Return the agno Knowledge instance."""
        return self._knowledge

    def list_files(self) -> list[Path]:
        """List all files currently present in the knowledge folder."""
        if not self.knowledge_path.exists():
            return []
        return sorted(path for path in self.knowledge_path.rglob("*") if path.is_file())

    def resolve_file_path(self, file_path: Path | str) -> Path:
        """Resolve a path and ensure it stays inside the knowledge folder."""
        candidate = Path(file_path)
        resolved = (
            candidate.expanduser().resolve() if candidate.is_absolute() else (self.knowledge_path / candidate).resolve()
        )

        try:
            resolved.relative_to(self.knowledge_path)
        except ValueError as exc:
            msg = f"Path {resolved} is outside knowledge folder {self.knowledge_path}"
            raise ValueError(msg) from exc

        return resolved

    def _relative_path(self, file_path: Path) -> str:
        return file_path.relative_to(self.knowledge_path).as_posix()

    def _reset_collection(self) -> None:
        if self._knowledge.vector_db is None:
            return
        self._knowledge.vector_db.delete()
        self._knowledge.vector_db.create()

    def _load_indexed_files_from_vector_db(self) -> set[str]:
        """Load unique source paths currently present in the vector collection."""
        vector_db = self._knowledge.vector_db
        if not isinstance(vector_db, ChromaDb):
            return set()
        if not vector_db.exists():
            return set()

        collection = vector_db.client.get_collection(name=vector_db.collection_name)
        total_count = collection.count()
        if total_count == 0:
            return set()

        indexed_files: set[str] = set()
        offset = 0
        batch_size = 1_000

        while offset < total_count:
            result = collection.get(
                limit=batch_size,
                offset=offset,
                include=["metadatas"],
            )

            metadatas = result.get("metadatas", []) or []
            for metadata in metadatas:
                if not isinstance(metadata, dict):
                    continue
                source_path = metadata.get("source_path")
                if isinstance(source_path, str) and source_path:
                    indexed_files.add(source_path)

            ids = result.get("ids", []) or []
            fetched_count = len(ids)
            if fetched_count == 0:
                break
            offset += fetched_count

        return indexed_files

    async def initialize(self) -> None:
        """Initialize and index all existing knowledge files."""
        indexed_count = await self.reindex_all()
        logger.info(
            "Knowledge base initialized",
            base_id=self.base_id,
            indexed_count=indexed_count,
            path=str(self.knowledge_path),
        )

    async def load_indexed_files(self) -> int:
        """Load in-memory indexed file state from the existing vector DB collection."""
        indexed_files = await asyncio.to_thread(self._load_indexed_files_from_vector_db)
        async with self._lock:
            self._indexed_files = indexed_files
        return len(indexed_files)

    async def start_watcher(self) -> None:
        """Start background file watching if enabled."""
        base_config = _knowledge_base_config(self.config, self.base_id)
        if not base_config.watch:
            return
        if self._watch_task is not None and not self._watch_task.done():
            return

        self._watch_stop_event = asyncio.Event()
        self._watch_task = asyncio.create_task(self._watch_loop())
        logger.info("Knowledge folder watcher started", base_id=self.base_id, path=str(self.knowledge_path))

    async def stop_watcher(self) -> None:
        """Stop the background file watcher."""
        if self._watch_task is None:
            return

        self._watch_stop_event.set()
        self._watch_task.cancel()
        with suppress(asyncio.CancelledError):
            await self._watch_task

        self._watch_task = None
        logger.info("Knowledge folder watcher stopped", base_id=self.base_id)

    async def _index_file_locked(self, resolved_path: Path, *, upsert: bool) -> bool:
        """Index one file while holding the manager lock."""
        relative_path = self._relative_path(resolved_path)
        metadata = {"source_path": relative_path}

        try:
            if upsert:
                # Agno/Chroma upsert keys by content hash, so stale chunks from an older
                # version of the same file can remain unless we clear by source metadata first.
                await asyncio.to_thread(self._knowledge.remove_vectors_by_metadata, metadata)
            await asyncio.to_thread(
                self._knowledge.insert,
                path=str(resolved_path),
                metadata=metadata,
                upsert=upsert,
            )
        except Exception:
            logger.exception("Failed to index knowledge file", base_id=self.base_id, path=str(resolved_path))
            return False

        self._indexed_files.add(relative_path)
        logger.info("Indexed knowledge file", base_id=self.base_id, path=relative_path)
        return True

    async def reindex_all(self) -> int:
        """Clear and rebuild the knowledge index from disk."""
        files = self.list_files()

        async with self._lock:
            await asyncio.to_thread(self._reset_collection)
            self._indexed_files.clear()
            for file_path in files:
                await self._index_file_locked(file_path, upsert=True)
            return len(self._indexed_files)

    async def index_file(self, file_path: Path | str, *, upsert: bool = True) -> bool:
        """Index or reindex a single file."""
        resolved_path = self.resolve_file_path(file_path)
        if not resolved_path.exists() or not resolved_path.is_file():
            return False

        async with self._lock:
            return await self._index_file_locked(resolved_path, upsert=upsert)

    async def remove_file(self, file_path: Path | str) -> bool:
        """Remove a file from the vector database index."""
        resolved_path = self.resolve_file_path(file_path)
        relative_path = self._relative_path(resolved_path)

        async with self._lock:
            removed = await asyncio.to_thread(
                self._knowledge.remove_vectors_by_metadata,
                {"source_path": relative_path},
            )
            self._indexed_files.discard(relative_path)

        logger.info("Removed knowledge file from index", base_id=self.base_id, path=relative_path, removed=removed)
        return removed

    def get_status(self) -> dict[str, Any]:
        """Get current knowledge indexing status."""
        files = self.list_files()
        return {
            "base_id": self.base_id,
            "folder_path": str(self.knowledge_path),
            "file_count": len(files),
            "indexed_count": len(self._indexed_files),
        }

    async def _watch_loop(self) -> None:
        """Watch the knowledge folder for file changes."""
        async for changes in awatch(self.knowledge_path, stop_event=self._watch_stop_event):
            if not changes:
                continue

            for change, changed_path in changes:
                await self._handle_file_change(change, Path(changed_path))

    async def _handle_file_change(self, change: Change, file_path: Path) -> None:
        """Handle one filesystem change event."""
        try:
            resolved_path = self.resolve_file_path(file_path)
        except ValueError:
            return

        if change in {Change.added, Change.modified}:
            if resolved_path.exists() and resolved_path.is_file():
                await self.index_file(resolved_path, upsert=True)
        elif change == Change.deleted:
            await self.remove_file(resolved_path)


_knowledge_managers: dict[str, KnowledgeManager] = {}


async def initialize_knowledge_managers(
    config: Config,
    storage_path: Path,
    *,
    start_watchers: bool = False,
    reindex_on_create: bool = True,
) -> dict[str, KnowledgeManager]:
    """Initialize process-wide knowledge managers for all configured knowledge bases."""
    configured_base_ids = set(config.knowledge_bases)

    for base_id in sorted(set(_knowledge_managers) - configured_base_ids):
        await _knowledge_managers[base_id].stop_watcher()
        del _knowledge_managers[base_id]

    for base_id in sorted(configured_base_ids):
        existing = _knowledge_managers.get(base_id)

        if existing is not None and existing.matches(config, storage_path):
            existing.config = config
            if start_watchers:
                await existing.start_watcher()
            continue

        if existing is not None:
            await existing.stop_watcher()

        manager = KnowledgeManager(base_id=base_id, config=config, storage_path=storage_path)
        if reindex_on_create:
            await manager.initialize()
        else:
            indexed_count = await manager.load_indexed_files()
            logger.info(
                "Knowledge manager initialized without full reindex",
                base_id=base_id,
                path=str(manager.knowledge_path),
                indexed_count=indexed_count,
            )

        if start_watchers:
            await manager.start_watcher()

        _knowledge_managers[base_id] = manager

    return dict(_knowledge_managers)


def get_knowledge_manager(base_id: str) -> KnowledgeManager | None:
    """Get one process-wide knowledge manager by base ID."""
    return _knowledge_managers.get(base_id)


async def shutdown_knowledge_managers() -> None:
    """Shutdown and clear all process-wide knowledge managers."""
    for manager in list(_knowledge_managers.values()):
        await manager.stop_watcher()

    _knowledge_managers.clear()

================================================================================
# FILE: src/mindroom/logging_config.py
================================================================================

"""Logging configuration for mindroom using structlog."""

from __future__ import annotations

import hashlib
import logging
import logging.config
from datetime import UTC, datetime
from pathlib import Path

import structlog

from mindroom.constants import STORAGE_PATH

__all__ = ["emoji", "get_logger", "setup_logging"]


class NioValidationFilter(logging.Filter):
    """Filter out harmless nio validation warnings that confuse AI agents."""

    def filter(self, record: logging.LogRecord) -> bool:
        """Filter out specific nio validation warnings.

        Returns:
            False to suppress the log record, True to keep it

        """
        # Filter out only the specific user_id and room_id validation warnings from nio
        if record.name == "nio.responses":
            msg = record.getMessage()
            if "Error validating response: 'user_id' is a required property" in msg:
                # This warning occurs when Matrix server responses don't include user_id
                # which happens during registration checks. It's harmless.
                return False
            if "Error validating response: 'room_id' is a required property" in msg:
                # Similar harmless warning for room_id
                return False
        return True


def emoji(agent_name: str) -> str:
    """Get an emoji-prefixed agent name string with consistent emoji based on the name.

    Args:
        agent_name: The agent name to add emoji to

    Returns:
        The agent name with a unique emoji prefix

    """
    # Emojis for different agents
    emojis = [
        "ðŸ¤–",  # robot
        "ðŸ§®",  # abacus
        "ðŸ’¡",  # light bulb
        "ðŸ”§",  # wrench
        "ðŸ“Š",  # chart
        "ðŸŽ¯",  # target
        "ðŸš€",  # rocket
        "âš¡",  # lightning
        "ðŸ”",  # magnifying glass
        "ðŸ“",  # memo
        "ðŸŽ¨",  # artist palette
        "ðŸ§ª",  # test tube
        "ðŸŽª",  # circus tent
        "ðŸŒŸ",  # star
        "ðŸ”®",  # crystal ball
        "ðŸ› ï¸",  # hammer and wrench
    ]

    # Use hash to get consistent emoji for each agent
    hash_value = int(hashlib.sha256(agent_name.encode()).hexdigest(), 16)
    emoji_index = hash_value % len(emojis)
    emoji = emojis[emoji_index]

    return f"{emoji} {agent_name}"


def setup_logging(level: str = "INFO") -> None:
    """Configure structlog for mindroom with file and console output.

    Args:
        level: Minimum logging level (e.g., "DEBUG", "INFO", "WARNING", "ERROR")

    """
    # Create logs directory if it doesn't exist
    logs_dir = Path(STORAGE_PATH) / "logs"
    logs_dir.mkdir(exist_ok=True, parents=True)

    # Create timestamped log file
    timestamp = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")
    log_file = logs_dir / f"mindroom_{timestamp}.log"

    # Shared processors that don't affect output format
    timestamper = structlog.processors.TimeStamper(fmt="iso")
    pre_chain = [
        structlog.stdlib.add_log_level,
        timestamper,
    ]

    # Configure logging with both console and file handlers
    logging.config.dictConfig(
        {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "plain": {
                    "()": structlog.stdlib.ProcessorFormatter,
                    "processors": [
                        structlog.stdlib.ProcessorFormatter.remove_processors_meta,
                        structlog.dev.ConsoleRenderer(colors=False),
                    ],
                    "foreign_pre_chain": pre_chain,
                },
                "colored": {
                    "()": structlog.stdlib.ProcessorFormatter,
                    "processors": [
                        structlog.stdlib.ProcessorFormatter.remove_processors_meta,
                        structlog.dev.ConsoleRenderer(
                            colors=True,
                            exception_formatter=structlog.dev.RichTracebackFormatter(
                                # The locals can be very large, so we hide them by default
                                show_locals=False,
                            ),
                        ),
                    ],
                    "foreign_pre_chain": pre_chain,
                },
            },
            "filters": {
                "nio_validation": {
                    "()": NioValidationFilter,
                },
            },
            "handlers": {
                "console": {
                    "level": level.upper(),
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stderr",
                    "formatter": "colored",
                    "filters": ["nio_validation"],
                },
                "file": {
                    "level": level.upper(),
                    "class": "logging.FileHandler",
                    "filename": str(log_file),
                    "mode": "a",
                    "encoding": "utf-8",
                    "formatter": "plain",
                    "filters": ["nio_validation"],
                },
            },
            "loggers": {
                "": {  # Root logger
                    "handlers": ["console", "file"],
                    "level": level.upper(),
                    "propagate": False,
                },
                # Reduce verbosity of nio (Matrix) library
                "nio": {
                    "level": "WARNING",
                },
                "nio.client": {
                    "level": "WARNING",
                },
                "nio.responses": {
                    "level": "WARNING",
                },
            },
        },
    )

    # Configure structlog to use stdlib logging
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            timestamper,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.UnicodeDecoder(),
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

    # Log startup message
    logger = get_logger(__name__)
    logger.info("Logging initialized", log_file=str(log_file), level=level)


def get_logger(name: str = __name__) -> structlog.stdlib.BoundLogger:
    """Get a structlog logger instance.

    Args:
        name: Logger name (typically __name__)

    Returns:
        Configured structlog logger

    """
    return structlog.get_logger(name)  # type: ignore[no-any-return]

================================================================================
# FILE: src/mindroom/matrix/__init__.py
================================================================================

"""Matrix operations module for mindroom."""

================================================================================
# FILE: src/mindroom/matrix/client.py
================================================================================

"""Matrix client operations and utilities."""

import io
import os
import re
import ssl as ssl_module
from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager
from html import escape
from pathlib import Path
from typing import Any

import markdown
import nio

from mindroom.constants import ENCRYPTION_KEYS_DIR
from mindroom.logging_config import get_logger

from .event_info import EventInfo
from .identity import MatrixID, extract_server_name_from_homeserver
from .large_messages import prepare_large_message
from .message_content import extract_and_resolve_message

logger = get_logger(__name__)


def _maybe_ssl_context(homeserver: str) -> ssl_module.SSLContext | None:
    if homeserver.startswith("https://"):
        if os.getenv("MATRIX_SSL_VERIFY", "true").lower() == "false":
            # Create context that disables verification for dev/self-signed certs
            ssl_context = ssl_module.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl_module.CERT_NONE
        else:
            # Use default context with proper verification
            ssl_context = ssl_module.create_default_context()
        return ssl_context
    return None


def create_matrix_client(
    homeserver: str,
    user_id: str | None = None,
    access_token: str | None = None,
    store_path: str | None = None,
) -> nio.AsyncClient:
    """Create a Matrix client with consistent configuration.

    Args:
        homeserver: The Matrix homeserver URL
        user_id: Optional user ID for authenticated client
        access_token: Optional access token for authenticated client
        store_path: Optional path for encryption key storage (defaults to .nio_store/<user_id>)

    Returns:
        nio.AsyncClient: Configured Matrix client instance

    """
    ssl_context = _maybe_ssl_context(homeserver)

    # Default store path for encryption support
    if store_path is None and user_id:
        safe_user_id = user_id.replace(":", "_").replace("@", "")
        store_path = str(ENCRYPTION_KEYS_DIR / safe_user_id)
        # Ensure the directory exists
        Path(store_path).mkdir(parents=True, exist_ok=True)

    client = nio.AsyncClient(homeserver, user_id, store_path=store_path, ssl=ssl_context)

    # Manually set user_id due to matrix-nio bug where constructor parameter doesn't work
    # See: https://github.com/matrix-nio/matrix-nio/issues/492
    if user_id:
        client.user_id = user_id

    if access_token:
        client.access_token = access_token

    return client


@asynccontextmanager
async def matrix_client(
    homeserver: str,
    user_id: str | None = None,
    access_token: str | None = None,
) -> AsyncGenerator[nio.AsyncClient, None]:
    """Context manager for Matrix client that ensures proper cleanup.

    Args:
        homeserver: The Matrix homeserver URL
        user_id: Optional user ID for authenticated client
        access_token: Optional access token for authenticated client

    Yields:
        nio.AsyncClient: The Matrix client instance

    Example:
        async with matrix_client("http://localhost:8008") as client:
            response = await client.login(password="secret")

    """
    client = create_matrix_client(homeserver, user_id, access_token)

    try:
        yield client
    finally:
        await client.close()


async def login(homeserver: str, user_id: str, password: str) -> nio.AsyncClient:
    """Login to Matrix and return authenticated client.

    Args:
        homeserver: The Matrix homeserver URL
        user_id: The full Matrix user ID (e.g., @user:localhost)
        password: The user's password

    Returns:
        Authenticated AsyncClient instance

    Raises:
        ValueError: If login fails

    """
    client = create_matrix_client(homeserver, user_id)

    response = await client.login(password)
    if isinstance(response, nio.LoginResponse):
        logger.info(f"Successfully logged in: {user_id}")
        return client
    await client.close()
    msg = f"Failed to login {user_id}: {response}"
    raise ValueError(msg)


async def register_user(
    homeserver: str,
    username: str,
    password: str,
    display_name: str,
) -> str:
    """Register a new Matrix user account.

    Args:
        homeserver: The Matrix homeserver URL
        username: The username for the Matrix account (without domain)
        password: The password for the account
        display_name: The display name for the user

    Returns:
        The full Matrix user ID (e.g., @user:localhost)

    Raises:
        ValueError: If registration fails

    """
    # Extract server name from homeserver URL
    server_name = extract_server_name_from_homeserver(homeserver)
    user_id = MatrixID.from_username(username, server_name).full_id

    async with matrix_client(homeserver) as client:
        # Try to register the user
        response = await client.register(
            username=username,
            password=password,
            device_name="mindroom_agent",
        )

        if isinstance(response, nio.RegisterResponse):
            logger.info(f"âœ… Successfully registered user: {user_id}")
            # After registration, we already have an access token
            client.user_id = response.user_id
            client.access_token = response.access_token
            client.device_id = response.device_id

            # Set display name using the existing session
            display_response = await client.set_displayname(display_name)
            if isinstance(display_response, nio.ErrorResponse):
                logger.warning(f"Failed to set display name: {display_response}")

            return user_id
        if isinstance(response, nio.ErrorResponse) and response.status_code == "M_USER_IN_USE":
            logger.info(f"User {user_id} already exists")
            return user_id
        msg = f"Failed to register user {username}: {response}"
        raise ValueError(msg)


async def invite_to_room(
    client: nio.AsyncClient,
    room_id: str,
    user_id: str,
) -> bool:
    """Invite a user to a room.

    Args:
        client: Authenticated Matrix client
        room_id: The room to invite to
        user_id: The user to invite

    Returns:
        True if successful, False otherwise

    """
    response = await client.room_invite(room_id, user_id)
    if isinstance(response, nio.RoomInviteResponse):
        logger.info(f"Invited {user_id} to room {room_id}")
        return True
    logger.error(f"Failed to invite {user_id} to room {room_id}: {response}")
    return False


async def create_room(
    client: nio.AsyncClient,
    name: str,
    alias: str | None = None,
    topic: str | None = None,
    power_users: list[str] | None = None,
) -> str | None:
    """Create a new Matrix room.

    Args:
        client: Authenticated Matrix client
        name: Room name
        alias: Optional room alias (without # and domain)
        topic: Optional room topic
        power_users: Optional list of user IDs to grant power level 50

    Returns:
        Room ID if successful, None otherwise

    """
    room_config: dict[str, Any] = {"name": name}
    if alias:
        room_config["alias"] = alias
    if topic:
        room_config["topic"] = topic

    if power_users:
        power_level_content: dict[str, Any] = {
            "users": dict.fromkeys(power_users, 50),
            "state_default": 50,  # Set default required power for state events
        }
        # Ensure the creator is an admin
        if client.user_id:
            power_level_content["users"][client.user_id] = 100
        room_config["initial_state"] = [{"type": "m.room.power_levels", "content": power_level_content}]

    response = await client.room_create(**room_config)
    if isinstance(response, nio.RoomCreateResponse):
        logger.info(f"Created room: {name} ({response.room_id})")
        room_id = str(response.room_id)

        # Invite power users to the room
        if power_users:
            for user_id in power_users:
                # Skip inviting ourselves
                if user_id != client.user_id:
                    await invite_to_room(client, room_id, user_id)

        return room_id
    logger.error(f"Failed to create room {name}: {response}")
    return None


async def create_dm_room(
    client: nio.AsyncClient,
    invite_user_ids: list[str],
    name: str | None = None,
) -> str | None:
    """Create a Direct Message room with specific users.

    Args:
        client: Authenticated Matrix client
        invite_user_ids: List of user IDs to invite to the DM
        name: Optional room name (defaults to "Direct Message")

    Returns:
        Room ID if successful, None otherwise

    """
    room_config: dict[str, Any] = {
        "preset": "trusted_private_chat",  # DM preset - no need to invite, they can join
        "is_direct": True,  # Mark as DM
        "invite": invite_user_ids,
    }

    if name:
        room_config["name"] = name

    response = await client.room_create(**room_config)
    if isinstance(response, nio.RoomCreateResponse):
        logger.info(f"Created DM room: {response.room_id}")
        return str(response.room_id)

    logger.error(f"Failed to create DM room: {response}")
    return None


async def join_room(client: nio.AsyncClient, room_id: str) -> bool:
    """Join a Matrix room.

    Args:
        client: Authenticated Matrix client
        room_id: Room ID or alias to join

    Returns:
        True if successful, False otherwise

    """
    response = await client.join(room_id)
    if isinstance(response, nio.JoinResponse):
        logger.info(f"Joined room: {room_id}")
        return True
    logger.warning(f"Could not join room {room_id}: {response}")
    return False


async def get_room_members(client: nio.AsyncClient, room_id: str) -> set[str]:
    """Get the current members of a room.

    Args:
        client: Authenticated Matrix client
        room_id: The room ID

    Returns:
        Set of user IDs in the room

    """
    response = await client.joined_members(room_id)
    if isinstance(response, nio.JoinedMembersResponse):
        return {member.user_id for member in response.members}
    logger.warning(f"âš ï¸ Could not check members for room {room_id}")
    return set()


async def get_joined_rooms(client: nio.AsyncClient) -> list[str] | None:
    """Get all rooms the client has joined.

    Args:
        client: Authenticated Matrix client

    Returns:
        List of room IDs the client has joined, or None if the request failed

    """
    response = await client.joined_rooms()
    if isinstance(response, nio.JoinedRoomsResponse):
        return list(response.rooms)
    logger.error(f"Failed to get joined rooms: {response}")
    return None


async def get_room_name(client: nio.AsyncClient, room_id: str) -> str:
    """Get the display name of a Matrix room.

    Args:
        client: Authenticated Matrix client
        room_id: The room ID to get the name for

    Returns:
        Room name if found, fallback name for DM/unnamed rooms

    """
    # Try to get the room name directly
    response = await client.room_get_state_event(room_id, "m.room.name")
    if isinstance(response, nio.RoomGetStateEventResponse) and response.content.get("name"):
        return str(response.content["name"])

    # Get room state for fallback naming
    response = await client.room_get_state(room_id)
    if not isinstance(response, nio.RoomGetStateResponse):
        return "Unnamed Room"

    # Check for room name in state events
    for event in response.events:
        if event.get("type") == "m.room.name" and event.get("content", {}).get("name"):
            return str(event["content"]["name"])

    # Build member list for DM/group room names
    members = [
        event.get("content", {}).get("displayname", event.get("state_key", ""))
        for event in response.events
        if event.get("type") == "m.room.member"
        and event.get("content", {}).get("membership") == "join"
        and event.get("state_key") != client.user_id
    ]

    if len(members) == 1:
        return f"DM with {members[0]}"
    if members:
        return f"Room with {', '.join(members[:3])}" + (" and others" if len(members) > 3 else "")

    return "Unnamed Room"


async def leave_room(client: nio.AsyncClient, room_id: str) -> bool:
    """Leave a Matrix room.

    Args:
        client: Authenticated Matrix client
        room_id: The room ID to leave

    Returns:
        True if successfully left the room, False otherwise

    """
    response = await client.room_leave(room_id)
    if isinstance(response, nio.RoomLeaveResponse):
        logger.info(f"Left room {room_id}")
        return True
    logger.error(f"Failed to leave room {room_id}: {response}")
    return False


async def send_message(client: nio.AsyncClient, room_id: str, content: dict[str, Any]) -> str | None:
    """Send a message to a Matrix room.

    Automatically handles large messages that exceed the Matrix event size limit
    by uploading the full content as MXC and sending a maximum-size preview.

    Args:
        client: Authenticated Matrix client
        room_id: The room ID to send the message to
        content: The message content dictionary

    Returns:
        The event ID of the sent message, or None if sending failed

    """
    # Handle large messages if needed
    content = await prepare_large_message(client, room_id, content)

    response = await client.room_send(
        room_id=room_id,
        message_type="m.room.message",
        content=content,
    )
    if isinstance(response, nio.RoomSendResponse):
        logger.debug(f"Sent message to {room_id}: {response.event_id}")
        return str(response.event_id)
    logger.error(f"Failed to send message to {room_id}: {response}")
    return None


async def fetch_thread_history(
    client: nio.AsyncClient,
    room_id: str,
    thread_id: str,
) -> list[dict[str, Any]]:
    """Fetch all messages in a thread.

    Args:
        client: The Matrix client instance
        room_id: The room ID to fetch messages from
        thread_id: The thread root event ID

    Returns:
        List of messages in chronological order, each containing sender, body, timestamp, and event_id

    """
    messages = []
    from_token = None
    root_message_found = False

    while True:
        response = await client.room_messages(
            room_id,
            start=from_token,
            limit=100,
            message_filter={"types": ["m.room.message"]},
            direction=nio.MessageDirection.back,
        )

        if not isinstance(response, nio.RoomMessagesResponse):
            logger.error("Failed to fetch thread history", room_id=room_id, error=str(response))
            break

        # Break if no new messages found
        if not response.chunk:
            break

        thread_messages_found = 0
        for event in response.chunk:
            if isinstance(event, nio.RoomMessageText):
                if event.event_id == thread_id and not root_message_found:
                    message_data = await extract_and_resolve_message(event, client)
                    messages.append(message_data)
                    root_message_found = True
                    thread_messages_found += 1
                else:
                    event_info = EventInfo.from_event(event.source)
                    if event_info.is_thread and event_info.thread_id == thread_id:
                        message_data = await extract_and_resolve_message(event, client)
                        messages.append(message_data)
                        thread_messages_found += 1

        if not response.end or thread_messages_found == 0:
            break
        from_token = response.end

    return list(reversed(messages))  # Return in chronological order


async def _latest_thread_event_id(
    client: nio.AsyncClient,
    room_id: str,
    thread_id: str,
) -> str:
    """Get the latest event ID in a thread for MSC3440 fallback compliance.

    This function fetches the thread history and returns the latest event ID.
    If the thread has no messages yet, returns the thread_id itself as fallback.

    Args:
        client: Matrix client
        room_id: Room ID
        thread_id: Thread root event ID

    Returns:
        The latest event ID in the thread, or thread_id if thread is empty

    """
    thread_msgs = await fetch_thread_history(client, room_id, thread_id)
    if thread_msgs:
        last_event_id = thread_msgs[-1].get("event_id")
        return str(last_event_id) if last_event_id else thread_id
    return thread_id


async def get_latest_thread_event_id_if_needed(
    client: nio.AsyncClient | None,
    room_id: str,
    thread_id: str | None,
    reply_to_event_id: str | None = None,
    existing_event_id: str | None = None,
) -> str | None:
    """Get the latest thread event ID only when needed for MSC3440 compliance.

    This helper encapsulates the common pattern of conditionally fetching
    the latest thread event ID based on various conditions.

    Args:
        client: Matrix client (can be None)
        room_id: Room ID
        thread_id: Thread root event ID (can be None)
        reply_to_event_id: Event ID being replied to (if any)
        existing_event_id: Existing event ID being edited (if any)

    Returns:
        The latest event ID in the thread if needed, None otherwise

    """
    # Only fetch latest thread event when:
    # 1. We have a thread_id
    # 2. We have a client
    # 3. We're not editing an existing message
    # 4. We're not making a genuine reply
    if thread_id and client and not existing_event_id and not reply_to_event_id:
        return await _latest_thread_event_id(client, room_id, thread_id)
    return None


_CONSECUTIVE_TOOL_BLOCKS = re.compile(
    r"(<tool>[\s\S]*?</tool>)(\s*<tool>[\s\S]*?</tool>)+",
)

_HTML_TAG_PATTERN = re.compile(r"</?([A-Za-z][A-Za-z0-9-]*)(?:\s+[^<>]*)?\s*/?>")

# Standard Matrix-safe HTML tags.
_GENERAL_FORMATTED_BODY_TAGS = frozenset(
    {
        "a",
        "b",
        "blockquote",
        "br",
        "caption",
        "code",
        "del",
        "details",
        "div",
        "em",
        "font",
        "h1",
        "h2",
        "h3",
        "h4",
        "h5",
        "h6",
        "hr",
        "i",
        "img",
        "li",
        "ol",
        "p",
        "pre",
        "s",
        "span",
        "strike",
        "strong",
        "sub",
        "summary",
        "sup",
        "table",
        "tbody",
        "td",
        "th",
        "thead",
        "tr",
        "u",
        "ul",
    },
)

# MindRoom custom tags intentionally rendered by custom Element forks.
_MINDROOM_CUSTOM_TAGS = (
    "tool",  # MindRoom: single tool call block (call + optional result), rendered as a collapsible entry.
    "tool-group",  # MindRoom: wrapper for consecutive tool blocks so the UI renders one grouped collapsible.
)
_MINDROOM_FORMATTED_BODY_TAGS = frozenset(_MINDROOM_CUSTOM_TAGS)

_ALLOWED_FORMATTED_BODY_TAGS = _GENERAL_FORMATTED_BODY_TAGS | _MINDROOM_FORMATTED_BODY_TAGS


def _escape_unsupported_html_tags(html_text: str) -> str:
    """Escape raw tags that Matrix clients commonly strip entirely.

    Unknown tags from model output (e.g. ``<search>``) can disappear in some
    clients. Escaping only the unsupported tags keeps them visible while still
    preserving intentional tags like ``<tool>`` for custom renderers.
    """

    def _replace_tag(match: re.Match[str]) -> str:
        tag_name = match.group(1).lower()
        if tag_name in _ALLOWED_FORMATTED_BODY_TAGS:
            return match.group(0)
        return escape(match.group(0))

    return _HTML_TAG_PATTERN.sub(_replace_tag, html_text)


def markdown_to_html(text: str) -> str:
    """Convert markdown text to HTML for Matrix formatted messages.

    Args:
        text: The markdown text to convert

    Returns:
        HTML formatted text

    """
    # Group consecutive <tool> blocks into <tool-group> so the frontend
    # can render them as a single merged collapsible.
    text = _CONSECUTIVE_TOOL_BLOCKS.sub(
        lambda m: f"<tool-group>{m.group(0)}</tool-group>",
        text,
    )

    # Configure markdown with common extensions
    md = markdown.Markdown(
        extensions=[
            "markdown.extensions.fenced_code",
            "markdown.extensions.codehilite",
            "markdown.extensions.tables",
            "markdown.extensions.nl2br",
        ],
        extension_configs={
            "markdown.extensions.codehilite": {
                "use_pygments": True,  # Use Pygments for syntax highlighting.
                "noclasses": True,  # Use inline styles instead of CSS classes
            },
        },
    )
    # Register custom elements as block-level so markdown doesn't wrap them
    # in <p> tags or convert \n to <br /> inside them.
    md.block_level_elements.extend(_MINDROOM_CUSTOM_TAGS)
    html_text: str = md.convert(text)
    return _escape_unsupported_html_tags(html_text)


async def edit_message(
    client: nio.AsyncClient,
    room_id: str,
    event_id: str,
    new_content: dict[str, Any],
    new_text: str,
) -> str | None:
    """Edit an existing Matrix message.

    Automatically handles large messages that exceed the Matrix event size limit
    by uploading the full content as MXC and sending a maximum-size preview.

    Args:
        client: The Matrix client
        room_id: The room ID where the message is
        event_id: The event ID of the message to edit
        new_content: The new content dictionary (from format_message_with_mentions)
        new_text: The new text (plain text version)

    Returns:
        The event ID of the edit message, or None if editing failed

    """
    edit_content = {
        "msgtype": "m.text",
        "body": f"* {new_text}",
        "format": "org.matrix.custom.html",
        "formatted_body": new_content.get("formatted_body", new_text),
        "m.new_content": new_content,
        "m.relates_to": {"rel_type": "m.replace", "event_id": event_id},
    }

    # send_message will handle large messages, including the lower threshold for edits
    return await send_message(client, room_id, edit_content)


async def _upload_avatar_file(
    client: nio.AsyncClient,
    avatar_path: Path,
) -> str | None:
    """Upload an avatar file to the Matrix server.

    Args:
        client: Authenticated Matrix client
        avatar_path: Path to the avatar image file

    Returns:
        The content URI if successful, None otherwise

    """
    if not avatar_path.exists():
        logger.warning(f"Avatar file not found: {avatar_path}")
        return None

    extension = avatar_path.suffix.lower()
    content_type = {
        ".png": "image/png",
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".webp": "image/webp",
        ".gif": "image/gif",
    }.get(extension, "image/png")

    with avatar_path.open("rb") as f:
        avatar_data = f.read()

    file_size = len(avatar_data)

    def data_provider(_upload_monitor: object, _unused_data: object) -> io.BytesIO:
        return io.BytesIO(avatar_data)

    upload_result = await client.upload(
        data_provider=data_provider,
        content_type=content_type,
        filename=avatar_path.name,
        filesize=file_size,
    )

    # nio returns tuple (response, error)
    if isinstance(upload_result, tuple):
        upload_response, error = upload_result
        if error:
            logger.error(f"Upload error: {error}")
            return None
    else:
        upload_response = upload_result

    if not isinstance(upload_response, nio.UploadResponse):
        logger.error(f"Failed to upload avatar: {upload_response}")
        return None

    if not upload_response.content_uri:
        logger.error("Upload response missing content_uri")
        return None

    return str(upload_response.content_uri)


async def set_avatar_from_file(
    client: nio.AsyncClient,
    avatar_path: Path,
) -> bool:
    """Set a user's avatar from a local file.

    Args:
        client: Authenticated Matrix client
        avatar_path: Path to the avatar image file

    Returns:
        True if successful, False otherwise

    """
    avatar_url = await _upload_avatar_file(client, avatar_path)
    if not avatar_url:
        return False

    response = await client.set_avatar(avatar_url)

    if isinstance(response, nio.ProfileSetAvatarResponse):
        logger.info(f"âœ… Successfully set avatar for {client.user_id}")
        return True

    logger.error(f"Failed to set avatar for {client.user_id}: {response}")
    return False


async def check_and_set_avatar(
    client: nio.AsyncClient,
    avatar_path: Path,
    room_id: str | None = None,
) -> bool:
    """Check if user or room has an avatar and set it if they don't.

    Args:
        client: Authenticated Matrix client
        avatar_path: Path to the avatar image file
        room_id: Optional room ID for setting room avatar (if None, sets user avatar)

    Returns:
        True if avatar was already set or successfully set, False otherwise

    """
    if room_id:
        # Check room avatar
        response = await client.room_get_state_event(room_id, "m.room.avatar")
        if isinstance(response, nio.RoomGetStateEventResponse) and response.content and response.content.get("url"):
            logger.debug(f"Avatar already set for room {room_id}")
            return True
        # Set room avatar
        return await set_room_avatar_from_file(client, room_id, avatar_path)
    # Check user avatar
    response = await client.get_profile(client.user_id)
    if isinstance(response, nio.ProfileGetResponse) and response.avatar_url:
        logger.debug(f"Avatar already set for {client.user_id}")
        return True
    # Set user avatar
    return await set_avatar_from_file(client, avatar_path)


async def set_room_avatar_from_file(
    client: nio.AsyncClient,
    room_id: str,
    avatar_path: Path,
) -> bool:
    """Set the avatar for a Matrix room from a file.

    Args:
        client: Authenticated Matrix client
        room_id: The room ID to set the avatar for
        avatar_path: Path to the avatar image file

    Returns:
        True if avatar was successfully set, False otherwise

    """
    avatar_url = await _upload_avatar_file(client, avatar_path)
    if not avatar_url:
        return False

    # Set room avatar using room state
    response = await client.room_put_state(
        room_id=room_id,
        event_type="m.room.avatar",
        content={"url": avatar_url},
    )

    if isinstance(response, nio.RoomPutStateResponse):
        logger.info(f"âœ… Successfully set avatar for room {room_id}")
        return True

    logger.error(f"Failed to set avatar for room {room_id}: {response}")
    return False

================================================================================
# FILE: src/mindroom/matrix/event_info.py
================================================================================

"""Comprehensive event relation analysis for Matrix events.

This module provides a unified API for analyzing all Matrix event relations
including threads (MSC3440), edits, replies, reactions, and more.
"""

from __future__ import annotations

from dataclasses import dataclass


@dataclass
class EventInfo:
    """Comprehensive analysis of Matrix event relations."""

    # Thread information (MSC3440)
    is_thread: bool
    """Whether this event is part of a thread."""

    thread_id: str | None
    """The thread root event ID if this is a thread message."""

    can_be_thread_root: bool
    """Whether this event can be used as a thread root per MSC3440."""

    safe_thread_root: str | None
    """Safe event ID to use as thread root (None means use this event)."""

    # Edit information
    is_edit: bool
    """Whether this event is an edit (m.replace)."""

    original_event_id: str | None
    """The event ID being edited if this is an edit."""

    # Reply information
    is_reply: bool
    """Whether this event is a reply to another event."""

    reply_to_event_id: str | None
    """The event ID being replied to if this is a reply."""

    # Reaction information
    is_reaction: bool
    """Whether this event is a reaction (m.annotation)."""

    reaction_key: str | None
    """The reaction key/emoji if this is a reaction."""

    reaction_target_event_id: str | None
    """The event ID being reacted to if this is a reaction."""

    # General relation information
    has_relations: bool
    """Whether this event has any relations."""

    relation_type: str | None
    """The relation type if any (m.replace, m.annotation, m.thread, etc)."""

    relates_to_event_id: str | None
    """The primary event ID this event relates to (if any)."""

    @staticmethod
    def from_event(event_source: dict | None) -> EventInfo:
        """Create EventInfo from a raw event source dictionary."""
        return _analyze_event_relations(event_source)


def _analyze_event_relations(event_source: dict | None) -> EventInfo:
    """Analyze complete relation information for a Matrix event.

    This unified function provides all relation-related information in one place,
    replacing manual extraction of m.relates_to throughout the codebase.

    Per MSC3440:
    - A thread can only be created from events that don't have any rel_type
    - Thread messages use rel_type: m.thread
    - Edits use rel_type: m.replace
    - Reactions use rel_type: m.annotation
    - Replies can be within threads or standalone

    Args:
        event_source: The event source dictionary (e.g., event.source for nio events)

    Returns:
        EventInfo object with complete relation analysis

    """
    if not event_source:
        return EventInfo(
            is_thread=False,
            thread_id=None,
            can_be_thread_root=True,
            safe_thread_root=None,
            is_edit=False,
            original_event_id=None,
            is_reply=False,
            reply_to_event_id=None,
            is_reaction=False,
            reaction_key=None,
            reaction_target_event_id=None,
            has_relations=False,
            relation_type=None,
            relates_to_event_id=None,
        )

    content = event_source.get("content", {})
    relates_to = content.get("m.relates_to", {})

    # Extract basic relation information
    relation_type = relates_to.get("rel_type")
    has_relations = bool(relates_to)
    relates_to_event_id = relates_to.get("event_id")

    # Thread analysis
    is_thread = relation_type == "m.thread"
    thread_id = relates_to_event_id if is_thread else None

    # Edit analysis
    is_edit = relation_type == "m.replace"
    original_event_id = relates_to_event_id if is_edit else None

    # Reaction analysis
    is_reaction = relation_type == "m.annotation"
    reaction_key = relates_to.get("key") if is_reaction else None
    reaction_target_event_id = relates_to_event_id if is_reaction else None

    # Reply analysis
    # Replies can exist within threads or as standalone
    # They have m.in_reply_to field
    in_reply_to = relates_to.get("m.in_reply_to", {})
    is_reply = bool(in_reply_to and in_reply_to.get("event_id"))
    reply_to_event_id = in_reply_to.get("event_id") if is_reply else None

    # Determine if this event can be a thread root (per MSC3440)
    # An event can only be a thread root if it has NO relations
    can_be_thread_root = not has_relations

    # Determine safe thread root for creating new threads
    safe_thread_root = None
    if not can_be_thread_root:
        # This event has relations, so it cannot be a thread root
        # Try to use the target of the relation as the thread root

        if relation_type in ("m.replace", "m.annotation", "m.reference"):
            # For edits, reactions, and references, use the target event
            if relates_to_event_id:
                safe_thread_root = str(relates_to_event_id)
        elif is_reply and reply_to_event_id:
            # For rich replies, use the event being replied to
            safe_thread_root = str(reply_to_event_id)

    return EventInfo(
        # Thread info
        is_thread=is_thread,
        thread_id=thread_id,
        can_be_thread_root=can_be_thread_root,
        safe_thread_root=safe_thread_root,
        # Edit info
        is_edit=is_edit,
        original_event_id=original_event_id,
        # Reply info
        is_reply=is_reply,
        reply_to_event_id=reply_to_event_id,
        # Reaction info
        is_reaction=is_reaction,
        reaction_key=reaction_key,
        reaction_target_event_id=reaction_target_event_id,
        # General info
        has_relations=has_relations,
        relation_type=relation_type,
        relates_to_event_id=relates_to_event_id,
    )

================================================================================
# FILE: src/mindroom/matrix/identity.py
================================================================================

"""Unified Matrix ID handling system."""

from __future__ import annotations

from dataclasses import dataclass
from functools import lru_cache
from typing import TYPE_CHECKING, ClassVar

from mindroom.constants import MATRIX_SERVER_NAME, ROUTER_AGENT_NAME

if TYPE_CHECKING:
    from mindroom.config import Config


@dataclass(frozen=True)
class MatrixID:
    """Immutable Matrix ID representation with parsing and validation."""

    username: str
    domain: str

    AGENT_PREFIX: ClassVar[str] = "mindroom_"

    @classmethod
    def parse(cls, matrix_id: str) -> MatrixID:
        """Parse a Matrix ID like @mindroom_calculator:localhost."""
        return _parse_matrix_id(matrix_id)

    @classmethod
    def from_agent(cls, agent_name: str, domain: str) -> MatrixID:
        """Create a MatrixID for an agent."""
        return cls(username=f"{cls.AGENT_PREFIX}{agent_name}", domain=domain)

    @classmethod
    def from_username(cls, username: str, domain: str) -> MatrixID:
        """Create a MatrixID from a username (without @ prefix)."""
        return cls(username=username, domain=domain)

    @property
    def full_id(self) -> str:
        """Get the full Matrix ID like @mindroom_calculator:localhost."""
        return f"@{self.username}:{self.domain}"

    def agent_name(self, config: Config) -> str | None:
        """Extract agent name if this is a configured agent ID."""
        if not self.username.startswith(self.AGENT_PREFIX):
            return None

        # Remove prefix
        name = self.username[len(self.AGENT_PREFIX) :]

        # Special check for the router agent:
        # The router is a built-in agent that handles command routing and doesn't
        # appear in config.agents. Without this check, extract_agent_name() would
        # return None for router messages, causing other agents to incorrectly
        # respond to router's error messages (e.g., when schedule parsing fails).
        if name == ROUTER_AGENT_NAME:
            return name

        # Validate against both regular agents and teams
        if name in config.agents:
            return name
        if name in config.teams:
            return name
        return None

    def __str__(self) -> str:
        """Return the full Matrix ID string representation."""
        return self.full_id


@dataclass(frozen=True)
class ThreadStateKey:
    """Represents a thread state key like 'thread_id:agent_name'."""

    thread_id: str
    agent_name: str

    @classmethod
    def parse(cls, state_key: str) -> ThreadStateKey:
        """Parse a state key."""
        parts = state_key.split(":", 1)
        if len(parts) != 2:
            msg = f"Invalid state key: {state_key}"
            raise ValueError(msg)
        return cls(thread_id=parts[0], agent_name=parts[1])

    @property
    def key(self) -> str:
        """Get the full state key."""
        return f"{self.thread_id}:{self.agent_name}"

    def __str__(self) -> str:
        """Return the state key string representation."""
        return self.key


@lru_cache(maxsize=512)
def _parse_matrix_id(matrix_id: str) -> MatrixID:
    """Cached wrapper around MatrixID.parse for performance."""
    if not matrix_id.startswith("@"):
        msg = f"Invalid Matrix ID: {matrix_id}"
        raise ValueError(msg)
    if ":" not in matrix_id:
        msg = f"Invalid Matrix ID, missing domain: {matrix_id}"
        raise ValueError(msg)
    parts = matrix_id[1:].split(":", 1)
    if len(parts) != 2:
        msg = f"Invalid Matrix ID format: {matrix_id}"
        raise ValueError(msg)

    return MatrixID(username=parts[0], domain=parts[1])


def is_agent_id(matrix_id: str, config: Config) -> bool:
    """Quick check if a Matrix ID is an agent."""
    return extract_agent_name(matrix_id, config) is not None


def extract_agent_name(sender_id: str, config: Config) -> str | None:
    """Extract agent name from Matrix user ID like @mindroom_calculator:localhost.

    Returns agent name (e.g., 'calculator') or None if not an agent.
    """
    if not sender_id.startswith("@") or ":" not in sender_id:
        return None
    mid = MatrixID.parse(sender_id)
    return mid.agent_name(config)


def extract_server_name_from_homeserver(homeserver: str) -> str:
    """Extract server name from a homeserver URL like "http://localhost:8008".

    If MATRIX_SERVER_NAME environment variable is set, use that instead.
    This is needed for federation setups where the internal hostname differs
    from the actual Matrix server name.
    """
    # Check for explicit server name override (for federation/docker setups)
    if MATRIX_SERVER_NAME:
        return MATRIX_SERVER_NAME

    # Otherwise extract from homeserver URL
    # Remove protocol
    server_part = homeserver.split("://", 1)[1] if "://" in homeserver else homeserver

    # Remove port if present
    if ":" in server_part:
        return server_part.split(":", 1)[0]
    return server_part

================================================================================
# FILE: src/mindroom/matrix/large_messages.py
================================================================================

"""Handle large Matrix messages that exceed the 64KB event limit.

This module provides minimal intervention for messages that are too large,
uploading the full text as an MXC attachment while maximizing the preview size.
"""

from __future__ import annotations

import io
import json
from typing import Any

import nio
from nio import crypto

from mindroom.logging_config import get_logger

logger = get_logger(__name__)

# Conservative limits accounting for Matrix overhead
NORMAL_MESSAGE_LIMIT = 55000  # ~55KB for regular messages
EDIT_MESSAGE_LIMIT = 27000  # ~27KB for edits (they roughly double in size)
PASSTHROUGH_CONTENT_KEYS = ("m.mentions", "com.mindroom.skip_mentions")


def _calculate_event_size(content: dict[str, Any]) -> int:
    """Calculate the approximate size of a Matrix event.

    Args:
        content: The message content dictionary

    Returns:
        Approximate size in bytes including JSON overhead

    """
    # Convert to canonical JSON (sorted keys, no spaces)
    canonical = json.dumps(content, sort_keys=True, separators=(",", ":"))
    # Add ~2KB overhead for event metadata, signatures, etc.
    return len(canonical.encode("utf-8")) + 2000


def _is_edit_message(content: dict[str, Any]) -> bool:
    """Check if this is an edit message."""
    return "m.new_content" in content or (
        "m.relates_to" in content and content.get("m.relates_to", {}).get("rel_type") == "m.replace"
    )


def _create_preview(text: str, max_bytes: int) -> str:
    """Create a preview that fits within byte limit.

    Args:
        text: The full text to preview
        max_bytes: Maximum size in bytes for the preview

    Returns:
        Preview text that fits within the byte limit

    """
    # Reserve space for continuation indicator
    indicator = "\n\n[Message continues in attached file]"
    indicator_bytes = len(indicator.encode("utf-8"))

    # If text fits entirely, return as-is
    if len(text.encode("utf-8")) <= max_bytes:
        return text

    # If budget is too small for any preview + indicator, return indicator only
    target_bytes = max_bytes - indicator_bytes
    if target_bytes <= 0:
        return indicator.lstrip()

    # Start with a reasonable estimate
    left, right = 0, min(len(text), target_bytes)
    best_pos = 0

    while left <= right:
        mid = (left + right) // 2
        try:
            # Check if this position creates valid UTF-8
            test_bytes = text[:mid].encode("utf-8")
            if len(test_bytes) <= target_bytes:
                best_pos = mid
                left = mid + 1
            else:
                right = mid - 1
        except UnicodeEncodeError:
            # Shouldn't happen with valid input
            right = mid - 1

    return text[:best_pos] + indicator


async def _upload_text_as_mxc(
    client: nio.AsyncClient,
    text: str,
    room_id: str | None = None,
) -> tuple[str | None, dict[str, Any] | None]:
    """Upload text content as an MXC file.

    Args:
        client: The Matrix client
        text: The text content to upload
        room_id: Optional room ID to check for encryption

    Returns:
        Tuple of (mxc_uri, file_info_dict) or (None, None) on failure

    """
    text_bytes = text.encode("utf-8")
    file_info = {
        "size": len(text_bytes),
        "mimetype": "text/plain",
    }

    # Check if room is encrypted
    room_encrypted = False
    if room_id and room_id in client.rooms:
        room = client.rooms[room_id]
        room_encrypted = room.encrypted

    if room_encrypted:
        # Encrypt the content for E2EE room
        try:
            encrypted_data = crypto.attachments.encrypt_attachment(text_bytes)
            upload_data = encrypted_data["data"]

            # Store encryption info for the file
            file_info = {
                "url": "",  # Will be set after upload
                "key": encrypted_data["key"],
                "iv": encrypted_data["iv"],
                "hashes": encrypted_data["hashes"],
                "v": "v2",
                "mimetype": "text/plain",
                "size": len(text_bytes),
            }
        except Exception:
            logger.exception("Failed to encrypt attachment")
            return None, None
    else:
        upload_data = text_bytes

    # Upload the file
    def data_provider(_monitor: object, _data: object) -> io.BytesIO:
        return io.BytesIO(upload_data)

    try:
        # nio.upload returns Tuple[Union[UploadResponse, UploadError], Optional[Dict[str, Any]]]
        upload_result, encryption_dict = await client.upload(
            data_provider=data_provider,
            content_type="application/octet-stream" if room_encrypted else "text/plain",
            filename="message.txt.enc" if room_encrypted else "message.txt",
            filesize=len(upload_data),
        )

        # Check if upload was successful
        if not isinstance(upload_result, nio.UploadResponse):
            logger.error(f"Failed to upload text: {upload_result}")
            return None, None

        if not upload_result.content_uri:
            logger.error("Upload response missing content_uri")
            return None, None

        mxc_uri = str(upload_result.content_uri)
        file_info["url"] = mxc_uri

    except Exception:
        logger.exception("Failed to upload text")
        return None, None
    else:
        return mxc_uri, file_info


async def prepare_large_message(
    client: nio.AsyncClient,
    room_id: str,
    content: dict[str, Any],
) -> dict[str, Any]:
    """Check if message is too large and prepare it if needed.

    This function:
    1. Checks the message size
    2. If too large, uploads the full text as MXC
    3. Replaces body with maximum-size preview
    4. Adds metadata for reconstruction

    Args:
        client: The Matrix client
        room_id: The room to send to
        content: The message content dictionary

    Returns:
        Original content (if small) or modified content with preview and MXC reference

    """
    is_edit = _is_edit_message(content)
    size_limit = EDIT_MESSAGE_LIMIT if is_edit else NORMAL_MESSAGE_LIMIT

    current_size = _calculate_event_size(content)
    if current_size <= size_limit:
        return content

    source_content = content["m.new_content"] if is_edit and "m.new_content" in content else content
    full_text = source_content["body"]

    logger.info(f"Message too large ({current_size} bytes), uploading to MXC")

    mxc_uri, file_info = await _upload_text_as_mxc(client, full_text, room_id)

    # Calculate how much space we have for preview
    # The structure adds: filename, url, info object, custom metadata
    attachment_overhead = 5000  # Conservative estimate for attachment JSON structure
    available_for_preview = size_limit - attachment_overhead

    preview = _create_preview(full_text, available_for_preview)

    modified_content: dict[str, Any] = {
        "msgtype": "m.file",
        "body": preview,
        "filename": "message.txt",
        "info": file_info,
    }

    for key in PASSTHROUGH_CONTENT_KEYS:
        if key in source_content:
            modified_content[key] = source_content[key]

    if room_id and room_id in client.rooms and client.rooms[room_id].encrypted:
        modified_content["file"] = file_info
    else:
        modified_content["url"] = mxc_uri

    modified_content["io.mindroom.long_text"] = {
        "version": 1,
        "original_size": len(full_text),
        "preview_size": len(preview),
        "is_complete_text": True,
    }

    if "m.relates_to" in content:
        modified_content["m.relates_to"] = content["m.relates_to"]

    if is_edit and "m.new_content" in content:
        modified_content = {
            "msgtype": "m.text",
            "body": f"* {preview}",
            "m.new_content": modified_content,
            "m.relates_to": content.get("m.relates_to", {}),
        }

    final_size = _calculate_event_size(modified_content)
    if final_size > 64000:
        logger.warning(f"Large message still exceeds 64KB after preparation ({final_size} bytes)")

    logger.info(f"Large message prepared: {len(full_text)} bytes -> {len(preview)} preview + MXC attachment")

    return modified_content

================================================================================
# FILE: src/mindroom/matrix/mentions.py
================================================================================

"""Matrix mention utilities."""

import re
from typing import TYPE_CHECKING, Any

from mindroom.config import Config
from mindroom.tool_events import build_tool_trace_content

from .client import markdown_to_html
from .identity import MatrixID
from .message_builder import build_message_content

if TYPE_CHECKING:
    from mindroom.tool_events import ToolTraceEntry


def parse_mentions_in_text(text: str, sender_domain: str, config: Config) -> tuple[str, list[str], str]:
    """Parse text for agent mentions and return processed text with user IDs.

    Args:
        text: Text that may contain @agent_name mentions
        sender_domain: Domain part of the sender's user ID (e.g., "localhost" from "@user:localhost")
        config: Application configuration

    Returns:
        Tuple of (plain_text, list_of_mentioned_user_ids, markdown_text_with_links)

    """
    # Pattern to match @agent_name (with optional @mindroom_ prefix or domain)
    # Matches: @calculator, @mindroom_calculator, @mindroom_calculator:localhost
    pattern = r"@(mindroom_)?(\w+)(?::[^\s]+)?"

    # Find all mentions and process them
    mentions_data = []
    for match in re.finditer(pattern, text):
        mention_info = _process_mention(match, config, sender_domain)
        if mention_info:
            mentions_data.append(mention_info)

    # Build outputs from collected data
    plain_text = text
    markdown_text = text
    mentioned_user_ids: list[str] = []

    # Apply replacements (reverse order to preserve positions)
    for original, user_id, display_name in reversed(mentions_data):
        # Plain text: replace with full Matrix ID
        plain_text = plain_text.replace(original, user_id, 1)

        # Markdown: replace with clickable link
        link = f"[@{display_name}](https://matrix.to/#/{user_id})"
        markdown_text = markdown_text.replace(original, link, 1)

        # Collect unique user IDs
        if user_id not in mentioned_user_ids:
            mentioned_user_ids.insert(0, user_id)  # Insert at start to maintain order

    return plain_text, mentioned_user_ids, markdown_text


def _process_mention(match: re.Match, config: Config, sender_domain: str) -> tuple[str, str, str] | None:
    """Process a single mention match and return replacement data.

    Args:
        match: The regex match object
        config: The loaded config
        sender_domain: Domain for constructing Matrix IDs

    Returns:
        Tuple of (original_text, matrix_user_id, display_name) or None if not a valid agent

    """
    original = match.group(0)
    prefix = match.group(1) or ""  # "mindroom_" or empty
    name = match.group(2)

    # Skip user mentions (mindroom_user_*)
    if name.startswith("user_"):
        return None

    # Try to find the agent (case-insensitive)
    agent_name = None
    name_lower = name.lower()

    # Check for direct match (case-insensitive)
    for config_agent_name in config.agents:
        if config_agent_name.lower() == name_lower:
            agent_name = config_agent_name
            break

    # If not found, try with mindroom_ prefix removed
    if not agent_name and prefix:
        name_without_prefix = name.replace("mindroom_", "")
        name_without_prefix_lower = name_without_prefix.lower()
        for config_agent_name in config.agents:
            if config_agent_name.lower() == name_without_prefix_lower:
                agent_name = config_agent_name
                break

    if agent_name:
        agent_config = config.agents[agent_name]
        user_id = MatrixID.from_agent(agent_name, sender_domain).full_id
        return (original, user_id, agent_config.display_name)

    return None


def format_message_with_mentions(
    config: Config,
    text: str,
    sender_domain: str = "localhost",
    thread_event_id: str | None = None,
    reply_to_event_id: str | None = None,
    latest_thread_event_id: str | None = None,
    tool_trace: list["ToolTraceEntry"] | None = None,
) -> dict[str, Any]:
    """Parse text for mentions and create properly formatted Matrix message.

    This is the universal function that should be used everywhere.

    Args:
        config: Application configuration
        text: Message text that may contain @agent_name mentions
        sender_domain: Domain part of the sender's user ID
        thread_event_id: Optional thread root event ID
        reply_to_event_id: Optional event ID to reply to (for genuine replies)
        latest_thread_event_id: Optional latest event ID in thread (for fallback compatibility)
        tool_trace: Optional structured tool trace metadata

    Returns:
        Properly formatted content dict for room_send

    """
    plain_text, mentioned_user_ids, markdown_text = parse_mentions_in_text(text, sender_domain, config)

    # Convert markdown (with links) to HTML
    # The markdown converter will properly handle the [@DisplayName](url) format
    formatted_html = markdown_to_html(markdown_text)
    extra_content = build_tool_trace_content(tool_trace)

    return build_message_content(
        body=plain_text,
        formatted_body=formatted_html,
        mentioned_user_ids=mentioned_user_ids,
        thread_event_id=thread_event_id,
        reply_to_event_id=reply_to_event_id,
        latest_thread_event_id=latest_thread_event_id,
        extra_content=extra_content,
    )

================================================================================
# FILE: src/mindroom/matrix/message_builder.py
================================================================================

"""Matrix message content builder with proper threading support."""

from typing import Any

from .client import markdown_to_html


def build_thread_relation(
    thread_event_id: str,
    reply_to_event_id: str | None = None,
    latest_thread_event_id: str | None = None,
) -> dict[str, Any]:
    """Build the m.relates_to structure for thread messages per MSC3440.

    Args:
        thread_event_id: The thread root event ID
        reply_to_event_id: Optional event ID for genuine replies within thread
        latest_thread_event_id: Latest event in thread (required for fallback if no reply_to)

    Returns:
        The m.relates_to structure for the message content

    """
    if reply_to_event_id:
        # Genuine reply to a specific message in the thread
        return {
            "rel_type": "m.thread",
            "event_id": thread_event_id,
            "is_falling_back": False,
            "m.in_reply_to": {"event_id": reply_to_event_id},
        }
    # Fallback: continuing thread without specific reply
    # Per MSC3440, should point to latest message in thread for backwards compatibility
    assert latest_thread_event_id is not None, "latest_thread_event_id is required for thread fallback"
    return {
        "rel_type": "m.thread",
        "event_id": thread_event_id,
        "is_falling_back": True,
        "m.in_reply_to": {"event_id": latest_thread_event_id},
    }


def build_message_content(
    body: str,
    formatted_body: str | None = None,
    mentioned_user_ids: list[str] | None = None,
    thread_event_id: str | None = None,
    reply_to_event_id: str | None = None,
    latest_thread_event_id: str | None = None,
    extra_content: dict[str, Any] | None = None,
) -> dict[str, Any]:
    """Build a complete Matrix message content dictionary.

    This handles all the Matrix protocol requirements for messages including:
    - Basic message structure
    - HTML formatting
    - User mentions
    - Thread relations (MSC3440 compliant)
    - Reply relations

    Args:
        body: The plain text message body
        formatted_body: Optional HTML formatted body (if not provided, converts from markdown)
        mentioned_user_ids: Optional list of Matrix user IDs to mention
        thread_event_id: Optional thread root event ID
        reply_to_event_id: Optional event ID to reply to
        latest_thread_event_id: Optional latest event in thread (for MSC3440 fallback)
        extra_content: Optional extra content fields to merge into the message

    Returns:
        Complete content dictionary ready for room_send

    """
    content: dict[str, Any] = {
        "msgtype": "m.text",
        "body": body,
        "format": "org.matrix.custom.html",
        "formatted_body": formatted_body if formatted_body else markdown_to_html(body),
    }

    # Add mentions if any
    if mentioned_user_ids:
        content["m.mentions"] = {"user_ids": mentioned_user_ids}

    # Add thread/reply relationship if specified
    if thread_event_id:
        content["m.relates_to"] = build_thread_relation(
            thread_event_id=thread_event_id,
            reply_to_event_id=reply_to_event_id,
            latest_thread_event_id=latest_thread_event_id,
        )
    elif reply_to_event_id:
        # Plain reply without thread (shouldn't happen in this bot)
        content["m.relates_to"] = {"m.in_reply_to": {"event_id": reply_to_event_id}}

    if extra_content:
        content.update(extra_content)

    return content

================================================================================
# FILE: src/mindroom/matrix/message_content.py
================================================================================

"""Centralized message content extraction with large message support.

This module provides utilities to extract the full content from Matrix messages,
including handling large messages that are stored as MXC attachments.
"""

from __future__ import annotations

import time
from typing import Any

import nio
from nio import crypto

from mindroom.logging_config import get_logger

logger = get_logger(__name__)

# MXC download cache - stores (content, timestamp) tuples
# Key: mxc_url, Value: (content, timestamp)
_mxc_cache: dict[str, tuple[str, float]] = {}
_cache_ttl = 3600.0  # 1 hour TTL


async def _get_full_message_body(
    message_data: dict[str, Any],
    client: nio.AsyncClient | None = None,
) -> str:
    """Extract the full message body, handling large message attachments.

    For regular messages, returns the body directly.
    For large messages with attachments, downloads and returns the full content.

    Args:
        message_data: Dict with message data including 'body' and 'content' keys
        client: Optional Matrix client for downloading attachments

    Returns:
        The full message body text

    """
    content = message_data.get("content", {})
    body = str(message_data.get("body", ""))

    # Check if this is a large message with our custom metadata
    if "io.mindroom.long_text" in content:
        # This is a large message - need to fetch the attachment
        if not client:
            logger.warning("Cannot fetch large message attachment without client, returning preview")
            return body

        # Get the MXC URL from either 'url' (unencrypted) or 'file' (encrypted)
        mxc_url = None
        if "url" in content:
            mxc_url = content["url"]
        elif "file" in content:
            file_info = content["file"]
            mxc_url = file_info.get("url")

        if not mxc_url:
            logger.warning("Large message missing MXC URL, returning preview")
            return body

        # Download the full content
        full_text = await _download_mxc_text(client, mxc_url, content.get("file"))
        if full_text:
            return full_text
        logger.warning("Failed to download large message, returning preview")
        return body

    # Regular message or no custom metadata
    return body


async def _download_mxc_text(  # noqa: PLR0911, C901
    client: nio.AsyncClient,
    mxc_url: str,
    file_info: dict[str, Any] | None = None,
) -> str | None:
    """Download text content from an MXC URL with caching.

    Args:
        client: Matrix client
        mxc_url: The MXC URL to download from
        file_info: Optional encryption info for E2EE rooms

    Returns:
        The downloaded text content, or None if download failed

    """
    # Check cache first
    current_time = time.time()
    if mxc_url in _mxc_cache:
        content, timestamp = _mxc_cache[mxc_url]
        if current_time - timestamp < _cache_ttl:
            logger.debug(f"Cache hit for MXC URL: {mxc_url}")
            return content
        # Expired, remove from cache
        del _mxc_cache[mxc_url]

    try:
        # Parse MXC URL
        if not mxc_url.startswith("mxc://"):
            logger.error(f"Invalid MXC URL: {mxc_url}")
            return None

        # Extract server and media ID
        parts = mxc_url[6:].split("/", 1)
        if len(parts) != 2:
            logger.error(f"Invalid MXC URL format: {mxc_url}")
            return None

        server_name, media_id = parts

        # Download the content
        response = await client.download(server_name, media_id)

        if not isinstance(response, nio.DownloadResponse):
            logger.error(f"Failed to download MXC content: {response}")
            return None

        # Handle encryption if needed
        if file_info and "key" in file_info:
            # Decrypt the content
            try:
                decrypted = crypto.attachments.decrypt_attachment(
                    response.body,
                    file_info["key"],
                    file_info["hashes"]["sha256"],
                    file_info["iv"],
                )
                text_bytes = decrypted
            except Exception:
                logger.exception("Failed to decrypt attachment")
                return None
        else:
            text_bytes = response.body

        # Decode to text
        try:
            decoded_text: str = text_bytes.decode("utf-8")
        except UnicodeDecodeError:
            logger.exception("Downloaded content is not valid UTF-8 text")
            return None

        else:
            # Cache the result
            _mxc_cache[mxc_url] = (decoded_text, time.time())
            logger.debug(f"Cached MXC content for: {mxc_url}")

            # Clean old entries if cache is getting large
            if len(_mxc_cache) > 100:
                _clean_expired_cache()

            return decoded_text

    except Exception:
        logger.exception("Error downloading MXC content")
        return None


async def extract_and_resolve_message(
    event: nio.RoomMessageText,
    client: nio.AsyncClient | None = None,
) -> dict[str, Any]:
    """Extract message data and resolve large message content if needed.

    This is a convenience function that combines extraction and resolution
    of large message content in a single call.

    Args:
        event: The Matrix event to extract data from
        client: Optional Matrix client for downloading attachments

    Returns:
        Dict with sender, body, timestamp, event_id, and content fields.
        If the message is large and client is provided, body will contain
        the full text from the attachment.

    """
    # Extract basic message data
    data = {
        "sender": event.sender,
        "body": event.body,
        "timestamp": event.server_timestamp,
        "event_id": event.event_id,
        "content": event.source.get("content", {}),
    }

    # Check if this is a large message and resolve if we have a client
    if client and "io.mindroom.long_text" in data["content"]:
        data["body"] = await _get_full_message_body(data, client)

    return data


def _clean_expired_cache() -> None:
    """Remove expired entries from the MXC cache."""
    current_time = time.time()
    expired_keys = [key for key, (_, timestamp) in _mxc_cache.items() if current_time - timestamp >= _cache_ttl]
    for key in expired_keys:
        del _mxc_cache[key]
    if expired_keys:
        logger.debug(f"Cleaned {len(expired_keys)} expired cache entries")


def clear_mxc_cache() -> None:
    """Clear the entire MXC cache. Useful for testing."""
    _mxc_cache.clear()

================================================================================
# FILE: src/mindroom/matrix/presence.py
================================================================================

"""Matrix presence and status message utilities."""

from __future__ import annotations

from typing import TYPE_CHECKING

import nio

from mindroom.constants import ENABLE_STREAMING, ROUTER_AGENT_NAME
from mindroom.logging_config import get_logger

if TYPE_CHECKING:
    from mindroom.config import Config

logger = get_logger(__name__)


async def set_presence_status(
    client: nio.AsyncClient,
    status_msg: str,
    presence: str = "online",
) -> None:
    """Set presence status for a Matrix user.

    Args:
        client: The Matrix client
        status_msg: The status message to display
        presence: The presence state (online, offline, unavailable)

    """
    response = await client.set_presence(presence, status_msg)

    if isinstance(response, nio.PresenceSetResponse):
        logger.info(f"Set presence status: {status_msg}")
    else:
        logger.warning(f"Failed to set presence: {response}")


def build_agent_status_message(
    agent_name: str,
    config: Config,
) -> str:
    """Build status message with model and role information for an agent.

    Args:
        agent_name: Name of the agent
        config: Application configuration

    Returns:
        Status message string, limited to 250 characters

    """
    status_parts = []

    # Get model name using the config method
    model_name = config.get_entity_model_name(agent_name)

    # Format model info
    if model_name in config.models:
        model_config = config.models[model_name]
        model_info = f"{model_config.provider}/{model_config.id}"
    else:
        model_info = model_name

    status_parts.append(f"ðŸ¤– Model: {model_info}")

    # Add role/purpose for teams and agents
    if agent_name == ROUTER_AGENT_NAME:
        status_parts.append("ðŸ“ Routes messages to appropriate agents")
    elif agent_name in config.teams:
        team_config = config.teams[agent_name]
        if team_config.role:
            status_parts.append(f"ðŸ‘¥ {team_config.role[:100]}")  # Limit length
        status_parts.append(f"ðŸ¤ Team: {', '.join(team_config.agents[:5])}")  # Show first 5 agents
    elif agent_name in config.agents:
        agent_config = config.agents[agent_name]
        if agent_config.role:
            status_parts.append(f"ðŸ’¼ {agent_config.role[:100]}")  # Limit length
        # Add tool count
        if agent_config.tools:
            status_parts.append(f"ðŸ”§ {len(agent_config.tools)} tools available")

    # Join all parts with separators
    return " | ".join(status_parts)


async def is_user_online(
    client: nio.AsyncClient,
    user_id: str,
) -> bool:
    """Check if a Matrix user is currently online.

    Args:
        client: The Matrix client to use for the presence check
        user_id: The Matrix user ID string (e.g., "@user:example.com")

    Returns:
        True if the user is online or unavailable (active but busy),
        False if offline or presence check fails

    """
    try:
        response = await client.get_presence(user_id)

        # Check if we got an error response
        if isinstance(response, nio.PresenceGetError):
            logger.warning(
                "Presence API error",
                user_id=user_id,
                error=response.message,
            )
            return False

        # Presence states: "online", "unavailable" (busy/idle), "offline"
        # We consider both "online" and "unavailable" as "online" for streaming purposes
        # since "unavailable" usually means the user is idle but still has the client open
        is_online = response.presence in ("online", "unavailable")

        logger.debug(
            "User presence check",
            user_id=user_id,
            presence=response.presence,
            is_online=is_online,
            last_active_ago=response.last_active_ago,
        )

        return is_online  # noqa: TRY300

    except Exception:
        logger.exception(
            "Error checking user presence",
            user_id=user_id,
        )
        # Default to non-streaming on error (safer)
        return False


async def should_use_streaming(
    client: nio.AsyncClient,
    room_id: str,
    requester_user_id: str | None = None,
) -> bool:
    """Determine if streaming should be used based on user presence.

    This checks if the human user who sent the message is online.
    If they are online, we use streaming (message editing) for real-time updates.
    If they are offline, we send the complete message at once to save API calls.

    Args:
        client: The Matrix client
        room_id: The room where the interaction is happening
        requester_user_id: The user who sent the message (optional)

    Returns:
        True if streaming should be used, False otherwise

    """
    # Check if streaming is globally disabled
    if not ENABLE_STREAMING:
        return False

    # If no requester specified, we can't check presence, default to streaming
    if not requester_user_id:
        logger.debug("No requester specified, defaulting to streaming")
        return True

    # Check if the requester is online
    is_online = await is_user_online(client, requester_user_id)

    logger.info(
        "Streaming decision",
        room_id=room_id,
        requester=requester_user_id,
        is_online=is_online,
        use_streaming=is_online,
    )

    return is_online

================================================================================
# FILE: src/mindroom/matrix/rooms.py
================================================================================

"""Matrix room management functions."""

from __future__ import annotations

import time
from pathlib import Path
from typing import TYPE_CHECKING

import nio

from mindroom.logging_config import get_logger
from mindroom.topic_generator import ensure_room_has_topic, generate_room_topic_ai

from .client import check_and_set_avatar, create_room, join_room, leave_room, matrix_client
from .identity import MatrixID, extract_server_name_from_homeserver
from .state import MatrixRoom, MatrixState

if TYPE_CHECKING:
    from mindroom.config import Config

logger = get_logger(__name__)


def room_key_to_name(room_key: str) -> str:
    """Convert a room key to a human-readable room name.

    Args:
        room_key: The room key (e.g., 'dev', 'analysis_room')

    Returns:
        Human-readable room name (e.g., 'Dev', 'Analysis Room')

    """
    return room_key.replace("_", " ").title()


def load_rooms() -> dict[str, MatrixRoom]:
    """Load room state from YAML file."""
    state = MatrixState.load()
    return state.rooms


def get_room_aliases() -> dict[str, str]:
    """Get mapping of room aliases to room IDs."""
    state = MatrixState.load()
    return state.get_room_aliases()


def get_room_id(room_key: str) -> str | None:
    """Get room ID for a given room key/alias."""
    state = MatrixState.load()
    room = state.get_room(room_key)
    return room.room_id if room else None


def add_room(room_key: str, room_id: str, alias: str, name: str) -> None:
    """Add a new room to the state."""
    state = MatrixState.load()
    state.add_room(room_key, room_id, alias, name)
    state.save()


def remove_room(room_key: str) -> bool:
    """Remove a room from the state."""
    state = MatrixState.load()
    if room_key in state.rooms:
        del state.rooms[room_key]
        state.save()
        return True
    return False


def resolve_room_aliases(room_list: list[str]) -> list[str]:
    """Resolve room aliases to room IDs.

    Args:
        room_list: List of room aliases or IDs

    Returns:
        List of room IDs (aliases resolved to IDs, IDs passed through)

    """
    room_aliases = get_room_aliases()
    return [room_aliases.get(room, room) for room in room_list]


def get_room_alias_from_id(room_id: str) -> str | None:
    """Get room alias from room ID (reverse lookup).

    Args:
        room_id: Matrix room ID

    Returns:
        Room alias if found, None otherwise

    """
    room_aliases = get_room_aliases()
    for alias, rid in room_aliases.items():
        if rid == room_id:
            return alias
    return None


async def ensure_room_exists(  # noqa: C901
    client: nio.AsyncClient,
    room_key: str,
    config: Config,
    room_name: str | None = None,
    power_users: list[str] | None = None,
) -> str | None:
    """Ensure a room exists, creating it if necessary.

    Args:
        client: Matrix client to use for room creation
        room_key: The room key/alias (without domain)
        config: Configuration with agent settings for topic generation
        room_name: Display name for the room (defaults to room_key with underscores replaced)
        power_users: List of user IDs to grant power levels to

    Returns:
        Room ID if room exists or was created, None on failure

    """
    existing_rooms = load_rooms()

    # First, try to resolve the room alias on the server
    # This handles cases where the room exists on server but not in our state
    server_name = extract_server_name_from_homeserver(client.homeserver)
    full_alias = f"#{room_key}:{server_name}"

    response = await client.room_resolve_alias(full_alias)
    if isinstance(response, nio.RoomResolveAliasResponse):
        room_id = response.room_id
        logger.debug(f"Room alias {full_alias} exists on server, room ID: {room_id}")

        # Update our state if needed
        if room_key not in existing_rooms or existing_rooms[room_key].room_id != room_id:
            if room_name is None:
                room_name = room_key_to_name(room_key)
            add_room(room_key, room_id, full_alias, room_name)
            logger.info(f"Updated state with existing room {room_key} (ID: {room_id})")

        # Try to join the room
        if await join_room(client, room_id):
            # For existing rooms, ensure they have a topic set
            if room_name is None:
                room_name = room_key_to_name(room_key)
            await ensure_room_has_topic(client, room_id, room_key, room_name, config)
            return str(room_id)
        # Room exists but we can't join - this means the room was created
        # but this user isn't a member. Return the room ID anyway since
        # the room does exist and invitations will be handled separately
        logger.debug(f"Room {room_key} exists but user not a member, returning room ID for invitation handling")
        return str(room_id)

    # Room alias doesn't exist on server, so we can create it
    if room_key in existing_rooms:
        # Remove stale entry from state
        logger.debug(f"Removing stale room {room_key} from state")
        remove_room(room_key)

    # Create the room
    if room_name is None:
        room_name = room_key_to_name(room_key)

    # Generate a contextual topic for the room using AI
    topic = await generate_room_topic_ai(room_key, room_name, config)
    logger.info(f"Creating room {room_key} with topic: {topic}")

    created_room_id = await create_room(
        client=client,
        name=room_name,
        alias=room_key,
        topic=topic,
        power_users=power_users or [],
    )

    if created_room_id:
        # Save room info
        add_room(room_key, created_room_id, full_alias, room_name)
        logger.info(f"Created room {room_key} with ID {created_room_id}")

        # Set room avatar if available (for newly created rooms)
        # Note: Avatars can also be updated later using scripts/generate_avatars.py
        avatar_path = Path(__file__).parent.parent.parent.parent / "avatars" / "rooms" / f"{room_key}.png"
        if avatar_path.exists():
            if await check_and_set_avatar(client, avatar_path, room_id=created_room_id):
                logger.info(f"Set avatar for newly created room {room_key}")
            else:
                logger.warning(f"Failed to set avatar for room {room_key}")

        return created_room_id
    logger.error(f"Failed to create room {room_key}")
    return None


async def ensure_all_rooms_exist(
    client: nio.AsyncClient,
    config: Config,
) -> dict[str, str]:
    """Ensure all configured rooms exist and invite user account.

    Args:
        client: Matrix client to use for room creation
        config: Configuration with room settings

    Returns:
        Dict mapping room keys to room IDs

    """
    from mindroom.agents import get_agent_ids_for_room  # noqa: PLC0415

    room_ids = {}

    # Get all configured rooms
    all_rooms = config.get_all_configured_rooms()

    for room_key in all_rooms:
        # Skip if this is a room ID (starts with !)
        if room_key.startswith("!"):
            # This is a room ID, not a room key/alias - skip it
            continue

        # Get power users for this room
        power_users = get_agent_ids_for_room(room_key, config)

        # Ensure room exists
        room_id = await ensure_room_exists(
            client=client,
            room_key=room_key,
            config=config,
            power_users=power_users,
        )

        if room_id:
            room_ids[room_key] = room_id

    return room_ids


async def ensure_user_in_rooms(homeserver: str, room_ids: dict[str, str]) -> None:
    """Ensure the user account is a member of all specified rooms.

    Args:
        homeserver: Matrix homeserver URL
        room_ids: Dict mapping room keys to room IDs

    """
    state = MatrixState.load()
    # User account is stored as "agent_user" (treated as a special agent)
    user_account = state.get_account("agent_user")
    if not user_account:
        logger.warning("No user account found, skipping user room membership")
        return

    server_name = extract_server_name_from_homeserver(homeserver)
    user_id = MatrixID.from_username(user_account.username, server_name).full_id

    # Create a client for the user to join rooms
    async with matrix_client(homeserver, user_id) as user_client:
        # Login as the user
        login_response = await user_client.login(password=user_account.password)
        if not isinstance(login_response, nio.LoginResponse):
            logger.error(f"Failed to login as user {user_id}: {login_response}")
            return

        logger.info(f"User {user_id} logged in to join rooms")

        for room_key, room_id in room_ids.items():
            # Try to join the room (will work if invited or room is public)
            join_success = await join_room(user_client, room_id)
            if join_success:
                logger.info(f"User {user_id} joined room {room_key}")
            else:
                logger.warning(f"User {user_id} failed to join room {room_key} - may need invitation")


DM_ROOM_CACHE: dict[tuple[str, str], tuple[float, bool]] = {}
DIRECT_ROOMS_CACHE: dict[str, tuple[float, set[str]]] = {}
_DM_ROOM_TTL: float = 300  # seconds
_DIRECT_ROOMS_TTL: float = 300  # seconds


def _dm_cache_key(client: nio.AsyncClient, room_id: str) -> tuple[str, str]:
    """Build a cache key that is scoped per user.

    DM membership via ``m.direct`` is account-specific, so room-only cache keys
    can leak incorrect results between different bot users.
    """
    return (str(client.user_id or ""), room_id)


async def _get_direct_room_ids(client: nio.AsyncClient) -> set[str]:
    """Get DM room IDs from the user's ``m.direct`` account data.

    Results are cached per user for ``_DIRECT_ROOMS_TTL`` seconds so that
    newly created DM rooms are picked up without a restart.
    """
    user_id = str(client.user_id or "")
    if not user_id:
        return set()

    cached = DIRECT_ROOMS_CACHE.get(user_id)
    if cached is not None:
        ts, room_ids = cached
        if time.monotonic() - ts < _DIRECT_ROOMS_TTL:
            return room_ids

    response = await client.list_direct_rooms()
    if isinstance(response, nio.DirectRoomsResponse):
        direct_room_ids = {room_id for room_ids in response.rooms.values() for room_id in room_ids}
        DIRECT_ROOMS_CACHE[user_id] = (time.monotonic(), direct_room_ids)
        return direct_room_ids
    if isinstance(response, nio.DirectRoomsErrorResponse) and response.status_code == "M_NOT_FOUND":
        # No m.direct account data is a stable empty state for this user.
        DIRECT_ROOMS_CACHE[user_id] = (time.monotonic(), set())

    return set()


def _is_two_member_group_room(client: nio.AsyncClient, room_id: str) -> bool:
    """Check if nio models this room as an unnamed two-member group.

    Rooms with an explicit topic are excluded because DMs almost never have one,
    while small project rooms often do.
    """
    room_lookup = getattr(client, "rooms", None)
    if not isinstance(room_lookup, dict):
        return False

    room = room_lookup.get(room_id)
    if room is None or not room.is_group or room.member_count != 2:
        return False
    return not room.topic


def _has_is_direct_marker(state_events: list[dict[str, object]]) -> bool:
    """Check ``m.room.member`` state events for the ``is_direct`` flag."""
    for event in state_events:
        if event.get("type") != "m.room.member":
            continue

        content = event.get("content")
        if isinstance(content, dict) and content.get("is_direct") is True:
            return True

    return False


async def is_dm_room(client: nio.AsyncClient, room_id: str) -> bool:
    """Check if a room is a Direct Message (DM) room.

    Detection uses multiple signals in this order:
    1. ``m.direct`` account data (via ``/account_data/m.direct``)
    2. Nio's in-memory room model for 2-member ad-hoc rooms
    3. Room state events with ``is_direct=true``

    Args:
        client: The Matrix client
        room_id: The room ID to check

    Returns:
        True if the room is a DM room, False otherwise

    """
    cache_key = _dm_cache_key(client, room_id)
    cached = DM_ROOM_CACHE.get(cache_key)
    if cached is not None:
        ts, is_dm = cached
        if time.monotonic() - ts < _DM_ROOM_TTL:
            return is_dm

    direct_room_ids = await _get_direct_room_ids(client)
    if room_id in direct_room_ids:
        DM_ROOM_CACHE[cache_key] = (time.monotonic(), True)
        return True

    # Preserve DM-like rooms even when servers don't expose `is_direct` in state.
    if _is_two_member_group_room(client, room_id):
        DM_ROOM_CACHE[cache_key] = (time.monotonic(), True)
        return True

    # Get the room state events, specifically member events.
    response = await client.room_get_state(room_id)
    if not isinstance(response, nio.RoomGetStateResponse):
        return False

    is_dm = _has_is_direct_marker(response.events)
    DM_ROOM_CACHE[cache_key] = (time.monotonic(), is_dm)
    return is_dm


async def leave_non_dm_rooms(
    client: nio.AsyncClient,
    room_ids: list[str],
) -> None:
    """Leave all rooms in *room_ids* that are not DM rooms."""
    for room_id in room_ids:
        if await is_dm_room(client, room_id):
            logger.debug(f"Preserving DM room {room_id}")
            continue
        success = await leave_room(client, room_id)
        if success:
            logger.info(f"Left room {room_id}")
        else:
            logger.error(f"Failed to leave room {room_id}")

================================================================================
# FILE: src/mindroom/matrix/state.py
================================================================================

"""Pydantic models for Matrix state."""

from datetime import UTC, datetime
from typing import Self

import yaml
from pydantic import BaseModel, Field, field_serializer

from mindroom.constants import MATRIX_STATE_FILE


class MatrixAccount(BaseModel):
    """Represents a Matrix account (user or agent)."""

    username: str
    password: str


class MatrixRoom(BaseModel):
    """Represents a Matrix room state."""

    room_id: str
    alias: str
    name: str
    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))

    @field_serializer("created_at")
    def serialize_datetime(self, dt: datetime) -> str:
        """Serialize datetime to ISO format string."""
        return dt.isoformat()


class MatrixState(BaseModel):
    """Complete Matrix state including accounts and rooms."""

    accounts: dict[str, MatrixAccount] = Field(default_factory=dict)
    rooms: dict[str, MatrixRoom] = Field(default_factory=dict)

    @classmethod
    def load(cls) -> Self:
        """Load state from file."""
        if not MATRIX_STATE_FILE.exists():
            return cls()

        with MATRIX_STATE_FILE.open() as f:
            data = yaml.safe_load(f) or {}

        return cls.model_validate(data)

    def save(self) -> None:
        """Save state to file."""
        # Use Pydantic's model_dump with custom serializer for datetime
        data = self.model_dump(mode="json")

        MATRIX_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
        with MATRIX_STATE_FILE.open("w") as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False)

    def get_account(self, key: str) -> MatrixAccount | None:
        """Get an account by key."""
        return self.accounts.get(key)

    def add_account(self, key: str, username: str, password: str) -> None:
        """Add or update an account."""
        self.accounts[key] = MatrixAccount(username=username, password=password)

    def get_room(self, key: str) -> MatrixRoom | None:
        """Get a room by key."""
        return self.rooms.get(key)

    def add_room(self, key: str, room_id: str, alias: str, name: str) -> None:
        """Add or update a room."""
        self.rooms[key] = MatrixRoom(room_id=room_id, alias=alias, name=name, created_at=datetime.now(tz=UTC))

    def get_room_aliases(self) -> dict[str, str]:
        """Get mapping of room aliases to room IDs."""
        return {key: room.room_id for key, room in self.rooms.items()}

================================================================================
# FILE: src/mindroom/matrix/typing.py
================================================================================

"""Typing indicator management for Matrix agents."""

from __future__ import annotations

import asyncio
from contextlib import asynccontextmanager, suppress
from typing import TYPE_CHECKING

import nio

from mindroom.logging_config import get_logger

if TYPE_CHECKING:
    from collections.abc import AsyncGenerator

logger = get_logger(__name__)


async def set_typing(
    client: nio.AsyncClient,
    room_id: str,
    typing: bool = True,
    timeout_seconds: int = 30,
) -> None:
    """Set typing status for a user in a room.

    Args:
        client: Matrix client instance
        room_id: Room to show typing indicator in
        typing: Whether to show or hide typing indicator
        timeout_seconds: How long the typing indicator should last (in seconds)

    """
    timeout_ms = timeout_seconds * 1000
    response = await client.room_typing(room_id, typing, timeout_ms)
    if isinstance(response, nio.RoomTypingError):
        logger.warning(
            "Failed to set typing status",
            room_id=room_id,
            typing=typing,
            error=response.message,
        )
    else:
        logger.debug("Set typing status", room_id=room_id, typing=typing)


@asynccontextmanager
async def typing_indicator(
    client: nio.AsyncClient,
    room_id: str,
    timeout_seconds: int = 30,
) -> AsyncGenerator[None, None]:
    """Context manager for showing typing indicator while processing.

    Usage:
        async with typing_indicator(client, room_id):
            # Do work here - typing indicator shown
            response = await generate_response()
        # Typing indicator automatically stopped

    Args:
        client: Matrix client instance
        room_id: Room to show typing indicator in
        timeout_seconds: How long each typing notification lasts

    """
    # Start typing
    await set_typing(client, room_id, True, timeout_seconds)

    # Create a task to periodically refresh the typing indicator
    # Matrix typing indicators expire, so we need to refresh them
    refresh_interval = min(timeout_seconds / 2, 15)  # Refresh at half timeout or 15s

    async def refresh_typing() -> None:
        """Refresh typing indicator periodically."""
        while True:
            await asyncio.sleep(refresh_interval)
            await set_typing(client, room_id, True, timeout_seconds)

    refresh_task = asyncio.create_task(refresh_typing())

    try:
        yield
    finally:
        # Cancel refresh task
        refresh_task.cancel()
        with suppress(asyncio.CancelledError):
            await refresh_task

        # Stop typing
        await set_typing(client, room_id, False)

================================================================================
# FILE: src/mindroom/matrix/users.py
================================================================================

"""Matrix user account management for agents."""

from dataclasses import dataclass
from functools import cached_property

import nio

from mindroom.config import Config
from mindroom.constants import ROUTER_AGENT_NAME
from mindroom.logging_config import get_logger

from .client import login, register_user
from .identity import MatrixID, extract_server_name_from_homeserver
from .state import MatrixState

logger = get_logger(__name__)


def extract_domain_from_user_id(user_id: str) -> str:
    """Extract domain from a Matrix user ID like "@user:example.com"."""
    if not user_id.startswith("@") or ":" not in user_id:
        return "localhost"
    return MatrixID.parse(user_id).domain


@dataclass
class AgentMatrixUser:
    """Represents a Matrix user account for an agent."""

    agent_name: str
    user_id: str
    display_name: str
    password: str
    access_token: str | None = None

    @cached_property
    def matrix_id(self) -> MatrixID:
        """MatrixID object from user_id."""
        return MatrixID.parse(self.user_id)


def get_agent_credentials(agent_name: str) -> dict[str, str] | None:
    """Get credentials for a specific agent from matrix_state.yaml.

    Args:
        agent_name: The agent name

    Returns:
        Dictionary with username and password, or None if not found

    """
    state = MatrixState.load()
    agent_key = f"agent_{agent_name}"
    account = state.get_account(agent_key)
    if account:
        return {"username": account.username, "password": account.password}
    return None


def save_agent_credentials(agent_name: str, username: str, password: str) -> None:
    """Save credentials for a specific agent to matrix_state.yaml.

    Args:
        agent_name: The agent name
        username: The Matrix username
        password: The Matrix password

    """
    state = MatrixState.load()
    agent_key = f"agent_{agent_name}"
    state.add_account(agent_key, username, password)
    state.save()
    logger.info(f"Saved credentials for agent {agent_name}")


async def create_agent_user(
    homeserver: str,
    agent_name: str,
    agent_display_name: str,
) -> AgentMatrixUser:
    """Create or retrieve a Matrix user account for an agent.

    Args:
        homeserver: The Matrix homeserver URL
        agent_name: The internal agent name (e.g., 'calculator')
        agent_display_name: The display name for the agent (e.g., 'CalculatorAgent')

    Returns:
        AgentMatrixUser object with account details

    """
    # Check if credentials already exist in matrix_state.yaml
    existing_creds = get_agent_credentials(agent_name)

    if existing_creds:
        username = existing_creds["username"]
        password = existing_creds["password"]
        logger.info(f"Using existing credentials for agent {agent_name} from matrix_state.yaml")
        registration_needed = False
    else:
        # Generate new credentials
        username = f"mindroom_{agent_name}"
        password = f"{agent_name}_secure_password"  # _{os.urandom(8).hex()}"
        logger.info(f"Generated new credentials for agent {agent_name}")
        registration_needed = True

    # Extract server name from homeserver URL
    server_name = extract_server_name_from_homeserver(homeserver)
    user_id = MatrixID.from_username(username, server_name).full_id

    # Try to register/verify the user
    try:
        await register_user(
            homeserver=homeserver,
            username=username,
            password=password,
            display_name=agent_display_name,
        )
        # Only save credentials after successful registration
        if registration_needed:
            save_agent_credentials(agent_name, username, password)
            logger.info(f"Saved credentials for agent {agent_name} after successful registration")
    except ValueError as e:
        # If user already exists, that's fine
        error_msg = str(e) if e else ""
        logger.debug(f"ValueError when registering {username}: {error_msg}")
        if "already exists" not in error_msg and "RegisterErrorResponse" not in error_msg:
            raise
        # Save credentials if the user already exists (registration succeeded in the past)
        if registration_needed and "already exists" in error_msg:
            save_agent_credentials(agent_name, username, password)
            logger.info(f"Saved credentials for agent {agent_name} (user already exists)")

    return AgentMatrixUser(
        agent_name=agent_name,
        user_id=user_id,
        display_name=agent_display_name,
        password=password,
    )


async def login_agent_user(homeserver: str, agent_user: AgentMatrixUser) -> nio.AsyncClient:
    """Login an agent user and return the authenticated client.

    Args:
        homeserver: The Matrix homeserver URL
        agent_user: The agent user to login

    Returns:
        Authenticated AsyncClient instance

    Raises:
        ValueError: If login fails

    """
    client = await login(homeserver, agent_user.user_id, agent_user.password)
    agent_user.access_token = client.access_token
    return client


# TODO: Check, this seems unused!
async def ensure_all_agent_users(homeserver: str, config: Config) -> dict[str, AgentMatrixUser]:
    """Ensure all configured agents and teams have Matrix user accounts.

    This includes user-configured agents, teams, and the built-in router agent.

    Args:
        homeserver: The Matrix homeserver URL
        config: Application configuration

    Returns:
        Dictionary mapping agent/team names to AgentMatrixUser objects

    """
    agent_users = {}

    # First, create the built-in router agent
    try:
        router_user = await create_agent_user(
            homeserver,
            ROUTER_AGENT_NAME,
            "RouterAgent",
        )
        agent_users[ROUTER_AGENT_NAME] = router_user
        logger.info(f"Ensured Matrix user for built-in router agent: {router_user.user_id}")
    except Exception:
        logger.exception("Failed to create Matrix user for built-in router agent")

    # Create user-configured agents
    for agent_name, agent_config in config.agents.items():
        try:
            agent_user = await create_agent_user(
                homeserver,
                agent_name,
                agent_config.display_name,
            )
            agent_users[agent_name] = agent_user
            logger.info(f"Ensured Matrix user for agent: {agent_name} -> {agent_user.user_id}")
        except Exception:
            # Continue with other agents even if one fails
            logger.exception("Failed to create Matrix user for agent", agent_name=agent_name)

    # Create team users
    for team_name, team_config in config.teams.items():
        try:
            team_user = await create_agent_user(
                homeserver,
                team_name,
                team_config.display_name,
            )
            agent_users[team_name] = team_user
            logger.info(f"Ensured Matrix user for team: {team_name} -> {team_user.user_id}")
        except Exception:
            # Continue with other teams even if one fails
            logger.exception("Failed to create Matrix user for team", team_name=team_name)

    return agent_users

================================================================================
# FILE: src/mindroom/memory/__init__.py
================================================================================

"""Memory management for mindroom agents and rooms."""

from .functions import (
    add_agent_memory,
    add_room_memory,
    build_memory_enhanced_prompt,
    format_memories_as_context,
    list_all_agent_memories,
    search_agent_memories,
    search_room_memories,
    store_conversation_memory,
)

__all__ = [
    "add_agent_memory",
    "add_room_memory",
    "build_memory_enhanced_prompt",
    "format_memories_as_context",
    "list_all_agent_memories",
    "search_agent_memories",
    "search_room_memories",
    "store_conversation_memory",
]

================================================================================
# FILE: src/mindroom/memory/config.py
================================================================================

"""Memory configuration and setup."""

import os
from pathlib import Path
from typing import Any

from mem0 import AsyncMemory

from mindroom.config import Config
from mindroom.credentials import get_credentials_manager
from mindroom.logging_config import get_logger

logger = get_logger(__name__)


def get_memory_config(storage_path: Path, config: Config) -> dict:  # noqa: C901, PLR0912
    """Get Mem0 configuration with ChromaDB backend.

    Args:
        storage_path: Base directory for memory storage
        config: Application configuration

    Returns:
        Configuration dictionary for Mem0

    """
    app_config = config
    creds_manager = get_credentials_manager()

    # Ensure storage directories exist
    chroma_path = storage_path / "chroma"
    chroma_path.mkdir(parents=True, exist_ok=True)

    # Build embedder config from config.yaml
    embedder_config: dict[str, Any] = {
        "provider": app_config.memory.embedder.provider,
        "config": {
            "model": app_config.memory.embedder.config.model,
        },
    }

    # Add provider-specific configuration
    if app_config.memory.embedder.provider == "openai":
        # Set environment variable from CredentialsManager for Mem0 to use
        api_key = creds_manager.get_api_key("openai")
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
        # Support custom OpenAI-compatible base URL (e.g., llama.cpp)
        if app_config.memory.embedder.config.host:
            embedder_config["config"]["openai_base_url"] = app_config.memory.embedder.config.host
    elif app_config.memory.embedder.provider == "ollama":
        # Check CredentialsManager for Ollama host
        ollama_creds = creds_manager.load_credentials("ollama")
        if ollama_creds and "host" in ollama_creds:
            host = ollama_creds["host"]
        else:
            host = app_config.memory.embedder.config.host or "http://localhost:11434"
        embedder_config["config"]["ollama_base_url"] = host

    # Build LLM config from memory configuration
    if app_config.memory.llm:
        llm_config: dict[str, Any] = {
            "provider": app_config.memory.llm.provider,
            "config": {},
        }

        # Copy config but handle provider-specific field names
        for key, value in app_config.memory.llm.config.items():
            if key == "host" and app_config.memory.llm.provider == "ollama":
                # Check CredentialsManager for Ollama host
                ollama_creds = creds_manager.load_credentials("ollama")
                if ollama_creds and "host" in ollama_creds:
                    llm_config["config"]["ollama_base_url"] = ollama_creds["host"]
                else:
                    llm_config["config"]["ollama_base_url"] = value or "http://localhost:11434"
            elif key != "host":  # Skip host for other fields
                llm_config["config"][key] = value

        # Set environment variables from CredentialsManager for Mem0 to use
        if app_config.memory.llm.provider == "openai":
            api_key = creds_manager.get_api_key("openai")
            if api_key:
                os.environ["OPENAI_API_KEY"] = api_key
        elif app_config.memory.llm.provider == "anthropic":
            api_key = creds_manager.get_api_key("anthropic")
            if api_key:
                os.environ["ANTHROPIC_API_KEY"] = api_key

        logger.info(
            f"Using {app_config.memory.llm.provider} model '{app_config.memory.llm.config.get('model')}' for memory",
        )
    else:
        # Fallback if no LLM configured
        logger.warning("No memory LLM configured, using default ollama/llama3.2")
        # Check CredentialsManager for Ollama host
        ollama_creds = creds_manager.load_credentials("ollama")
        ollama_host = ollama_creds["host"] if ollama_creds and "host" in ollama_creds else "http://localhost:11434"

        llm_config = {
            "provider": "ollama",
            "config": {
                "model": "llama3.2",
                "ollama_base_url": ollama_host,
                "temperature": 0.1,
                "top_p": 1,
            },
        }

    return {
        "embedder": embedder_config,
        "llm": llm_config,
        "vector_store": {
            "provider": "chroma",
            "config": {
                "collection_name": "mindroom_memories",
                "path": str(chroma_path),
            },
        },
    }


async def create_memory_instance(storage_path: Path, config: Config) -> AsyncMemory:
    """Create a Mem0 memory instance with ChromaDB backend.

    Args:
        storage_path: Base directory for memory storage
        config: Application configuration

    Returns:
        Configured AsyncMemory instance

    """
    config_dict = get_memory_config(storage_path, config)

    # Create AsyncMemory instance with dictionary config directly
    # Mem0 expects a dict for configuration, not config objects
    memory = await AsyncMemory.from_config(config_dict)

    logger.info(f"Created memory instance with ChromaDB at {storage_path}")
    return memory

================================================================================
# FILE: src/mindroom/memory/functions.py
================================================================================

"""Simple memory management functions following Mem0 patterns."""

from __future__ import annotations

from typing import TYPE_CHECKING, Any, Protocol, TypedDict, cast

from mindroom.logging_config import get_logger

from .config import create_memory_instance

if TYPE_CHECKING:
    from pathlib import Path

    from mindroom.config import Config


class MemoryResult(TypedDict, total=False):
    """Type for memory search results from Mem0."""

    id: str
    memory: str
    hash: str
    metadata: dict[str, Any] | None
    score: float
    created_at: str
    updated_at: str | None
    user_id: str


logger = get_logger(__name__)


class ScopedMemoryReader(Protocol):
    """Minimal protocol for reading a memory by ID."""

    async def get(self, memory_id: str) -> object:
        """Return the memory payload for a given memory ID."""


class MemoryNotFoundError(ValueError):
    """Raised when a memory ID does not exist in the caller's allowed scope."""

    def __init__(self, memory_id: str) -> None:
        super().__init__(f"No memory found with id={memory_id}")


def _build_team_user_id(agent_names: list[str]) -> str:
    """Build a canonical team user_id from agent names."""
    return f"team_{'+'.join(sorted(agent_names))}"


def _get_allowed_memory_user_ids(caller_context: str | list[str], config: Config) -> set[str]:
    """Get all user_id scopes the caller is allowed to access."""
    if isinstance(caller_context, list):
        return {_build_team_user_id(caller_context)}

    allowed_user_ids = {f"agent_{caller_context}"}
    allowed_user_ids.update(get_team_ids_for_agent(caller_context, config))
    return allowed_user_ids


async def _get_scoped_memory_by_id(
    memory: ScopedMemoryReader,
    memory_id: str,
    caller_context: str | list[str],
    config: Config,
) -> MemoryResult | None:
    """Fetch a memory and ensure it belongs to the caller's allowed scopes."""
    result = await memory.get(memory_id)
    if not isinstance(result, dict):
        return None

    allowed_user_ids = _get_allowed_memory_user_ids(caller_context, config)
    memory_user_id = result.get("user_id")
    if memory_user_id not in allowed_user_ids:
        logger.warning(
            "Memory access denied",
            memory_id=memory_id,
            memory_user_id=memory_user_id,
            allowed_user_ids=sorted(allowed_user_ids),
        )
        return None

    return cast("MemoryResult", result)


async def add_agent_memory(
    content: str,
    agent_name: str,
    storage_path: Path,
    config: Config,
    metadata: dict | None = None,
) -> None:
    """Add a memory for an agent.

    Args:
        content: The memory content to store
        agent_name: Name of the agent
        storage_path: Storage path for memory
        config: Application configuration
        metadata: Optional metadata to store with memory

    """
    memory = await create_memory_instance(storage_path, config)

    if metadata is None:
        metadata = {}
    metadata["agent"] = agent_name

    messages = [{"role": "user", "content": content}]

    # Use agent_name as user_id to namespace memories per agent
    try:
        await memory.add(messages, user_id=f"agent_{agent_name}", metadata=metadata)
        logger.info("Memory added", agent=agent_name)
    except Exception:
        logger.exception("Failed to add memory", agent=agent_name)
        raise


def get_team_ids_for_agent(agent_name: str, config: Config) -> list[str]:
    """Get all team IDs that include the specified agent.

    Args:
        agent_name: Name of the agent to find teams for
        config: Application configuration containing team definitions

    Returns:
        List of team IDs (in the format "team_agent1+agent2+...")

    """
    team_ids: list[str] = []

    if not config.teams:
        return team_ids

    for team_config in config.teams.values():
        if agent_name in team_config.agents:
            # Create the same team ID format used in storage
            sorted_agents = sorted(team_config.agents)
            team_id = f"team_{'+'.join(sorted_agents)}"
            team_ids.append(team_id)

    return team_ids


async def search_agent_memories(
    query: str,
    agent_name: str,
    storage_path: Path,
    config: Config,
    limit: int = 3,
) -> list[MemoryResult]:
    """Search agent memories including team memories.

    Args:
        query: Search query
        agent_name: Name of the agent
        storage_path: Storage path for memory
        config: Application configuration
        limit: Maximum number of results

    Returns:
        List of relevant memories from both individual and team contexts

    """
    memory = await create_memory_instance(storage_path, config)

    # Search individual agent memories
    search_result = await memory.search(query, user_id=f"agent_{agent_name}", limit=limit)
    results = search_result["results"] if isinstance(search_result, dict) and "results" in search_result else []

    # Also search team memories
    team_ids = get_team_ids_for_agent(agent_name, config)
    for team_id in team_ids:
        team_result = await memory.search(query, user_id=team_id, limit=limit)
        team_memories = team_result["results"] if isinstance(team_result, dict) and "results" in team_result else []

        # Merge results, avoiding duplicates based on memory content
        existing_memories = {r.get("memory", "") for r in results}
        for mem in team_memories:
            if mem.get("memory", "") not in existing_memories:
                results.append(mem)

        logger.debug("Team memories found", team_id=team_id, count=len(team_memories))

    logger.debug("Total memories found", count=len(results), agent=agent_name)

    # Return top results after merging
    return results[:limit]


async def list_all_agent_memories(
    agent_name: str,
    storage_path: Path,
    config: Config,
    limit: int = 100,
) -> list[MemoryResult]:
    """List all memories for an agent.

    Args:
        agent_name: Name of the agent
        storage_path: Storage path for memory
        config: Application configuration
        limit: Maximum number of memories to return

    Returns:
        List of all agent memories

    """
    memory = await create_memory_instance(storage_path, config)
    result = await memory.get_all(user_id=f"agent_{agent_name}", limit=limit)
    return result["results"] if isinstance(result, dict) and "results" in result else []


async def get_agent_memory(
    memory_id: str,
    caller_context: str | list[str],
    storage_path: Path,
    config: Config,
) -> MemoryResult | None:
    """Get a single memory by ID.

    Args:
        memory_id: The memory ID to retrieve
        caller_context: Agent name or team members requesting this memory
        storage_path: Storage path for memory
        config: Application configuration

    Returns:
        The memory dict, or None if not found

    """
    memory = await create_memory_instance(storage_path, config)
    return await _get_scoped_memory_by_id(memory, memory_id, caller_context, config)


async def update_agent_memory(
    memory_id: str,
    content: str,
    caller_context: str | list[str],
    storage_path: Path,
    config: Config,
) -> None:
    """Update a single memory by ID.

    Args:
        memory_id: The memory ID to update
        content: The new content for the memory
        caller_context: Agent name or team members requesting this update
        storage_path: Storage path for memory
        config: Application configuration

    """
    memory = await create_memory_instance(storage_path, config)
    scoped_memory = await _get_scoped_memory_by_id(memory, memory_id, caller_context, config)
    if scoped_memory is None:
        raise MemoryNotFoundError(memory_id)
    await memory.update(memory_id, content)
    logger.info("Memory updated", memory_id=memory_id)


async def delete_agent_memory(
    memory_id: str,
    caller_context: str | list[str],
    storage_path: Path,
    config: Config,
) -> None:
    """Delete a single memory by ID.

    Args:
        memory_id: The memory ID to delete
        caller_context: Agent name or team members requesting this deletion
        storage_path: Storage path for memory
        config: Application configuration

    """
    memory = await create_memory_instance(storage_path, config)
    scoped_memory = await _get_scoped_memory_by_id(memory, memory_id, caller_context, config)
    if scoped_memory is None:
        raise MemoryNotFoundError(memory_id)
    await memory.delete(memory_id)
    logger.info("Memory deleted", memory_id=memory_id)


async def add_room_memory(
    content: str,
    room_id: str,
    storage_path: Path,
    config: Config,
    agent_name: str | None = None,
    metadata: dict | None = None,
) -> None:
    """Add a memory for a room.

    Args:
        content: The memory content to store
        room_id: Room ID
        storage_path: Storage path for memory
        config: Application configuration
        agent_name: Optional agent that created this memory
        metadata: Optional metadata to store with memory

    """
    memory = await create_memory_instance(storage_path, config)

    if metadata is None:
        metadata = {}
    metadata["room_id"] = room_id
    if agent_name:
        metadata["contributed_by"] = agent_name

    messages = [{"role": "user", "content": content}]

    safe_room_id = room_id.replace(":", "_").replace("!", "")
    await memory.add(messages, user_id=f"room_{safe_room_id}", metadata=metadata)
    logger.debug("Room memory added", room_id=room_id)


async def search_room_memories(
    query: str,
    room_id: str,
    storage_path: Path,
    config: Config,
    limit: int = 3,
) -> list[MemoryResult]:
    """Search room memories.

    Args:
        query: Search query
        room_id: Room ID
        storage_path: Storage path for memory
        config: Application configuration
        limit: Maximum number of results

    Returns:
        List of relevant memories

    """
    memory = await create_memory_instance(storage_path, config)
    safe_room_id = room_id.replace(":", "_").replace("!", "")
    search_result = await memory.search(query, user_id=f"room_{safe_room_id}", limit=limit)

    results = search_result["results"] if isinstance(search_result, dict) and "results" in search_result else []

    logger.debug("Room memories found", count=len(results), room_id=room_id)
    return results


def format_memories_as_context(memories: list[MemoryResult], context_type: str = "agent") -> str:
    """Format memories into a context string.

    Args:
        memories: List of memory objects from search
        context_type: Type of context ("agent" or "room")

    Returns:
        Formatted context string

    """
    if not memories:
        return ""

    context_parts = [
        f"[Automatically extracted {context_type} memories - may not be relevant to current context]",
        f"Previous {context_type} memories that might be related:",
    ]
    for memory in memories:
        content = memory.get("memory", "")
        context_parts.append(f"- {content}")

    return "\n".join(context_parts)


async def build_memory_enhanced_prompt(
    prompt: str,
    agent_name: str,
    storage_path: Path,
    config: Config,
    room_id: str | None = None,
) -> str:
    """Build a prompt enhanced with relevant memories.

    Args:
        prompt: The original user prompt
        agent_name: Name of the agent
        storage_path: Path for memory storage
        config: Application configuration
        room_id: Optional room ID for room context

    Returns:
        Enhanced prompt with memory context

    """
    logger.debug("Building enhanced prompt", agent=agent_name)
    enhanced_prompt = prompt

    agent_memories = await search_agent_memories(prompt, agent_name, storage_path, config)
    if agent_memories:
        agent_context = format_memories_as_context(agent_memories, "agent")
        enhanced_prompt = f"{agent_context}\n\n{prompt}"
        logger.debug("Agent memories added", count=len(agent_memories))

    if room_id:
        room_memories = await search_room_memories(prompt, room_id, storage_path, config)
        if room_memories:
            room_context = format_memories_as_context(room_memories, "room")
            enhanced_prompt = f"{room_context}\n\n{enhanced_prompt}"
            logger.debug("Room memories added", count=len(room_memories))

    return enhanced_prompt


def _build_conversation_messages(
    thread_history: list[dict],
    current_prompt: str,
    user_id: str,
) -> list[dict]:
    """Build conversation messages in mem0 format from thread history.

    Args:
        thread_history: List of messages with sender and body
        current_prompt: The current user prompt being processed
        user_id: The Matrix user ID to identify user messages

    Returns:
        List of messages in mem0 format with role and content

    """
    messages = []

    # Process thread history
    for msg in thread_history:
        body = msg.get("body", "").strip()
        if not body:
            continue

        sender = msg.get("sender", "")
        # Determine role based on sender
        # If sender matches the user, it's a user message; otherwise it's assistant
        role = "user" if sender == user_id else "assistant"
        messages.append({"role": role, "content": body})

    # Add the current prompt as a user message
    messages.append({"role": "user", "content": current_prompt})

    return messages


async def store_conversation_memory(
    prompt: str,
    agent_name: str | list[str],
    storage_path: Path,
    session_id: str,
    config: Config,
    room_id: str | None = None,
    thread_history: list[dict] | None = None,
    user_id: str | None = None,
) -> None:
    """Store conversation in memory for future recall.

    Uses mem0's intelligent extraction to identify relevant facts, preferences,
    and context from the conversation. Provides full conversation context when
    available to allow better understanding of user intent.

    For teams, pass a list of agent names to store memory once under a shared
    namespace, avoiding duplicate LLM processing.

    Args:
        prompt: The current user prompt
        agent_name: Name of the agent or list of agent names for teams
        storage_path: Path for memory storage
        session_id: Session ID for the conversation
        config: Application configuration
        room_id: Optional room ID for room memory
        thread_history: Optional thread history for context
        user_id: Optional user ID to identify user messages in thread

    """
    if not prompt:
        return

    # Build conversation messages in mem0 format
    if thread_history and user_id:
        # Use structured messages with roles for better context
        messages = _build_conversation_messages(thread_history, prompt, user_id)
    else:
        # Fallback to simple user message
        messages = [{"role": "user", "content": prompt}]

    # Store for agent memory with structured messages
    memory = await create_memory_instance(storage_path, config)

    # Handle both single agents and teams
    if isinstance(agent_name, list):
        # For teams, store once under a team namespace
        # Sort agent names for consistent team ID
        team_id = _build_team_user_id(agent_name)

        metadata = {
            "type": "conversation",
            "session_id": session_id,
            "is_team": True,
            "team_members": agent_name,  # Keep original order for reference
        }

        try:
            await memory.add(messages, user_id=team_id, metadata=metadata)
            logger.info("Team memory added", team_id=team_id, members=agent_name)
        except Exception as e:
            logger.exception("Failed to add team memory", team_id=team_id, error=str(e))
    else:
        # Single agent - store normally
        metadata = {
            "type": "conversation",
            "session_id": session_id,
            "agent": agent_name,
        }

        try:
            await memory.add(messages, user_id=f"agent_{agent_name}", metadata=metadata)
            logger.info("Memory added", agent=agent_name)
        except Exception as e:
            logger.exception("Failed to add memory", agent=agent_name, error=str(e))

    if room_id:
        # Also store for room context
        contributed_by = agent_name if isinstance(agent_name, str) else f"team:{','.join(agent_name)}"
        room_metadata = {
            "type": "conversation",
            "session_id": session_id,
            "room_id": room_id,
            "contributed_by": contributed_by,
        }

        safe_room_id = room_id.replace(":", "_").replace("!", "")
        try:
            await memory.add(messages, user_id=f"room_{safe_room_id}", metadata=room_metadata)
            logger.debug("Room memory added", room_id=room_id)
        except Exception as e:
            logger.exception("Failed to add room memory", room_id=room_id, error=str(e))

================================================================================
# FILE: src/mindroom/plugins.py
================================================================================

"""Plugin loader for Mindroom tools and skills."""

from __future__ import annotations

import json
import re
import sys
from dataclasses import dataclass
from importlib import util
from pathlib import Path
from typing import TYPE_CHECKING

from .constants import DEFAULT_AGENTS_CONFIG
from .logging_config import get_logger
from .skills import set_plugin_skill_roots

if TYPE_CHECKING:
    from .config import Config

logger = get_logger(__name__)

PLUGIN_MANIFEST = "mindroom.plugin.json"
REPO_ROOT = Path(__file__).resolve().parents[2]


@dataclass(frozen=True)
class PluginManifest:
    """Validated plugin manifest data."""

    name: str
    tools_module: str | None
    skills: list[str]


@dataclass(frozen=True)
class Plugin:
    """Loaded plugin details."""

    name: str
    root: Path
    manifest_path: Path
    tools_module_path: Path | None
    skill_dirs: list[Path]


@dataclass
class _PluginCacheEntry:
    manifest_mtime: float
    plugin: Plugin


_PLUGIN_CACHE: dict[Path, _PluginCacheEntry] = {}
_TOOL_MODULE_CACHE: dict[Path, float] = {}


def load_plugins(config: Config, *, config_path: Path | None = None) -> list[Plugin]:
    """Load plugins from config and register their tools and skills."""
    plugin_paths = getattr(config, "plugins", None)
    if not plugin_paths:
        set_plugin_skill_roots([])
        return []

    plugins: list[Plugin] = []
    skill_roots: list[Path] = []

    for plugin_path in plugin_paths:
        root = _resolve_plugin_root(plugin_path, config_path)
        plugin = _load_plugin(root)
        if plugin is None:
            continue
        plugins.append(plugin)
        skill_roots.extend(plugin.skill_dirs)

    if plugins:
        logger.info("Loaded plugins", plugins=[plugin.name for plugin in plugins])

    set_plugin_skill_roots(skill_roots)
    return plugins


def _resolve_plugin_root(plugin_path: str, config_path: Path | None) -> Path:
    path = Path(plugin_path).expanduser()
    if path.is_absolute():
        return path

    base = (config_path or DEFAULT_AGENTS_CONFIG).expanduser().resolve()
    relative = (base.parent / path).resolve()
    if relative.exists():
        return relative

    module_root = _resolve_python_plugin_root(plugin_path)
    if module_root is not None:
        return module_root

    return relative


def _resolve_python_plugin_root(plugin_path: str) -> Path | None:
    parsed = _parse_python_plugin_spec(plugin_path)
    if parsed is None:
        return None

    module_name, subpath, explicit = parsed
    spec = util.find_spec(module_name)
    if spec is None:
        if explicit:
            logger.warning("Plugin module not found", module=module_name, spec=plugin_path)
        return None

    if spec.submodule_search_locations:
        root = Path(next(iter(spec.submodule_search_locations)))
    elif spec.origin:
        root = Path(spec.origin).parent
    else:
        if explicit:
            logger.warning("Plugin module has no filesystem location", module=module_name)
        return None

    resolved_root = (root / subpath).resolve() if subpath else root.resolve()
    if not resolved_root.exists() or not resolved_root.is_dir():
        if explicit:
            logger.warning("Plugin module path is not a directory", module=module_name, path=str(resolved_root))
        return None

    return resolved_root


def _parse_python_plugin_spec(plugin_path: str) -> tuple[str, str | None, bool] | None:
    prefixes = ("python:", "pkg:", "module:")
    for prefix in prefixes:
        if plugin_path.startswith(prefix):
            spec = plugin_path[len(prefix) :]
            explicit = True
            break
    else:
        spec = plugin_path
        explicit = False
        if "/" in spec or spec.startswith("."):
            return None

    parts = spec.split(":", 1)
    module_name = parts[0].strip()
    if not module_name:
        return None
    subpath = parts[1].strip() if len(parts) > 1 else None
    if subpath == "":
        subpath = None
    return module_name, subpath, explicit


def _load_plugin(root: Path) -> Plugin | None:
    if not root.exists() or not root.is_dir():
        logger.warning("Plugin path does not exist", path=str(root))
        return None

    manifest_path = root / PLUGIN_MANIFEST
    if not manifest_path.exists():
        logger.warning("Plugin manifest missing", path=str(manifest_path))
        return None

    if not root.is_relative_to(REPO_ROOT):
        logger.warning("Loading non-bundled plugin", path=str(root))

    try:
        manifest_mtime = manifest_path.stat().st_mtime
    except OSError as exc:
        logger.warning("Failed to stat plugin manifest", path=str(manifest_path), error=str(exc))
        return None

    cached = _PLUGIN_CACHE.get(manifest_path)
    if cached and cached.manifest_mtime == manifest_mtime:
        _load_tools_module(cached.plugin)
        return cached.plugin

    manifest = _parse_manifest(manifest_path)
    if manifest is None:
        return None

    tools_module_path = _resolve_tools_module(root, manifest.tools_module)
    skill_dirs = _resolve_skill_dirs(root, manifest.skills)

    plugin = Plugin(
        name=manifest.name,
        root=root,
        manifest_path=manifest_path,
        tools_module_path=tools_module_path,
        skill_dirs=skill_dirs,
    )

    _PLUGIN_CACHE[manifest_path] = _PluginCacheEntry(manifest_mtime=manifest_mtime, plugin=plugin)

    _load_tools_module(plugin)
    return plugin


def _parse_manifest(path: Path) -> PluginManifest | None:
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except (OSError, json.JSONDecodeError) as exc:
        logger.warning("Failed to parse plugin manifest", path=str(path), error=str(exc))
        return None

    if not isinstance(data, dict):
        logger.warning("Plugin manifest must be a JSON object", path=str(path))
        return None

    name = data.get("name")
    if not isinstance(name, str) or not name.strip():
        logger.warning("Plugin manifest missing name", path=str(path))
        return None

    tools_module = data.get("tools_module")
    if tools_module is not None and not isinstance(tools_module, str):
        logger.warning("Plugin tools_module must be a string", path=str(path))
        return None

    raw_skills = data.get("skills", [])
    if raw_skills is None:
        raw_skills = []
    if not isinstance(raw_skills, list) or any(not isinstance(item, str) for item in raw_skills):
        logger.warning("Plugin skills must be a list of strings", path=str(path))
        return None

    return PluginManifest(name=name.strip(), tools_module=tools_module, skills=raw_skills)


def _resolve_tools_module(root: Path, tools_module: str | None) -> Path | None:
    if not tools_module:
        return None
    module_path = (root / tools_module).resolve()
    if not module_path.exists():
        logger.warning("Plugin tools module not found", path=str(module_path))
        return None
    return module_path


def _resolve_skill_dirs(root: Path, skills: list[str]) -> list[Path]:
    skill_dirs: list[Path] = []
    for relative_path in skills:
        path = (root / relative_path).resolve()
        if not path.exists() or not path.is_dir():
            logger.warning("Plugin skill path is not a directory", path=str(path))
            continue
        skill_dirs.append(path)
    return skill_dirs


def _load_tools_module(plugin: Plugin) -> None:
    if plugin.tools_module_path is None:
        return

    module_path = plugin.tools_module_path
    try:
        mtime = module_path.stat().st_mtime
    except OSError as exc:
        logger.warning("Failed to stat plugin tools module", path=str(module_path), error=str(exc))
        return

    cached_mtime = _TOOL_MODULE_CACHE.get(module_path)
    if cached_mtime == mtime:
        return

    module_name = _module_name(plugin.name, module_path)
    spec = util.spec_from_file_location(module_name, module_path)
    if spec is None or spec.loader is None:
        logger.warning("Failed to load plugin tools module", path=str(module_path))
        return

    module = util.module_from_spec(spec)
    sys.modules[module_name] = module
    try:
        spec.loader.exec_module(module)
    except Exception as exc:
        logger.warning("Plugin tools module execution failed", path=str(module_path), error=str(exc))
        return

    _TOOL_MODULE_CACHE[module_path] = mtime


def _module_name(plugin_name: str, module_path: Path) -> str:
    slug = re.sub(r"[^a-zA-Z0-9_]+", "_", plugin_name).strip("_") or "plugin"
    digest = abs(hash(str(module_path)))
    return f"mindroom_plugin_{slug}_{digest}"

================================================================================
# FILE: src/mindroom/response_tracker.py
================================================================================

"""Track which messages have been responded to by agents."""

from __future__ import annotations

import fcntl
import json
import time
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, TypedDict

from .constants import TRACKING_DIR
from .logging_config import get_logger

if TYPE_CHECKING:
    from pathlib import Path

logger = get_logger(__name__)


class ResponseRecord(TypedDict):
    """Record of a response to a user message."""

    timestamp: float
    response_id: str | None


@dataclass
class ResponseTracker:
    """Track which event IDs have been responded to by an agent."""

    agent_name: str
    base_path: Path = TRACKING_DIR
    _responses: dict[str, ResponseRecord] = field(default_factory=dict, init=False)
    _responses_file: Path = field(init=False)

    def __post_init__(self) -> None:
        """Initialize paths and load existing responses."""
        self.base_path.mkdir(parents=True, exist_ok=True)
        self._responses_file = self.base_path / f"{self.agent_name}_responded.json"
        self._load_responses()
        # Perform automatic cleanup on initialization
        self._cleanup_old_events()

    def has_responded(self, event_id: str) -> bool:
        """Check if we've already responded to this event.

        Args:
            event_id: The Matrix event ID

        Returns:
            True if we've already responded to this event

        """
        return event_id in self._responses

    def mark_responded(self, event_id: str, response_event_id: str | None = None) -> None:
        """Mark an event as responded to with current timestamp.

        Args:
            event_id: The Matrix event ID we responded to
            response_event_id: The event ID of our response message (optional)

        """
        self._responses[event_id] = {
            "timestamp": time.time(),
            "response_id": response_event_id,
        }
        self._save_responses()
        logger.debug(f"Marked event {event_id} as responded for agent {self.agent_name}")

    def get_response_event_id(self, user_event_id: str) -> str | None:
        """Get the response event ID for a given user message event ID.

        Args:
            user_event_id: The user's message event ID

        Returns:
            The agent's response event ID if it exists, None otherwise

        """
        record = self._responses.get(user_event_id)
        return record["response_id"] if record else None

    def _load_responses(self) -> None:
        """Load the responses from disk."""
        if not self._responses_file.exists():
            self._responses = {}
            return

        with self._responses_file.open() as f:
            data = json.load(f)
            self._responses = data

    def _save_responses(self) -> None:
        """Save the responses to disk using file locking."""
        with self._responses_file.open("w") as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                json.dump(self._responses, f, indent=2)
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

    def _cleanup_old_events(self, max_events: int = 10000, max_age_days: int = 30) -> None:
        """Remove old events based on count and age.

        Args:
            max_events: Maximum number of events to track
            max_age_days: Maximum age of events in days

        """
        current_time = time.time()
        max_age_seconds = max_age_days * 24 * 60 * 60

        # First remove events older than max_age_days
        self._responses = {
            event_id: record
            for event_id, record in self._responses.items()
            if current_time - record["timestamp"] < max_age_seconds
        }

        # Then trim to max_events if still over limit
        if len(self._responses) > max_events:
            # Sort by timestamp and keep only the most recent ones
            sorted_events = sorted(self._responses.items(), key=lambda x: x[1]["timestamp"])
            self._responses = dict(sorted_events[-max_events:])

        self._save_responses()
        logger.info(f"Cleaned up old events for {self.agent_name}, keeping {len(self._responses)} events")

================================================================================
# FILE: src/mindroom/room_cleanup.py
================================================================================

"""Room cleanup utilities for removing stale bot memberships from Matrix rooms.

With the new self-managing agent pattern, agents handle their own room
memberships. This module only handles cleanup of stale/orphaned bots.

DM rooms are preserved and not cleaned up.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import nio

from .logging_config import get_logger
from .matrix.client import get_joined_rooms, get_room_members
from .matrix.identity import MatrixID
from .matrix.rooms import is_dm_room
from .matrix.state import MatrixState

if TYPE_CHECKING:
    from .config import Config

logger = get_logger(__name__)


def _get_all_known_bot_usernames() -> set[str]:
    """Get all bot usernames that have ever been created (from matrix_state.yaml).

    Returns:
        Set of all known bot usernames

    """
    state = MatrixState.load()
    bot_usernames = set()

    # Get all agent accounts from state
    for key in state.accounts:
        # Skip the user account (agent_user is the human user, not a bot)
        if key.startswith("agent_") and key != "agent_user":
            account = state.accounts[key]
            bot_usernames.add(account.username)

    return bot_usernames


async def _cleanup_orphaned_bots_in_room(
    client: nio.AsyncClient,
    room_id: str,
    config: Config,
) -> list[str]:
    """Remove orphaned bots from a single room.

    When DM mode is enabled, actual DM rooms are skipped to preserve them.

    Args:
        client: An authenticated Matrix client with kick permissions
        room_id: The room to check
        config: Current configuration

    Returns:
        List of bot usernames that were kicked

    """
    # When DM mode is enabled, check if this is actually a DM room
    if await is_dm_room(client, room_id):
        logger.debug(f"Skipping DM room {room_id} cleanup")
        return []

    # Get room members
    member_ids = await get_room_members(client, room_id)
    if not member_ids:
        logger.warning(f"No members found or failed to get members for room {room_id}")
        return []

    # Get configured bots for this room
    configured_bots = config.get_configured_bots_for_room(room_id)
    known_bot_usernames = _get_all_known_bot_usernames()

    kicked_bots = []

    for user_id in member_ids:
        matrix_id = MatrixID.parse(user_id)

        # Check if this is a mindroom bot and shouldn't be in this room
        if matrix_id.username in known_bot_usernames and matrix_id.username not in configured_bots:
            logger.info(
                f"Found orphaned bot {matrix_id.username} in room {room_id} "
                f"(configured bots for this room: {configured_bots})",
            )

            # Kick the bot
            kick_response = await client.room_kick(room_id, user_id, reason="Bot no longer configured for this room")

            if isinstance(kick_response, nio.RoomKickResponse):
                logger.info(f"Kicked {matrix_id.username} from {room_id}")
                kicked_bots.append(matrix_id.username)
            else:
                logger.error(f"Failed to kick {matrix_id.username} from {room_id}: {kick_response}")

    return kicked_bots


async def cleanup_all_orphaned_bots(
    client: nio.AsyncClient,
    config: Config,
) -> dict[str, list[str]]:
    """Remove all orphaned bots from all rooms the client has access to.

    This should be called by a user or bot with admin/moderator permissions
    in the rooms that need cleaning.

    Returns:
        Dictionary mapping room IDs to lists of kicked bot usernames

    """
    # Track what we're doing
    kicked_bots: dict[str, list[str]] = {}

    # Get all rooms the client is in
    joined_rooms = await get_joined_rooms(client)
    if joined_rooms is None:
        return kicked_bots

    logger.info(f"Checking {len(joined_rooms)} rooms for orphaned bots")

    for room_id in joined_rooms:
        room_kicked = await _cleanup_orphaned_bots_in_room(client, room_id, config)
        if room_kicked:
            kicked_bots[room_id] = room_kicked

    # Summary
    total_kicked = sum(len(bots) for bots in kicked_bots.values())
    if total_kicked > 0:
        logger.info(f"Kicked {total_kicked} orphaned bots from {len(kicked_bots)} rooms")
    else:
        logger.info("No orphaned bots found in any room")

    return kicked_bots

================================================================================
# FILE: src/mindroom/routing.py
================================================================================

"""Simple AI routing for multi-agent threads."""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

from agno.agent import Agent
from pydantic import BaseModel, Field

from .agents import describe_agent
from .ai import get_model_instance
from .logging_config import get_logger
from .matrix.identity import MatrixID

if TYPE_CHECKING:
    from .config import Config

logger = get_logger(__name__)


class AgentSuggestion(BaseModel):
    """Structured output for agent routing decisions."""

    agent_name: str = Field(description="The name of the agent that should respond")
    reasoning: str = Field(description="Brief explanation of why this agent was chosen")


async def suggest_agent_for_message(
    message: str,
    available_agents: list[MatrixID],
    config: Config,
    thread_context: list[dict[str, Any]] | None = None,
) -> str | None:
    """Use AI to suggest which agent should respond to a message."""
    try:
        # The available_agents list already contains only configured agents
        agent_names = [mid.agent_name(config) for mid in available_agents]

        # Build agent descriptions
        agent_descriptions = []
        for agent_name in agent_names:
            assert agent_name is not None
            description = describe_agent(agent_name, config)
            agent_descriptions.append(f"{agent_name}:\n  {description}")

        agents_info = "\n\n".join(agent_descriptions)

        prompt = f"""Decide which agent should respond to this message.

Available agents and their capabilities:

{agents_info}

Message: "{message}"

Choose the most appropriate agent based on their role, tools, and instructions."""

        if thread_context:
            context = "Previous messages:\n"
            for msg in thread_context[-3:]:  # Last 3 messages
                sender = msg.get("sender", "")
                # For display, just show the username or domain
                if sender.startswith("@") and ":" in sender:
                    sender_id = MatrixID.parse(sender)
                    # Show agent name or just domain for users
                    sender = sender_id.agent_name(config) or sender_id.domain
                body = msg.get("body", "")[:100]
                context += f"{sender}: {body}\n"
            prompt = context + "\n" + prompt

        # Get router model from config
        router_model_name = config.router.model

        model = get_model_instance(config, router_model_name)
        logger.info(f"Using router model: {router_model_name} -> {model.__class__.__name__}(id={model.id})")

        agent = Agent(
            name="Router",
            role="Route messages to appropriate agents",
            model=model,
            output_schema=AgentSuggestion,
        )

        response = await agent.arun(prompt, session_id="routing")
        suggestion = response.content

        # With output_schema, we should always get the correct type
        if not isinstance(suggestion, AgentSuggestion):
            logger.error(
                "Unexpected response type from AI routing",
                expected="AgentSuggestion",
                actual=type(suggestion).__name__,
            )
            return None

        # The AI should only suggest agents from the available list
        if suggestion.agent_name not in agent_names:
            logger.warning("AI suggested invalid agent", suggested=suggestion.agent_name, available=agent_names)
            return None

        logger.info("Routing decision", agent=suggestion.agent_name, reason=suggestion.reasoning)
    except Exception as e:
        # Log error and return None - the router will fall back to not routing
        logger.exception("Routing failed", error=str(e))
        return None
    else:
        return suggestion.agent_name

================================================================================
# FILE: src/mindroom/scheduling.py
================================================================================

"""Scheduled task management with AI-powered workflow scheduling."""

from __future__ import annotations

import asyncio
import json
import uuid
from datetime import UTC, datetime, timedelta
from typing import TYPE_CHECKING, Literal, NamedTuple
from zoneinfo import ZoneInfo

import humanize
import nio
from agno.agent import Agent
from cron_descriptor import get_description  # type: ignore[import-untyped]
from croniter import croniter  # type: ignore[import-untyped]
from pydantic import BaseModel, Field

from .ai import get_model_instance
from .logging_config import get_logger
from .matrix.client import (
    fetch_thread_history,
    get_latest_thread_event_id_if_needed,
    send_message,
)
from .matrix.identity import MatrixID
from .matrix.mentions import format_message_with_mentions, parse_mentions_in_text
from .matrix.message_builder import build_message_content
from .thread_utils import get_agents_in_thread, get_available_agents_in_room

if TYPE_CHECKING:
    from .config import Config

logger = get_logger(__name__)

# Event type for scheduled tasks in Matrix state
SCHEDULED_TASK_EVENT_TYPE = "com.mindroom.scheduled.task"

# Maximum length for message preview in task listings
MESSAGE_PREVIEW_LENGTH = 50

# Global task storage for running asyncio tasks
_running_tasks: dict[str, asyncio.Task] = {}


class _AgentValidationResult(NamedTuple):
    """Result of agent mention validation."""

    all_valid: bool
    valid_agents: list[MatrixID]
    invalid_agents: list[MatrixID]


# ---- Workflow scheduling primitives ----


class CronSchedule(BaseModel):
    """Standard cron-like schedule definition."""

    minute: str = Field(default="*", description="0-59, *, */5, or comma-separated")
    hour: str = Field(default="*", description="0-23, *, */2, or comma-separated")
    day: str = Field(default="*", description="1-31, *, or comma-separated")
    month: str = Field(default="*", description="1-12, *, or comma-separated")
    weekday: str = Field(default="*", description="0-6 (0=Sunday), *, or comma-separated")

    def to_cron_string(self) -> str:
        """Convert to standard cron format."""
        return f"{self.minute} {self.hour} {self.day} {self.month} {self.weekday}"

    def to_natural_language(self) -> str:
        """Convert cron schedule to natural language description."""
        try:
            cron_str = self.to_cron_string()
            return str(get_description(cron_str))
        except Exception:
            return f"Cron: {self.to_cron_string()}"


class ScheduledWorkflow(BaseModel):
    """Structured representation of a scheduled task or workflow."""

    schedule_type: Literal["once", "cron"]
    execute_at: datetime | None = None
    cron_schedule: CronSchedule | None = None
    message: str
    description: str
    created_by: str | None = None
    thread_id: str | None = None
    room_id: str | None = None


class WorkflowParseError(BaseModel):
    """Error response when workflow parsing fails."""

    error: str
    suggestion: str | None = None


async def parse_workflow_schedule(
    request: str,
    config: Config,
    available_agents: list[MatrixID],
    current_time: datetime | None = None,
) -> ScheduledWorkflow | WorkflowParseError:
    """Parse natural language into structured workflow using AI."""
    if current_time is None:
        current_time = datetime.now(UTC)

    assert available_agents, "No agents available for scheduling"
    agent_list = ", ".join(f"@{name}" for name in available_agents)

    prompt = f"""Parse this scheduling request into a structured workflow.

Current time (UTC): {current_time.isoformat()}Z
Request: "{request}"

Your task is to:
1. Determine if this is a one-time task or recurring (cron)
2. Extract the schedule/timing
3. Create a message that mentions the appropriate agents

Available agents: {agent_list}

IMPORTANT: Event-based and conditional requests:
When users say "if", "when", "whenever", "once X happens" or describe events/conditions:
1. Convert to an appropriate recurring (cron) schedule for polling
2. Include BOTH the condition check AND the action in the message
3. Choose polling frequency based on urgency and type

Important rules:
- For conditional/event-based requests, ALWAYS include the check condition in the message
- Mention relevant agents with @ only when needed
- Convert time expressions to UTC for the schedule, but DO NOT include them in the message
- Remove time phrases like "in 15 seconds" from the message itself
- If schedule_type is "once", you MUST provide execute_at
- If schedule_type is "cron", you MUST provide cron_schedule

Examples of event/condition phrasing to include in the message (do not include times in these examples):
- @email_assistant Check for emails containing 'urgent'. If found, @phone_agent notify the user.
- @crypto_agent Check Bitcoin price. If below $40,000, @notification_agent alert the user.
- @monitoring_agent Check server CPU usage. If above 80%, @ops_agent scale up the servers.
- @reddit_agent Check for new mentions of our product. If found, @analyst analyze the sentiment and key points.
"""

    model = get_model_instance(config, "default")

    agent = Agent(
        name="WorkflowParser",
        role="Parse scheduling requests into structured workflows",
        model=model,
        output_schema=ScheduledWorkflow,
    )

    try:
        response = await agent.arun(prompt, session_id=f"workflow_parse_{uuid.uuid4()}")
        result = response.content

        if isinstance(result, ScheduledWorkflow):
            if result.schedule_type == "once" and not result.execute_at:
                # Match previous behavior: default to 30 minutes from now
                result.execute_at = current_time + timedelta(minutes=30)
            elif result.schedule_type == "cron" and not result.cron_schedule:
                result.cron_schedule = CronSchedule(minute="0", hour="9", day="*", month="*", weekday="*")

            logger.info("Successfully parsed workflow schedule", request=request, schedule_type=result.schedule_type)
            return result

        logger.error("Unexpected response type from AI", response_type=type(result).__name__)
        return WorkflowParseError(
            error="Failed to parse the schedule request",
            suggestion="Try being more specific about the timing and what you want to happen",
        )

    except Exception as e:
        logger.exception("Error parsing workflow schedule", error=str(e), request=request)
        return WorkflowParseError(
            error=f"Error parsing schedule: {e!s}",
            suggestion="Try a simpler format like 'Daily at 9am, check my email'",
        )


async def execute_scheduled_workflow(
    client: nio.AsyncClient,
    workflow: ScheduledWorkflow,
    config: Config,
) -> None:
    """Execute a scheduled workflow by posting its message to the thread."""
    if not workflow.room_id:
        logger.error("Cannot execute workflow without room_id")
        return

    try:
        automated_message = (
            f"â° [Automated Task]\n{workflow.message}\n\n_Note: Automated task - no follow-up expected._"
        )
        latest_thread_event_id = await get_latest_thread_event_id_if_needed(
            client,
            workflow.room_id,
            workflow.thread_id,
        )
        content = format_message_with_mentions(
            config,
            automated_message,
            sender_domain=config.domain,
            thread_event_id=workflow.thread_id,
            latest_thread_event_id=latest_thread_event_id,
        )
        await send_message(client, workflow.room_id, content)
        logger.info("Executed scheduled workflow", description=workflow.description, thread_id=workflow.thread_id)
    except Exception as e:
        logger.exception("Failed to execute scheduled workflow")
        if workflow.room_id:
            error_message = f"âŒ Scheduled task failed: {workflow.description}\nError: {e!s}"
            error_content = build_message_content(
                body=error_message,
                thread_event_id=workflow.thread_id,
                latest_thread_event_id=workflow.thread_id,
            )
            await send_message(client, workflow.room_id, error_content)


async def run_cron_task(
    client: nio.AsyncClient,
    task_id: str,
    workflow: ScheduledWorkflow,
    running_tasks: dict[str, asyncio.Task],
    config: Config,
) -> None:
    """Run a recurring task based on cron schedule."""
    if not workflow.cron_schedule:
        logger.error("No cron schedule provided for recurring task")
        return

    cron_string = workflow.cron_schedule.to_cron_string()

    try:
        cron = croniter(cron_string, datetime.now(UTC))
        while True:
            next_run = cron.get_next(datetime)
            delay = (next_run - datetime.now(UTC)).total_seconds()
            if delay > 0:
                logger.info(
                    f"Waiting {delay:.0f} seconds until next execution",
                    task_id=task_id,
                    next_run=next_run.isoformat(),
                )
                await asyncio.sleep(delay)
            await execute_scheduled_workflow(client, workflow, config)
            if task_id not in running_tasks:
                logger.info(f"Task {task_id} no longer in running tasks, stopping")
                break
    except asyncio.CancelledError:
        logger.info(f"Cron task {task_id} was cancelled")
        raise
    except Exception as e:
        logger.exception(f"Error in cron task {task_id}")
        if workflow.room_id:
            error_message = f"âŒ Recurring task failed: {workflow.description}\nTask ID: {task_id}\nError: {e!s}"
            error_content = build_message_content(
                body=error_message,
                thread_event_id=workflow.thread_id,
                latest_thread_event_id=workflow.thread_id,
            )
            await send_message(client, workflow.room_id, error_content)


async def run_once_task(
    client: nio.AsyncClient,
    task_id: str,
    workflow: ScheduledWorkflow,
    config: Config,
) -> None:
    """Run a one-time scheduled task."""
    if not workflow.execute_at:
        logger.error("No execution time provided for one-time task")
        return

    try:
        delay = (workflow.execute_at - datetime.now(UTC)).total_seconds()
        if delay > 0:
            logger.info(
                f"Waiting {delay:.0f} seconds until execution",
                task_id=task_id,
                execute_at=workflow.execute_at.isoformat(),
            )
            await asyncio.sleep(delay)
        await execute_scheduled_workflow(client, workflow, config)
    except asyncio.CancelledError:
        logger.info(f"One-time task {task_id} was cancelled")
        raise
    except Exception as e:
        logger.exception(f"Error in one-time task {task_id}")
        if workflow.room_id:
            error_message = f"âŒ One-time task failed: {workflow.description}\nTask ID: {task_id}\nError: {e!s}"
            error_content = build_message_content(
                body=error_message,
                thread_event_id=workflow.thread_id,
                latest_thread_event_id=workflow.thread_id,
            )
            await send_message(client, workflow.room_id, error_content)


async def _validate_agent_mentions(
    message: str,
    room: nio.MatrixRoom,
    thread_id: str | None,
    config: Config,
) -> _AgentValidationResult:
    """Validate that all mentioned agents are accessible.

    Args:
        message: The message that may contain @agent mentions
        room: The Matrix room object
        thread_id: The thread ID where the schedule will execute (if in a thread)
        config: Application configuration

    Returns:
        _AgentValidationResult with validation status and agent lists

    """
    # Parse mentions - this handles all the agent name resolution properly
    _, mentioned_user_ids, _ = parse_mentions_in_text(message, config.domain, config)

    if not mentioned_user_ids:
        # No agents mentioned, validation passes
        return _AgentValidationResult(all_valid=True, valid_agents=[], invalid_agents=[])

    # Extract agent names from the mentioned user IDs

    mentioned_agents: list[MatrixID] = []
    for user_id in mentioned_user_ids:
        mid = MatrixID.parse(user_id)
        agent_name = mid.agent_name(config)
        if agent_name and mid not in mentioned_agents:
            mentioned_agents.append(mid)

    if not mentioned_agents:
        # No valid agents mentioned
        return _AgentValidationResult(all_valid=True, valid_agents=[], invalid_agents=[])

    valid_agents: list[MatrixID] = []
    invalid_agents: list[MatrixID] = []

    if thread_id:
        # For threads, check if agents are in the room
        room_agents = get_available_agents_in_room(room, config)

        # Agents can now respond in any room they're in
        for mid in mentioned_agents:
            if mid in room_agents:
                valid_agents.append(mid)
            else:
                invalid_agents.append(mid)
    else:
        # For room messages, check if agents are configured for the room
        room_agents = get_available_agents_in_room(room, config)
        for mid in mentioned_agents:
            if mid in room_agents:
                valid_agents.append(mid)
            else:
                invalid_agents.append(mid)

    all_valid = len(invalid_agents) == 0
    return _AgentValidationResult(
        all_valid=all_valid,
        valid_agents=valid_agents,
        invalid_agents=invalid_agents,
    )


def _format_scheduled_time(dt: datetime, timezone_str: str) -> str:
    """Format a datetime with timezone and relative time delta.

    Args:
        dt: Datetime in UTC
        timezone_str: Timezone string (e.g., 'America/New_York')

    Returns:
        Formatted string like "2024-01-15 3:30 PM EST (in 2 hours)"

    """
    # Convert UTC to target timezone
    tz = ZoneInfo(timezone_str)
    local_dt = dt.astimezone(tz)

    # Get human-readable relative time using humanize
    now = datetime.now(UTC)
    relative_str = humanize.naturaltime(dt, when=now)

    # Format the datetime string with 24-hour time
    time_str = local_dt.strftime("%Y-%m-%d %H:%M %Z")
    return f"{time_str} ({relative_str})"


async def schedule_task(  # noqa: C901, PLR0912, PLR0915
    client: nio.AsyncClient,
    room_id: str,
    thread_id: str | None,
    scheduled_by: str,
    full_text: str,
    config: Config,
    room: nio.MatrixRoom,
    mentioned_agents: list[MatrixID] | None = None,
) -> tuple[str | None, str]:
    """Schedule a workflow from natural language request.

    Returns:
        Tuple of (task_id, response_message)

    """
    # Get agents that are available in the thread
    available_agents: list[MatrixID] = []
    if thread_id:
        # Get agents already participating in the thread
        thread_history = await fetch_thread_history(client, room_id, thread_id)
        available_agents = get_agents_in_thread(thread_history, config)

    # Add any agents mentioned in the command itself
    if mentioned_agents:
        for mid in mentioned_agents:
            if mid not in available_agents:
                available_agents.append(mid)

    # If no agents found in thread or mentions, fall back to agents in the room
    if not available_agents:
        available_agents = get_available_agents_in_room(room, config)

    # Parse the workflow request with available agents
    workflow_result = await parse_workflow_schedule(full_text, config, available_agents)

    if isinstance(workflow_result, WorkflowParseError):
        error_msg = f"âŒ {workflow_result.error}"
        if workflow_result.suggestion:
            error_msg += f"\n\nðŸ’¡ {workflow_result.suggestion}"
        return (None, error_msg)

    # Handle workflow task
    # Validate workflow before proceeding
    if workflow_result.schedule_type == "once" and not workflow_result.execute_at:
        return (None, "âŒ Failed to schedule: One-time task missing execution time")
    if workflow_result.schedule_type == "cron" and not workflow_result.cron_schedule:
        return (None, "âŒ Failed to schedule: Recurring task missing cron schedule")

    # Validate that all mentioned agents are accessible
    validation_result = await _validate_agent_mentions(workflow_result.message, room, thread_id, config)

    if not validation_result.all_valid:
        error_msg = "âŒ Failed to schedule: The following agents are not available in this "
        if thread_id:
            error_msg += "thread"
        else:
            error_msg += "room"
        error_msg += f": {', '.join(agent.full_id for agent in validation_result.invalid_agents)}"

        # Provide helpful suggestions
        suggestions: list[str] = []
        for agent in validation_result.invalid_agents:
            agent_name = agent.agent_name(config)
            if agent_name:
                # Agent exists but not available in this room/thread
                suggestions.append(f"{agent.full_id} is not available in this {'thread' if thread_id else 'room'}")
            else:
                suggestions.append(f"{agent.full_id} does not exist")

        if suggestions:
            error_msg += "\n\nðŸ’¡ " + "\nðŸ’¡ ".join(suggestions)

        return (None, error_msg)

    # Add metadata to workflow
    workflow_result.created_by = scheduled_by
    workflow_result.thread_id = thread_id
    workflow_result.room_id = room_id

    # Create task ID
    task_id = str(uuid.uuid4())[:8]

    # Store workflow in Matrix state
    task_data = {
        "task_id": task_id,
        "workflow": workflow_result.model_dump_json(),
        "status": "pending",
        "created_at": datetime.now(UTC).isoformat(),
    }

    logger.info(
        "Storing workflow task in Matrix state",
        task_id=task_id,
        room_id=room_id,
        thread_id=thread_id,
        schedule_type=workflow_result.schedule_type,
    )

    await client.room_put_state(
        room_id=room_id,
        event_type=SCHEDULED_TASK_EVENT_TYPE,
        content=task_data,
        state_key=task_id,
    )

    # Start the appropriate async task
    if workflow_result.schedule_type == "once":
        task = asyncio.create_task(
            run_once_task(client, task_id, workflow_result, config),
        )
    else:  # cron
        task = asyncio.create_task(
            run_cron_task(client, task_id, workflow_result, _running_tasks, config),
        )

    _running_tasks[task_id] = task

    # Build success message
    if workflow_result.schedule_type == "once" and workflow_result.execute_at:
        # Format time with timezone and relative delta
        formatted_time = _format_scheduled_time(workflow_result.execute_at, config.timezone)
        success_msg = f"âœ… Scheduled for {formatted_time}\n"
    elif workflow_result.cron_schedule:
        # Show both natural language and cron syntax
        natural_desc = workflow_result.cron_schedule.to_natural_language()
        cron_str = workflow_result.cron_schedule.to_cron_string()
        success_msg = f"âœ… Scheduled recurring task: **{natural_desc}**\n"
        success_msg += f"   _(Cron: `{cron_str}`)_\n"
    else:
        success_msg = "âœ… Task scheduled\n"

    success_msg += f"\n**Task:** {workflow_result.description}\n"
    success_msg += f"**Will post:** {workflow_result.message}\n"
    success_msg += f"\n**Task ID:** `{task_id}`"

    return (task_id, success_msg)


async def list_scheduled_tasks(  # noqa: C901, PLR0912
    client: nio.AsyncClient,
    room_id: str,
    thread_id: str | None = None,
    config: Config | None = None,
) -> str:
    """List scheduled tasks in human-readable format."""
    response = await client.room_get_state(room_id)

    if not isinstance(response, nio.RoomGetStateResponse):
        logger.error("Failed to get room state", response=str(response), room_id=room_id, thread_id=thread_id)
        return "Unable to retrieve scheduled tasks."

    tasks = []
    tasks_in_other_threads = []

    for event in response.events:
        if event["type"] == SCHEDULED_TASK_EVENT_TYPE:
            content = event["content"]
            if content.get("status") == "pending":
                try:
                    # Parse the workflow
                    workflow_data = json.loads(content["workflow"])
                    workflow = ScheduledWorkflow(**workflow_data)

                    # Determine display time
                    if workflow.schedule_type == "once" and workflow.execute_at:
                        display_time = workflow.execute_at
                        schedule_type = "once"
                    else:
                        # For cron, show the natural language description
                        display_time = None
                        if workflow.cron_schedule:
                            schedule_type = workflow.cron_schedule.to_natural_language()
                        else:
                            schedule_type = "recurring"

                    task_info = {
                        "id": event["state_key"],
                        "time": display_time,
                        "schedule_type": schedule_type,
                        "description": workflow.description,
                        "message": workflow.message,
                        "thread_id": workflow.thread_id,
                    }

                    # Separate tasks by thread
                    if thread_id and workflow.thread_id and workflow.thread_id != thread_id:
                        tasks_in_other_threads.append(task_info)
                    else:
                        tasks.append(task_info)
                except (KeyError, ValueError, json.JSONDecodeError):
                    logger.exception("Failed to parse task")
                    continue

    if not tasks and not tasks_in_other_threads:
        return "No scheduled tasks found."

    if not tasks and tasks_in_other_threads:
        return f"No scheduled tasks in this thread.\n\nðŸ“Œ {len(tasks_in_other_threads)} task(s) scheduled in other threads. Use !list_schedules in those threads to see details."

    # Sort by execution time (one-time tasks) or put recurring tasks at the end
    tasks.sort(key=lambda t: (t["time"] is None, t["time"] or datetime.max.replace(tzinfo=UTC)))

    lines = ["**Scheduled Tasks:**"]
    for task in tasks:
        if task["schedule_type"] == "once" and task["time"]:
            # Get timezone from config or use UTC as fallback
            timezone = config.timezone if config else "UTC"
            time_str = _format_scheduled_time(task["time"], timezone)
        else:
            # For recurring tasks, schedule_type now contains the natural language description
            time_str = task["schedule_type"]

        msg_preview = task["message"][:MESSAGE_PREVIEW_LENGTH] + (
            "..." if len(task["message"]) > MESSAGE_PREVIEW_LENGTH else ""
        )
        lines.append(f'â€¢ `{task["id"]}` - {time_str}\n  {task["description"]}\n  Message: "{msg_preview}"')

    return "\n".join(lines)


async def cancel_scheduled_task(
    client: nio.AsyncClient,
    room_id: str,
    task_id: str,
) -> str:
    """Cancel a scheduled task."""
    # Cancel the asyncio task if running
    if task_id in _running_tasks:
        _running_tasks[task_id].cancel()
        del _running_tasks[task_id]

    # First check if task exists
    response = await client.room_get_state_event(
        room_id=room_id,
        event_type=SCHEDULED_TASK_EVENT_TYPE,
        state_key=task_id,
    )

    if not isinstance(response, nio.RoomGetStateEventResponse):
        return f"âŒ Task `{task_id}` not found."

    # Update to cancelled
    await client.room_put_state(
        room_id=room_id,
        event_type=SCHEDULED_TASK_EVENT_TYPE,
        content={"status": "cancelled"},
        state_key=task_id,
    )

    return f"âœ… Cancelled task `{task_id}`"


async def cancel_all_scheduled_tasks(
    client: nio.AsyncClient,
    room_id: str,
) -> str:
    """Cancel all scheduled tasks in a room."""
    # Get all scheduled tasks
    response = await client.room_get_state(room_id)

    if not isinstance(response, nio.RoomGetStateResponse):
        logger.error("Failed to get room state", response=str(response))
        return "âŒ Unable to retrieve scheduled tasks."

    cancelled_count = 0
    failed_count = 0

    for event in response.events:
        if event["type"] == SCHEDULED_TASK_EVENT_TYPE:
            content = event["content"]
            if content.get("status") == "pending":
                task_id = event["state_key"]

                # Cancel the asyncio task if running
                if task_id in _running_tasks:
                    _running_tasks[task_id].cancel()
                    del _running_tasks[task_id]

                # Update to cancelled in Matrix state
                try:
                    await client.room_put_state(
                        room_id=room_id,
                        event_type=SCHEDULED_TASK_EVENT_TYPE,
                        content={"status": "cancelled"},
                        state_key=task_id,
                    )
                    cancelled_count += 1
                    logger.info(f"Cancelled task {task_id}")
                except Exception:
                    logger.exception(f"Failed to cancel task {task_id}")
                    failed_count += 1

    if cancelled_count == 0:
        return "No scheduled tasks to cancel."

    result = f"âœ… Cancelled {cancelled_count} scheduled task(s)"
    if failed_count > 0:
        result += f"\nâš ï¸ Failed to cancel {failed_count} task(s)"

    return result


async def restore_scheduled_tasks(client: nio.AsyncClient, room_id: str, config: Config) -> int:  # noqa: C901, PLR0912
    """Restore scheduled tasks from Matrix state after bot restart.

    Returns:
        Number of tasks restored

    """
    response = await client.room_get_state(room_id)
    if not isinstance(response, nio.RoomGetStateResponse):
        return 0

    restored_count = 0
    for event in response.events:
        if event["type"] != SCHEDULED_TASK_EVENT_TYPE:
            continue

        content = event["content"]
        if content.get("status") != "pending":
            continue

        try:
            task_id: str = event["state_key"]

            # Parse the workflow
            workflow_data = json.loads(content["workflow"])
            workflow = ScheduledWorkflow(**workflow_data)

            # Validate workflow has required fields
            if workflow.schedule_type == "once":
                if not workflow.execute_at:
                    logger.warning(f"Skipping one-time task {task_id} without execution time")
                    continue
                # Skip past one-time tasks
                if workflow.execute_at <= datetime.now(UTC):
                    logger.debug(f"Skipping past one-time task {task_id}")
                    continue
            elif workflow.schedule_type == "cron":
                if not workflow.cron_schedule:
                    logger.warning(f"Skipping recurring task {task_id} without cron schedule")
                    continue
            else:
                logger.warning(f"Unknown schedule type for task {task_id}: {workflow.schedule_type}")
                continue

            # Start the appropriate task
            if workflow.schedule_type == "once":
                task = asyncio.create_task(run_once_task(client, task_id, workflow, config))
            else:
                task = asyncio.create_task(run_cron_task(client, task_id, workflow, _running_tasks, config))

            _running_tasks[task_id] = task
            restored_count += 1

        except (KeyError, ValueError, json.JSONDecodeError):
            logger.exception("Failed to restore task")
            continue

    if restored_count > 0:
        logger.info("Restored scheduled tasks in room", room_id=room_id, restored_count=restored_count)

    return restored_count

================================================================================
# FILE: src/mindroom/skills.py
================================================================================

"""Skill integration built on Agno skills with OpenClaw-compatible metadata."""

from __future__ import annotations

import os
import platform
import re
import shutil
from collections.abc import Mapping, Sequence
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING, Any

import json5
import yaml
from agno.skills import LocalSkills, Skills
from agno.skills.loaders import SkillLoader
from agno.skills.skill import Skill

from .credentials import get_credentials_manager
from .logging_config import get_logger

if TYPE_CHECKING:
    from .config import Config

logger = get_logger(__name__)

SKILL_FILENAME = "SKILL.md"
_FRONTMATTER_PATTERN = re.compile(r"^---\s*\n(.*?)\n---\s*\n?(.*)$", re.DOTALL)

_OS_ALIASES = {
    "darwin": {"darwin", "macos", "mac", "osx"},
    "linux": {"linux"},
    "windows": {"windows", "win", "win32"},
}

_PLUGIN_SKILL_ROOTS: list[Path] = []
SkillSnapshot = tuple[tuple[str, int, int], ...]
_SKILL_CACHE: dict[Path, tuple[SkillSnapshot, list[Skill]]] = {}


@dataclass
class MindroomSkillsLoader(SkillLoader):
    """Load skills via Agno with OpenClaw compatibility filtering."""

    roots: Sequence[Path]
    config: Config
    allowlist: Sequence[str] | None = None
    env_vars: Mapping[str, str] | None = None
    credential_keys: set[str] | None = None

    def load(self) -> list[Skill]:
        """Return the eligible skills for the configured roots and allowlist."""
        env_vars = os.environ if self.env_vars is None else self.env_vars
        credential_keys = self.credential_keys if self.credential_keys is not None else _collect_credential_keys()
        config_data = self.config.model_dump()
        allowlist_set = set(self.allowlist or [])

        skills_by_name: dict[str, Skill] = {}
        for root in _unique_paths(self.roots):
            for skill in _load_root_skills(root):
                normalized = _normalize_skill(skill)
                if normalized is None:
                    continue
                if self.allowlist and normalized.name not in allowlist_set:
                    continue
                if not _is_skill_eligible(
                    normalized,
                    config_data,
                    env_vars=env_vars,
                    credential_keys=credential_keys,
                ):
                    continue
                skills_by_name[normalized.name] = normalized

        if self.allowlist:
            return [skills_by_name[name] for name in self.allowlist if name in skills_by_name]
        return list(skills_by_name.values())


def build_agent_skills(
    agent_name: str,
    config: Config,
    *,
    skill_roots: Sequence[Path] | None = None,
    env_vars: Mapping[str, str] | None = None,
    credential_keys: set[str] | None = None,
) -> Skills | None:
    """Build an Agno Skills object for a specific agent."""
    agent_config = config.get_agent(agent_name)
    if not agent_config.skills:
        return None

    roots = list(skill_roots or get_default_skill_roots())
    loader = MindroomSkillsLoader(
        roots=roots,
        config=config,
        allowlist=agent_config.skills,
        env_vars=env_vars,
        credential_keys=credential_keys,
    )
    return Skills(loaders=[loader])


@dataclass(frozen=True)
class SkillCommandDispatch:
    """Dispatch configuration for a skill command."""

    tool_name: str
    arg_mode: str = "raw"
    kind: str = "tool"


@dataclass(frozen=True)
class SkillCommandSpec:
    """Resolved skill command metadata from SKILL.md."""

    name: str
    description: str
    source_path: Path
    user_invocable: bool
    disable_model_invocation: bool
    dispatch: SkillCommandDispatch | None = None


@dataclass(frozen=True)
class SkillListing:
    """Summary information for a discoverable skill."""

    name: str
    description: str
    path: Path
    origin: str


def resolve_skill_command_spec(  # noqa: C901
    skill_name: str,
    config: Config,
    agent_name: str,
    *,
    skill_roots: Sequence[Path] | None = None,
    env_vars: Mapping[str, str] | None = None,
    credential_keys: set[str] | None = None,
) -> SkillCommandSpec | None:
    """Resolve command dispatch metadata for a skill, if enabled for the agent."""
    agent_config = config.get_agent(agent_name)
    requested_name = skill_name.strip()
    if not requested_name:
        return None

    allowlist = {name.strip().lower() for name in agent_config.skills}
    if not allowlist or requested_name.lower() not in allowlist:
        return None

    env_vars = os.environ if env_vars is None else env_vars
    credential_keys = credential_keys if credential_keys is not None else _collect_credential_keys()
    config_data = config.model_dump()

    resolved: SkillCommandSpec | None = None
    roots = list(skill_roots or get_default_skill_roots())
    for root in _unique_paths(roots):
        for skill_dir in _iter_skill_dirs(root):
            frontmatter = _read_skill_frontmatter(skill_dir / SKILL_FILENAME)
            if frontmatter is None:
                continue

            name = frontmatter.get("name", skill_dir.name)
            description = frontmatter.get("description", "")
            if not isinstance(name, str) or not name.strip():
                logger.warning("Skill missing name", path=str(skill_dir))
                continue
            if not isinstance(description, str) or not description.strip():
                logger.warning("Skill missing description", name=name, path=str(skill_dir))
                continue

            if name.strip().lower() != requested_name.lower():
                continue

            metadata = _parse_metadata(frontmatter.get("metadata"), path=str(skill_dir))
            if metadata is None:
                continue

            skill = Skill(
                name=name.strip(),
                description=description.strip(),
                instructions="",
                source_path=str(skill_dir),
                metadata=metadata,
            )
            if not _is_skill_eligible(
                skill,
                config_data,
                env_vars=env_vars,
                credential_keys=credential_keys,
            ):
                continue

            user_invocable = _parse_frontmatter_bool(
                _get_frontmatter_value(frontmatter, "user-invocable", "user_invocable"),
                default=True,
            )
            disable_model_invocation = _parse_frontmatter_bool(
                _get_frontmatter_value(frontmatter, "disable-model-invocation", "disable_model_invocation"),
                default=False,
            )
            dispatch = _parse_command_dispatch(frontmatter, name.strip(), skill_dir)
            resolved = SkillCommandSpec(
                name=name.strip(),
                description=description.strip(),
                source_path=skill_dir,
                user_invocable=user_invocable,
                disable_model_invocation=disable_model_invocation,
                dispatch=dispatch,
            )

    return resolved


def set_plugin_skill_roots(roots: Sequence[Path]) -> None:
    """Replace the plugin-provided skill roots."""
    global _PLUGIN_SKILL_ROOTS
    _PLUGIN_SKILL_ROOTS = _unique_paths(roots)
    clear_skill_cache()


def get_plugin_skill_roots() -> list[Path]:
    """Return the current plugin-provided skill roots."""
    return list(_PLUGIN_SKILL_ROOTS)


def get_user_skills_dir() -> Path:
    """Return the user-managed skills directory."""
    return Path.home() / ".mindroom" / "skills"


def get_bundled_skills_dir() -> Path:
    """Return the bundled skills directory from the repo root."""
    return Path(__file__).resolve().parents[2] / "skills"


def get_default_skill_roots() -> list[Path]:
    """Return the default skill search roots in precedence order."""
    return _unique_paths([get_bundled_skills_dir(), *_PLUGIN_SKILL_ROOTS, get_user_skills_dir()])


def list_skill_listings(roots: Sequence[Path] | None = None) -> list[SkillListing]:
    """Return skill listings with precedence rules applied."""
    roots = list(roots or get_default_skill_roots())
    bundled_root = get_bundled_skills_dir().expanduser().resolve()
    user_root = get_user_skills_dir().expanduser().resolve()
    plugin_roots = {root.expanduser().resolve() for root in get_plugin_skill_roots()}

    skills_by_name: dict[str, SkillListing] = {}
    for root in _unique_paths(roots):
        origin = _root_origin(root, bundled_root, user_root, plugin_roots)
        for skill_dir in _iter_skill_dirs(root):
            frontmatter = _read_skill_frontmatter(skill_dir / SKILL_FILENAME)
            if frontmatter is None:
                continue

            name = frontmatter.get("name", skill_dir.name)
            description = frontmatter.get("description", "")
            if not isinstance(name, str) or not name.strip():
                continue
            if not isinstance(description, str) or not description.strip():
                continue

            listing = SkillListing(
                name=name.strip(),
                description=description.strip(),
                path=skill_dir / SKILL_FILENAME,
                origin=origin,
            )
            skills_by_name[listing.name] = listing

    return sorted(skills_by_name.values(), key=lambda item: item.name.lower())


def resolve_skill_listing(skill_name: str, roots: Sequence[Path] | None = None) -> SkillListing | None:
    """Resolve a skill listing by name, honoring precedence rules."""
    normalized = skill_name.strip().lower()
    if not normalized:
        return None
    for listing in list_skill_listings(roots):
        if listing.name.lower() == normalized:
            return listing
    return None


def skill_can_edit(skill_path: Path) -> bool:
    """Return True if a skill file is editable by users."""
    user_root = get_user_skills_dir().expanduser().resolve()
    try:
        resolved = skill_path.expanduser().resolve()
    except OSError:
        return False
    if resolved != user_root and user_root not in resolved.parents:
        return False
    return os.access(resolved, os.W_OK)


def clear_skill_cache() -> None:
    """Clear cached skill loads."""
    _SKILL_CACHE.clear()


def get_skill_snapshot(roots: Sequence[Path] | None = None) -> SkillSnapshot:
    """Return a snapshot of SKILL.md files under the provided roots."""
    roots = list(roots or get_default_skill_roots())
    entries: list[tuple[str, int, int]] = []
    for root in _unique_paths(roots):
        entries.extend(_snapshot_skill_files(root))
    entries.sort()
    return tuple(entries)


def _snapshot_skill_files(root: Path) -> list[tuple[str, int, int]]:
    if not root.exists() or not root.is_dir():
        return []

    entries: list[tuple[str, int, int]] = []
    for skill_file in root.rglob(SKILL_FILENAME):
        try:
            stat = skill_file.stat()
        except OSError:
            continue
        entries.append((str(skill_file), stat.st_mtime_ns, stat.st_size))
    entries.sort()
    return entries


def _iter_skill_dirs(root: Path) -> list[Path]:
    if not root.exists() or not root.is_dir():
        return []

    if (root / SKILL_FILENAME).exists():
        return [root]

    skill_dirs = [
        path
        for path in root.iterdir()
        if path.is_dir() and not path.name.startswith(".") and (path / SKILL_FILENAME).exists()
    ]
    return sorted(skill_dirs)


def _read_skill_frontmatter(skill_path: Path) -> dict[str, Any] | None:
    try:
        content = skill_path.read_text(encoding="utf-8")
    except Exception as exc:
        logger.warning("Failed to read skill file", path=str(skill_path), error=str(exc))
        return None

    match = _FRONTMATTER_PATTERN.match(content)
    if not match:
        logger.warning("Skill missing frontmatter", path=str(skill_path))
        return None

    frontmatter_text = match.group(1)
    try:
        frontmatter = yaml.safe_load(frontmatter_text) or {}
    except Exception as exc:
        logger.warning("Failed to parse skill frontmatter", path=str(skill_path), error=str(exc))
        return None

    if not isinstance(frontmatter, dict):
        logger.warning("Skill frontmatter must be a mapping", path=str(skill_path))
        return None

    return frontmatter


def _get_frontmatter_value(frontmatter: Mapping[str, object], *keys: str) -> object | None:
    for key in keys:
        if key in frontmatter:
            return frontmatter[key]
    return None


def _parse_frontmatter_bool(value: object, *, default: bool) -> bool:
    if value is None:
        return default
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        normalized = value.strip().lower()
        if normalized in {"true", "yes", "1"}:
            return True
        if normalized in {"false", "no", "0"}:
            return False
    if isinstance(value, (int, float)):
        return bool(value)
    return default


def _parse_command_dispatch(
    frontmatter: Mapping[str, Any],
    skill_name: str,
    skill_dir: Path,
) -> SkillCommandDispatch | None:
    dispatch_raw = _get_frontmatter_value(frontmatter, "command-dispatch", "command_dispatch")
    if not isinstance(dispatch_raw, str) or dispatch_raw.strip().lower() != "tool":
        return None

    tool_name = _get_frontmatter_value(frontmatter, "command-tool", "command_tool")
    if not isinstance(tool_name, str) or not tool_name.strip():
        logger.warning(
            "Skill command dispatch missing command-tool",
            skill=skill_name,
            path=str(skill_dir),
        )
        return None

    arg_mode_raw = _get_frontmatter_value(frontmatter, "command-arg-mode", "command_arg_mode")
    arg_mode = "raw"
    if isinstance(arg_mode_raw, str) and arg_mode_raw.strip():
        normalized = arg_mode_raw.strip().lower()
        if normalized != "raw":
            logger.warning(
                "Unknown skill command arg mode; defaulting to raw",
                skill=skill_name,
                arg_mode=arg_mode_raw,
                path=str(skill_dir),
            )
    return SkillCommandDispatch(tool_name=tool_name.strip(), arg_mode=arg_mode)


def _load_root_skills(root: Path) -> list[Skill]:
    if not root.exists() or not root.is_dir():
        return []

    resolved_root = root.expanduser().resolve()
    snapshot = tuple(_snapshot_skill_files(resolved_root))
    cached = _SKILL_CACHE.get(resolved_root)
    if cached and cached[0] == snapshot:
        return cached[1]

    loader = LocalSkills(str(resolved_root), validate=False)
    try:
        skills = loader.load()
    except Exception as exc:
        logger.warning("Failed to load skills", path=str(resolved_root), error=str(exc))
        if cached:
            return cached[1]
        return []

    _SKILL_CACHE[resolved_root] = (snapshot, skills)
    return skills


def _normalize_skill(skill: Skill) -> Skill | None:
    if not isinstance(skill.name, str) or not skill.name.strip():
        logger.warning("Skill missing name", path=str(skill.source_path))
        return None
    if not isinstance(skill.description, str) or not skill.description.strip():
        logger.warning("Skill missing description", name=skill.name, path=str(skill.source_path))
        return None

    skill.name = skill.name.strip()
    skill.description = skill.description.strip()

    metadata = _parse_metadata(skill.metadata, path=skill.source_path)
    if metadata is None:
        return None
    skill.metadata = metadata
    return skill


def _parse_metadata(raw: object, *, path: str) -> dict[str, Any] | None:
    if raw is None or (isinstance(raw, str) and not raw.strip()):
        return {}
    if isinstance(raw, dict):
        return raw
    if isinstance(raw, str):
        try:
            parsed = json5.loads(raw)
        except Exception as exc:
            logger.warning("Failed to parse skill metadata JSON5", path=path, error=str(exc))
            return None
        if isinstance(parsed, dict):
            return parsed
        logger.warning("Skill metadata JSON5 must be an object", path=path)
        return None

    logger.warning("Skill metadata must be a mapping or JSON5 string", path=path)
    return None


def _is_skill_eligible(
    skill: Skill,
    config_data: Mapping[str, Any],
    *,
    env_vars: Mapping[str, str],
    credential_keys: set[str],
) -> bool:
    metadata = skill.metadata or {}
    openclaw = metadata.get("openclaw")
    if not isinstance(openclaw, dict):
        return True

    if openclaw.get("always") is True:
        return True

    os_requirements = _normalize_str_list(openclaw.get("os"))
    if os_requirements and not _matches_current_os(os_requirements):
        return False

    requires = openclaw.get("requires")
    return _requirements_met(requires, config_data, env_vars, credential_keys, skill.name)


def _normalize_str_list(value: object) -> list[str]:
    if value is None:
        return []
    if isinstance(value, str):
        return [value]
    if isinstance(value, (list, tuple, set)):
        return [item for item in value if isinstance(item, str)]
    return []


def _matches_current_os(requirements: Sequence[str]) -> bool:
    current_os = platform.system().lower()
    aliases = _OS_ALIASES.get(current_os, {current_os})
    return any(requirement.lower() in aliases for requirement in requirements)


def _env_requirements_met(
    requirements: Sequence[str],
    env_vars: Mapping[str, str],
    credential_keys: set[str],
) -> bool:
    for requirement in requirements:
        if env_vars.get(requirement):
            continue
        if requirement in credential_keys:
            continue
        return False
    return True


def _missing_bins(requirements: Sequence[str]) -> list[str]:
    return [requirement for requirement in requirements if shutil.which(requirement) is None]


def _any_bins_requirements_met(requirements: Sequence[str]) -> bool:
    return any(shutil.which(requirement) for requirement in requirements)


def _config_requirements_met(requirements: Sequence[str], config_data: Mapping[str, Any]) -> bool:
    return all(_config_path_truthy(config_data, requirement) for requirement in requirements)


def _requirements_met(
    requires: object,
    config_data: Mapping[str, Any],
    env_vars: Mapping[str, str],
    credential_keys: set[str],
    skill_name: str,
) -> bool:
    if not isinstance(requires, dict):
        return True

    env_requirements = _normalize_str_list(requires.get("env"))
    if env_requirements and not _env_requirements_met(env_requirements, env_vars, credential_keys):
        return False

    config_requirements = _normalize_str_list(requires.get("config"))
    if config_requirements and not _config_requirements_met(config_requirements, config_data):
        return False

    bin_requirements = _normalize_str_list(requires.get("bins"))
    if bin_requirements:
        missing_bins = _missing_bins(bin_requirements)
        if missing_bins:
            logger.debug("Skill missing required binaries", skill=skill_name, bins=missing_bins)
            return False

    any_bins_requirements = _normalize_str_list(requires.get("anyBins"))
    if any_bins_requirements and not _any_bins_requirements_met(any_bins_requirements):
        logger.debug("Skill missing any required binaries", skill=skill_name, bins=any_bins_requirements)
        return False

    return True


def _config_path_truthy(config_data: Mapping[str, Any], path: str) -> bool:
    current: Any = config_data
    for part in path.split("."):
        if isinstance(current, Mapping) and part in current:
            current = current[part]
        else:
            return False
    return bool(current)


def _collect_credential_keys() -> set[str]:
    credentials_manager = get_credentials_manager()
    keys: set[str] = set()
    for service in credentials_manager.list_services():
        credentials = credentials_manager.load_credentials(service) or {}
        for key, value in credentials.items():
            if value:
                keys.add(key)
    return keys


def _unique_paths(paths: Sequence[Path]) -> list[Path]:
    seen: set[Path] = set()
    unique_paths: list[Path] = []
    for path in paths:
        resolved = path.expanduser().resolve()
        if resolved in seen:
            continue
        seen.add(resolved)
        unique_paths.append(resolved)
    return unique_paths


def _root_origin(root: Path, bundled_root: Path, user_root: Path, plugin_roots: set[Path]) -> str:
    if root == bundled_root:
        return "bundled"
    if root == user_root:
        return "user"
    if root in plugin_roots:
        return "plugin"
    return "custom"

================================================================================
# FILE: src/mindroom/stop.py
================================================================================

"""Minimal stop button functionality for the bot."""

from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import TYPE_CHECKING

import nio

if TYPE_CHECKING:
    from nio import AsyncClient

import structlog

logger = structlog.get_logger(__name__)


@dataclass
class TrackedMessage:
    """Track a message with stop button."""

    message_id: str
    room_id: str
    task: asyncio.Task[None]
    reaction_event_id: str | None = None


class StopManager:
    """Manager for handling stop reactions."""

    def __init__(self) -> None:
        """Initialize the stop manager."""
        # Track multiple concurrent messages by message_id
        self.tracked_messages: dict[str, TrackedMessage] = {}
        # Keep references to cleanup tasks
        self.cleanup_tasks: list[asyncio.Task] = []
        logger.info("StopManager initialized")

    def set_current(
        self,
        message_id: str,
        room_id: str,
        task: asyncio.Task[None],
        reaction_event_id: str | None = None,
    ) -> None:
        """Track a message generation."""
        self.tracked_messages[message_id] = TrackedMessage(
            message_id=message_id,
            room_id=room_id,
            task=task,
            reaction_event_id=reaction_event_id,
        )
        logger.info(
            "Tracking message generation",
            message_id=message_id,
            room_id=room_id,
            reaction_event_id=reaction_event_id,
            total_tracked=len(self.tracked_messages),
        )

    def clear_message(
        self,
        message_id: str,
        client: AsyncClient,
        remove_button: bool = True,
        delay: float = 5.0,
    ) -> None:
        """Clear tracking for a specific message and optionally remove stop button.

        Args:
            message_id: The message ID to clear
            client: Matrix client for removing stop button
            remove_button: Whether to remove the stop button (default True)
            delay: Seconds to wait before clearing (default 5.0)

        """

        async def delayed_clear() -> None:
            """Clear the message and remove stop button after a delay."""
            if remove_button and message_id in self.tracked_messages:
                tracked = self.tracked_messages[message_id]
                if tracked.reaction_event_id:
                    logger.info("Removing stop button in cleanup", message_id=message_id)
                    try:
                        await client.room_redact(
                            room_id=tracked.room_id,
                            event_id=tracked.reaction_event_id,
                            reason="Response completed",
                        )
                        tracked.reaction_event_id = None
                    except Exception as e:
                        logger.warning(f"Failed to remove stop button in cleanup: {e}")

            await asyncio.sleep(delay)
            if message_id in self.tracked_messages:
                logger.info("Clearing tracked message after delay", message_id=message_id, delay=delay)
                del self.tracked_messages[message_id]

        if message_id in self.tracked_messages:
            logger.info(
                "Scheduling message cleanup",
                message_id=message_id,
                delay=delay,
                remove_button=remove_button,
            )
            task = asyncio.create_task(delayed_clear())
            self.cleanup_tasks.append(task)
            # Clean up old completed tasks
            self.cleanup_tasks = [t for t in self.cleanup_tasks if not t.done()]
        else:
            logger.debug("Message not tracked, skipping cleanup", message_id=message_id)

    async def handle_stop_reaction(self, message_id: str) -> bool:
        """Handle a stop reaction for a message.

        Returns True if the task was cancelled, False otherwise.
        """
        logger.info(
            "Handling stop reaction",
            message_id=message_id,
            tracked_messages=list(self.tracked_messages.keys()),
        )

        if message_id in self.tracked_messages:
            tracked = self.tracked_messages[message_id]
            if tracked.task and not tracked.task.done():
                logger.info("Cancelling task for message", message_id=message_id)
                tracked.task.cancel()
                # Don't clear here - let the finally block handle it
                return True
            logger.info(
                "Task already completed or missing",
                message_id=message_id,
                task_exists=tracked.task is not None,
                task_done=tracked.task.done() if tracked.task else None,
            )
        else:
            logger.warning("Stop reaction for untracked message", message_id=message_id)
        return False

    async def add_stop_button(self, client: AsyncClient, room_id: str, message_id: str) -> str | None:
        """Add a stop button reaction to a message.

        Returns:
            The event ID of the reaction if successful, None otherwise.

        """
        logger.info("Adding stop button", room_id=room_id, message_id=message_id)
        try:
            response = await client.room_send(
                room_id=room_id,
                message_type="m.reaction",
                content={
                    "m.relates_to": {
                        "rel_type": "m.annotation",
                        "event_id": message_id,
                        "key": "ðŸ›‘",
                    },
                },
            )
            if isinstance(response, nio.RoomSendResponse):
                event_id = str(response.event_id)
                logger.info("Stop button added successfully", reaction_event_id=event_id, message_id=message_id)
                # Update the tracked message with the reaction event ID
                if message_id in self.tracked_messages:
                    self.tracked_messages[message_id].reaction_event_id = event_id
                return event_id
            logger.warning("Failed to add stop button - no event_id in response", response=response)
        except Exception as e:
            logger.exception("Exception adding stop button", error=str(e))
        return None

    async def remove_stop_button(self, client: AsyncClient, message_id: str | None = None) -> None:
        """Remove the stop button reaction immediately when user clicks it.

        Args:
            client: The Matrix client
            message_id: The message ID to remove the button from

        """
        if message_id and message_id in self.tracked_messages:
            tracked = self.tracked_messages[message_id]
            if tracked.reaction_event_id and tracked.room_id:
                logger.info(
                    "Removing stop button immediately (user clicked)",
                    message_id=message_id,
                    reaction_event_id=tracked.reaction_event_id,
                )
                try:
                    await client.room_redact(
                        room_id=tracked.room_id,
                        event_id=tracked.reaction_event_id,
                        reason="User clicked stop",
                    )
                    tracked.reaction_event_id = None
                    logger.info("Stop button removed successfully")
                except Exception as e:
                    logger.exception("Failed to remove stop button", error=str(e))
            else:
                logger.debug(
                    "Stop button already removed or missing",
                    message_id=message_id,
                    has_reaction_id=tracked.reaction_event_id is not None,
                )
        else:
            logger.debug("Message not tracked, cannot remove stop button", message_id=message_id)

================================================================================
# FILE: src/mindroom/streaming.py
================================================================================

"""Streaming response implementation for real-time message updates."""

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import TYPE_CHECKING

from agno.run.agent import RunContentEvent, ToolCallCompletedEvent, ToolCallStartedEvent

from . import interactive
from .logging_config import get_logger
from .matrix.client import edit_message, send_message
from .matrix.mentions import format_message_with_mentions
from .tool_events import (
    StructuredStreamChunk,
    ToolTraceEntry,
    complete_pending_tool_block,
    extract_tool_completed_info,
    format_tool_started_event,
)

if TYPE_CHECKING:
    from collections.abc import AsyncIterator

    import nio

    from .config import Config

from .matrix.client import get_latest_thread_event_id_if_needed

logger = get_logger(__name__)

# Global constant for the in-progress marker
IN_PROGRESS_MARKER = " â‹¯"
StreamInputChunk = str | StructuredStreamChunk | RunContentEvent | ToolCallStartedEvent | ToolCallCompletedEvent


def _longest_common_prefix_len(first: list[ToolTraceEntry], second: list[ToolTraceEntry]) -> int:
    """Return the number of leading tool-trace entries shared by both lists."""
    max_len = min(len(first), len(second))
    index = 0
    while index < max_len and first[index] == second[index]:
        index += 1
    return index


def _merge_tool_trace(existing: list[ToolTraceEntry], incoming: list[ToolTraceEntry]) -> list[ToolTraceEntry]:
    """Merge a trace snapshot without dropping entries when stream styles are mixed."""
    if not existing:
        return incoming.copy()
    if not incoming:
        return existing.copy()

    shared_prefix = _longest_common_prefix_len(existing, incoming)
    if shared_prefix == len(existing):
        # Incoming is newer or equal.
        return incoming.copy()
    if shared_prefix == len(incoming):
        # Incoming is an older prefix; keep current entries.
        return existing.copy()

    # Diverged snapshots: preserve known history and append unseen tail.
    return existing + incoming[shared_prefix:]


@dataclass
class StreamingResponse:
    """Manages a streaming response with incremental message updates."""

    room_id: str
    reply_to_event_id: str | None
    thread_id: str | None
    sender_domain: str
    config: Config
    accumulated_text: str = ""
    event_id: str | None = None  # None until first message sent
    last_update: float = 0.0
    update_interval: float = 5.0
    min_update_interval: float = 0.5
    interval_ramp_seconds: float = 15.0
    latest_thread_event_id: str | None = None  # For MSC3440 compliance
    tool_trace: list[ToolTraceEntry] = field(default_factory=list)
    stream_started_at: float | None = None

    def _update(self, new_chunk: str) -> None:
        """Append new chunk to accumulated text."""
        self.accumulated_text += new_chunk

    def _current_update_interval(self, current_time: float) -> float:
        """Return the current throttling interval.

        Streaming starts with faster edits, then ramps toward the steady-state
        interval to reduce edit noise for long responses.
        """
        if self.stream_started_at is None or self.interval_ramp_seconds <= 0:
            return self.update_interval

        fast_interval = min(self.min_update_interval, self.update_interval)
        elapsed = max(0.0, current_time - self.stream_started_at)
        if elapsed >= self.interval_ramp_seconds:
            return self.update_interval

        progress = elapsed / self.interval_ramp_seconds
        return fast_interval + (self.update_interval - fast_interval) * progress

    async def _throttled_send(self, client: nio.AsyncClient) -> None:
        """Send/edit the message if enough time has passed since the last update."""
        current_time = time.time()
        if self.stream_started_at is None:
            self.stream_started_at = current_time
        current_interval = self._current_update_interval(current_time)
        if current_time - self.last_update >= current_interval:
            await self._send_or_edit_message(client)
            self.last_update = current_time

    async def update_content(self, new_chunk: str, client: nio.AsyncClient) -> None:
        """Add new content and potentially update the message."""
        self._update(new_chunk)
        await self._throttled_send(client)

    async def finalize(self, client: nio.AsyncClient) -> None:
        """Send final message update."""
        await self._send_or_edit_message(client, is_final=True)

    async def _send_or_edit_message(self, client: nio.AsyncClient, is_final: bool = False) -> None:
        """Send new message or edit existing one."""
        if not self.accumulated_text.strip():
            return

        # Always ensure we have a thread_id - use the original message as thread root if needed
        effective_thread_id = self.thread_id if self.thread_id else self.reply_to_event_id

        # Add in-progress marker during streaming (not on final update)
        text_to_send = self.accumulated_text
        if not is_final:
            text_to_send += IN_PROGRESS_MARKER

        # Format the text (handles interactive questions if present)
        response = interactive.parse_and_format_interactive(text_to_send, extract_mapping=False)
        display_text = response.formatted_text

        # Only use latest_thread_event_id for the initial message (not edits)
        latest_for_message = self.latest_thread_event_id if self.event_id is None else None

        content = format_message_with_mentions(
            config=self.config,
            text=display_text,
            sender_domain=self.sender_domain,
            thread_event_id=effective_thread_id,
            reply_to_event_id=self.reply_to_event_id,
            latest_thread_event_id=latest_for_message,
            tool_trace=self.tool_trace,
        )

        if self.event_id is None:
            # First message - send new
            logger.debug("Sending initial streaming message")
            response_event_id = await send_message(client, self.room_id, content)
            if response_event_id:
                self.event_id = response_event_id
                logger.debug("Initial streaming message sent", event_id=self.event_id)
            else:
                logger.error("Failed to send initial streaming message")
        else:
            # Subsequent updates - edit existing message
            logger.debug("Editing streaming message", event_id=self.event_id)
            response_event_id = await edit_message(client, self.room_id, self.event_id, content, display_text)
            if not response_event_id:
                logger.error("Failed to edit streaming message")


class ReplacementStreamingResponse(StreamingResponse):
    """StreamingResponse variant that replaces content instead of appending.

    Useful for structured live rendering where the full document is rebuilt
    on each tick and we want the message to reflect the latest full view,
    not incremental concatenation.
    """

    def _update(self, new_chunk: str) -> None:
        """Replace accumulated text with new chunk."""
        self.accumulated_text = new_chunk


async def send_streaming_response(  # noqa: C901, PLR0912
    client: nio.AsyncClient,
    room_id: str,
    reply_to_event_id: str | None,
    thread_id: str | None,
    sender_domain: str,
    config: Config,
    response_stream: AsyncIterator[StreamInputChunk],
    streaming_cls: type[StreamingResponse] = StreamingResponse,
    header: str | None = None,
    existing_event_id: str | None = None,
) -> tuple[str | None, str]:
    """Stream chunks to a Matrix room, returning (event_id, accumulated_text).

    Args:
        client: Matrix client
        room_id: Destination room
        reply_to_event_id: Event to reply to (can be None when in a thread)
        thread_id: Thread root if already in a thread
        sender_domain: Sender's homeserver domain for mention formatting
        config: App config for mention formatting
        response_stream: Async iterator yielding text chunks or response events
        streaming_cls: StreamingResponse class to use (default: StreamingResponse, alternative: ReplacementStreamingResponse)
        header: Optional text prefix to send before chunks
        existing_event_id: If editing an existing message, pass its ID

    Returns:
        Tuple of (final event_id or None, full accumulated text)

    """
    latest_thread_event_id = await get_latest_thread_event_id_if_needed(
        client,
        room_id,
        thread_id,
        reply_to_event_id,
        existing_event_id,
    )

    streaming = streaming_cls(
        room_id=room_id,
        reply_to_event_id=reply_to_event_id,
        thread_id=thread_id,
        sender_domain=sender_domain,
        config=config,
        latest_thread_event_id=latest_thread_event_id,
    )

    # Ensure the first chunk triggers an initial send immediately
    streaming.last_update = float("-inf")

    if existing_event_id:
        streaming.event_id = existing_event_id
        streaming.accumulated_text = ""

    if header:
        await streaming.update_content(header, client)

    async for chunk in response_stream:
        # Handle different types of chunks from the stream
        if isinstance(chunk, str):
            text_chunk = chunk
        elif isinstance(chunk, StructuredStreamChunk):
            text_chunk = chunk.content
            if chunk.tool_trace is not None:
                streaming.tool_trace = _merge_tool_trace(streaming.tool_trace, chunk.tool_trace)
        elif isinstance(chunk, RunContentEvent) and chunk.content:
            text_chunk = str(chunk.content)
        elif isinstance(chunk, ToolCallStartedEvent):
            text_chunk, trace_entry = format_tool_started_event(chunk.tool)
            if trace_entry is not None:
                streaming.tool_trace.append(trace_entry)
        elif isinstance(chunk, ToolCallCompletedEvent):
            info = extract_tool_completed_info(chunk.tool)
            if info:
                tool_name, result = info
                streaming.accumulated_text, trace_entry = complete_pending_tool_block(
                    streaming.accumulated_text,
                    tool_name,
                    result,
                )
                streaming.tool_trace.append(trace_entry)
                await streaming._throttled_send(client)
                continue
            text_chunk = ""
        else:
            logger.debug(f"Unhandled streaming event type: {type(chunk).__name__}")
            continue

        if text_chunk:
            await streaming.update_content(text_chunk, client)

    await streaming.finalize(client)

    return streaming.event_id, streaming.accumulated_text

================================================================================
# FILE: src/mindroom/teams.py
================================================================================

"""Team-based collaboration for multiple agents."""

from __future__ import annotations

from enum import Enum
from typing import TYPE_CHECKING, Any, Literal, NamedTuple

from agno.agent import Agent
from agno.models.message import Message
from agno.run.agent import RunContentEvent as AgentRunContentEvent
from agno.run.agent import RunOutput
from agno.run.agent import ToolCallCompletedEvent as AgentToolCallCompletedEvent
from agno.run.agent import ToolCallStartedEvent as AgentToolCallStartedEvent
from agno.run.team import RunContentEvent as TeamRunContentEvent
from agno.run.team import TeamRunOutput
from agno.run.team import ToolCallCompletedEvent as TeamToolCallCompletedEvent
from agno.run.team import ToolCallStartedEvent as TeamToolCallStartedEvent
from agno.team import Team
from pydantic import BaseModel, Field

from . import agent_prompts
from .ai import get_model_instance
from .constants import ROUTER_AGENT_NAME
from .error_handling import get_user_friendly_error_message
from .logging_config import get_logger
from .matrix.rooms import get_room_alias_from_id
from .thread_utils import get_available_agents_in_room
from .tool_events import (
    StructuredStreamChunk,
    ToolTraceEntry,
    complete_pending_tool_block,
    extract_tool_completed_info,
    format_tool_started_event,
)

if TYPE_CHECKING:
    from collections.abc import AsyncIterator

    import nio

    from .bot import MultiAgentOrchestrator
    from .config import Config
    from .matrix.identity import MatrixID


logger = get_logger(__name__)

# Message length limits for team context and logging
MAX_CONTEXT_MESSAGE_LENGTH = 200  # Maximum length for messages to include in thread context
MAX_LOG_MESSAGE_LENGTH = 500  # Maximum length for messages in team response logs
TeamStreamChunk = str | StructuredStreamChunk


class TeamMode(str, Enum):
    """Team collaboration modes."""

    COORDINATE = "coordinate"  # Leader delegates and synthesizes (can be sequential OR parallel)
    COLLABORATE = "collaborate"  # All members work on same task in parallel


class TeamModeDecision(BaseModel):
    """AI decision for team collaboration mode."""

    mode: Literal["coordinate", "collaborate"] = Field(
        description="coordinate for delegation and synthesis, collaborate for all working on same task",
    )
    reasoning: str = Field(description="Brief explanation of why this mode was chosen")


def format_team_header(agent_names: list[str]) -> str:
    """Format the team response header.

    Args:
        agent_names: List of agent names in the team

    Returns:
        Formatted header string

    """
    return f"ðŸ¤ **Team Response** ({', '.join(agent_names)}):\n\n"


def format_member_contribution(agent_name: str, content: str, indent: int = 0) -> str:
    """Format a single team member's contribution.

    Args:
        agent_name: Name of the agent
        content: The agent's response content
        indent: Indentation level

    Returns:
        Formatted contribution string

    """
    indent_str = "  " * indent
    return f"{indent_str}**{agent_name}**: {content}"


def format_team_consensus(consensus: str, indent: int = 0) -> list[str]:
    """Format the team consensus section.

    Args:
        consensus: The consensus content
        indent: Indentation level

    Returns:
        List of formatted lines for the consensus

    """
    indent_str = "  " * indent
    parts = []
    if consensus:
        parts.append(f"\n{indent_str}**Team Consensus**:")
        parts.append(f"{indent_str}{consensus}")
    return parts


def format_no_consensus_note(indent: int = 0) -> str:
    """Format the note when there's no team consensus.

    Args:
        indent: Indentation level

    Returns:
        Formatted note string

    """
    indent_str = "  " * indent
    return f"\n{indent_str}*No team consensus - showing individual responses only*"


def format_team_response(response: TeamRunOutput | RunOutput) -> list[str]:
    """Format a complete team response with member contributions.

    Handles nested teams recursively with proper indentation.

    Args:
        response: The team or agent response to extract contributions from

    Returns:
        List of formatted contribution strings

    """
    return _format_contributions_recursive(response, indent=0, include_consensus=True)


def _format_contributions_recursive(  # noqa: C901
    response: TeamRunOutput | RunOutput,
    indent: int,
    include_consensus: bool,
) -> list[str]:
    """Internal recursive function for formatting contributions.

    Args:
        response: The response to extract from
        indent: Current indentation level
        include_consensus: Whether to include team consensus

    Returns:
        List of formatted contribution strings

    """
    parts = []
    indent_str = "  " * indent

    if isinstance(response, TeamRunOutput):
        if response.member_responses:
            for member_resp in response.member_responses:
                if isinstance(member_resp, TeamRunOutput):
                    team_name = member_resp.team_name or "Nested Team"
                    parts.append(f"{indent_str}**{team_name}** (Team):")
                    nested_parts = _format_contributions_recursive(
                        member_resp,
                        indent=indent + 1,
                        include_consensus=False,  # No consensus for nested teams
                    )
                    parts.extend(nested_parts)
                elif isinstance(member_resp, RunOutput):
                    agent_name = member_resp.agent_name or "Team Member"
                    content = _get_response_content(member_resp)
                    if content:
                        parts.append(format_member_contribution(agent_name, content, indent))

        if include_consensus:
            if response.content:
                parts.extend(format_team_consensus(response.content, indent))
            elif parts:
                parts.append(format_no_consensus_note(indent))

    elif isinstance(response, RunOutput):
        agent_name = response.agent_name or "Agent"
        content = _get_response_content(response)
        if content:
            parts.append(format_member_contribution(agent_name, content, indent))

    return parts


def _get_response_content(response: TeamRunOutput | RunOutput) -> str:
    """Get content from a response object.

    Args:
        response: The response to extract content from

    Returns:
        The extracted content as a string

    """
    if response.content:
        return str(response.content)

    # Note: This concatenates ALL assistant messages, which might include
    # multiple turns in a conversation. Consider if you want just the
    # last message or all of them.
    if response.messages:
        messages_list: list[Any] = response.messages
        content_parts = [
            str(msg.content)
            for msg in messages_list
            if isinstance(msg, Message) and msg.role == "assistant" and msg.content
        ]

        return "\n\n".join(content_parts) if content_parts else ""

    return ""


class TeamFormationDecision(NamedTuple):
    """Result of decide_team_formation."""

    should_form_team: bool
    agents: list[MatrixID]
    mode: TeamMode


async def select_team_mode(
    message: str,
    agent_names: list[str],
    config: Config,
) -> TeamMode:
    """Use AI to determine optimal team collaboration mode.

    Args:
        message: The user's message/task
        agent_names: List of agents that will form the team
        config: Application configuration for model access

    Returns:
        TeamMode.COORDINATE or TeamMode.COLLABORATE

    """
    prompt = f"""Determine the best team collaboration mode for this task.

Task: {message}
Agents: {", ".join(agent_names)}

Team Modes (from Agno documentation):
- "coordinate": Team leader delegates tasks to members and synthesizes their outputs.
               The leader decides whether to send tasks sequentially or in parallel based on what's appropriate.
- "collaborate": All team members are given the SAME task and work on it simultaneously.
                The leader synthesizes all their outputs into a cohesive response.

Decision Guidelines:
- Use "coordinate" when agents need to do DIFFERENT subtasks (whether sequential or parallel)
- Use "collaborate" when you want ALL agents working on the SAME problem for diverse perspectives

Examples:
- "Email me then call me" â†’ coordinate (different tasks: email agent sends email, phone agent makes call)
- "Get weather and news" â†’ coordinate (different tasks: weather agent gets weather, news agent gets news)
- "Research this topic and analyze the data" â†’ coordinate (different subtasks for each agent)
- "What do you think about X?" â†’ collaborate (all agents provide their perspective on the same question)
- "Brainstorm solutions" â†’ collaborate (all agents work on the same brainstorming task)

Return the mode and a one-sentence reason why."""

    model = get_model_instance(config, "default")
    agent = Agent(
        name="TeamModeDecider",
        role="Determine team mode",
        model=model,
        output_schema=TeamModeDecision,
    )

    try:
        response = await agent.arun(prompt, session_id="team_mode_decision")
        decision = response.content
        if isinstance(decision, TeamModeDecision):
            logger.info(f"Team mode: {decision.mode} - {decision.reasoning}")
            return TeamMode.COORDINATE if decision.mode == "coordinate" else TeamMode.COLLABORATE
        # Fallback if response is unexpected
        logger.debug(f"Unexpected response type from AI: {type(decision).__name__}, defaulting to collaborate")
        return TeamMode.COLLABORATE  # noqa: TRY300
    except Exception as e:
        logger.debug(f"AI team mode decision failed (will use default): {e}")
        return TeamMode.COLLABORATE


async def decide_team_formation(
    agent: MatrixID,
    tagged_agents: list[MatrixID],
    agents_in_thread: list[MatrixID],
    all_mentioned_in_thread: list[MatrixID],
    room: nio.MatrixRoom,
    message: str | None = None,
    config: Config | None = None,
    use_ai_decision: bool = True,
    is_dm_room: bool = False,
    is_thread: bool = False,
) -> TeamFormationDecision:
    """Determine if a team should form and with which mode.

    Args:
        agent: The agent calling this function
        tagged_agents: Agents explicitly mentioned in the current message
        agents_in_thread: Agents that have participated in the thread
        all_mentioned_in_thread: All agents ever mentioned in the thread
        room: The Matrix room object (for checking available agents)
        message: The user's message (for AI decision context)
        config: Application configuration (for AI model access)
        use_ai_decision: Whether to use AI for mode selection
        is_dm_room: Whether this is a DM room
        is_thread: Whether the current message is in a thread

    Returns:
        TeamFormationDecision with team formation decision

    """
    team_agents: list[MatrixID] = []

    # Case 1: Multiple agents explicitly tagged
    if len(tagged_agents) > 1:
        logger.info(f"Team formation needed for tagged agents: {tagged_agents}")
        team_agents = tagged_agents

    # Case 2: No agents tagged but multiple were mentioned before in thread
    elif not tagged_agents and len(all_mentioned_in_thread) > 1:
        logger.info(f"Team formation needed for previously mentioned agents: {all_mentioned_in_thread}")
        team_agents = all_mentioned_in_thread

    # Case 3: No agents tagged but multiple in thread
    elif not tagged_agents and len(agents_in_thread) > 1:
        logger.info(f"Team formation needed for thread agents: {agents_in_thread}")
        team_agents = agents_in_thread

    # Case 4: DM room with multiple agents and no mentions (main timeline only)
    # We avoid forming a team inside an existing thread to preserve
    # single-agent ownership unless the thread itself involves multiple agents
    elif is_dm_room and not is_thread and not tagged_agents and room and config:
        available_agents = get_available_agents_in_room(room, config)
        if len(available_agents) > 1:
            logger.info(f"Team formation needed for DM room with multiple agents: {available_agents}")
            team_agents = available_agents

    if not team_agents:
        return TeamFormationDecision(
            should_form_team=False,
            agents=[],
            mode=TeamMode.COLLABORATE,
        )

    is_first_agent = min(team_agents, key=lambda x: x.username) == agent
    # Only do this AI call for the first agent to avoid duplication
    if use_ai_decision and message and config and is_first_agent:
        agent_names = [mid.agent_name(config) or mid.username for mid in team_agents]
        mode = await select_team_mode(message, agent_names, config)
    else:
        # Fallback to hardcoded logic when AI decision is disabled or unavailable
        # Use COORDINATE when agents are explicitly tagged (they likely have different roles)
        # Use COLLABORATE when agents are from thread history (likely discussing same topic)
        mode = TeamMode.COORDINATE if len(tagged_agents) > 1 else TeamMode.COLLABORATE
        logger.info(f"Using hardcoded mode selection: {mode.value}")

    return TeamFormationDecision(should_form_team=True, agents=team_agents, mode=mode)


def _build_prompt_with_context(
    message: str,
    thread_history: list[dict] | None = None,
) -> str:
    """Build a prompt with thread context if available.

    Args:
        message: The user's message
        thread_history: Optional thread history for context

    Returns:
        Formatted prompt with context

    """
    if not thread_history:
        return message

    recent_messages = thread_history[-30:]  # Last 30 messages for context
    context_parts = []
    for msg in recent_messages:
        sender = msg.get("sender", "Unknown")
        body = msg.get("content", {}).get("body", "")
        if body and len(body) < MAX_CONTEXT_MESSAGE_LENGTH:
            context_parts.append(f"{sender}: {body}")

    if context_parts:
        context = "\n".join(context_parts)
        return f"Thread Context:\n{context}\n\nUser: {message}"

    return message


def _get_agents_from_orchestrator(
    agent_names: list[str],
    orchestrator: MultiAgentOrchestrator,
) -> list[Agent]:
    """Get Agent instances from orchestrator for the given agent names.

    Args:
        agent_names: List of agent names to get
        orchestrator: The orchestrator containing agent bots

    Returns:
        List of Agent instances (excluding router and missing agents)

    """
    agents: list[Agent] = []
    for name in agent_names:
        if name == ROUTER_AGENT_NAME:
            continue

        if name not in orchestrator.agent_bots:
            logger.warning(f"Agent '{name}' not found in orchestrator - may not be in room")
            continue

        agent_bot = orchestrator.agent_bots[name]
        if agent_bot.agent is not None:
            agent = agent_bot.agent
            # Remove interactive question prompts to prevent emoji conflicts in team responses
            if isinstance(agent.instructions, list):
                agent.instructions = [
                    instr for instr in agent.instructions if instr != agent_prompts.INTERACTIVE_QUESTION_PROMPT
                ]
            agents.append(agent)
        else:
            logger.warning(f"Agent bot '{name}' has no agent instance")

    return agents


def _create_team_instance(
    agents: list[Agent],
    agent_names: list[str],
    mode: TeamMode,
    orchestrator: MultiAgentOrchestrator,
    model_name: str | None = None,
) -> Team:
    """Create a configured Team instance.

    Args:
        agents: List of Agent instances for the team
        agent_names: List of agent names (for team name)
        mode: Team collaboration mode
        orchestrator: The orchestrator containing configuration
        model_name: Optional model name override

    Returns:
        Configured Team instance

    """
    assert orchestrator.config is not None
    model = get_model_instance(orchestrator.config, model_name or "default")

    return Team(
        members=agents,  # type: ignore[arg-type]
        name=f"Team-{'-'.join(agent_names)}",
        model=model,
        delegate_to_all_members=mode == TeamMode.COLLABORATE,
        show_members_responses=True,
        debug_mode=False,
        # Agno will automatically list members with their names, roles, and tools
    )


def select_model_for_team(team_name: str, room_id: str, config: Config) -> str:
    """Get the appropriate model for a team in a specific room.

    Priority:
    1. Room-specific model from room_models
    2. Team's configured model
    3. Global default model

    Args:
        team_name: Name of the team
        room_id: Matrix room ID
        config: Application configuration

    Returns:
        Model name to use

    """
    room_alias = get_room_alias_from_id(room_id)

    if room_alias and room_alias in config.room_models:
        model = config.room_models[room_alias]
        logger.info(f"Using room-specific model for {team_name} in {room_alias}: {model}")
        return model

    if team_name in config.teams:
        team_config = config.teams[team_name]
        if team_config.model:
            logger.info(f"Using team-specific model for {team_name}: {team_config.model}")
            return team_config.model

    logger.info(f"Using default model for {team_name}")
    return "default"


NO_AGENTS_RESPONSE = "Sorry, no agents available for team collaboration."


async def team_response(
    agent_names: list[str],
    mode: TeamMode,
    message: str,
    orchestrator: MultiAgentOrchestrator,
    thread_history: list[dict] | None = None,
    model_name: str | None = None,
) -> str:
    """Create a team and execute response."""
    agents = _get_agents_from_orchestrator(agent_names, orchestrator)

    if not agents:
        return NO_AGENTS_RESPONSE

    prompt = _build_prompt_with_context(message, thread_history)
    team = _create_team_instance(agents, agent_names, mode, orchestrator, model_name)
    agent_list = ", ".join(str(a.name) for a in agents if a.name)

    logger.info(f"Executing team response with {len(agents)} agents in {mode.value} mode")
    logger.info(f"TEAM PROMPT: {prompt[:500]}")

    try:
        response = await team.arun(prompt)
    except Exception as e:
        logger.exception(f"Error in team response with agents {agent_list}")
        # Return user-friendly error message
        team_name = f"Team ({agent_list})"
        return get_user_friendly_error_message(e, team_name)

    if isinstance(response, TeamRunOutput):
        if response.member_responses:
            logger.debug(f"Team had {len(response.member_responses)} member responses")

        logger.info(f"Team consensus content: {response.content[:200] if response.content else 'None'}")

        parts = format_team_response(response)
        team_response = "\n\n".join(parts) if parts else "No team response generated."
    else:
        logger.warning(f"Unexpected response type: {type(response)}", response=response)
        team_response = str(response)

    logger.info(f"TEAM RESPONSE ({agent_list}): {team_response[:MAX_LOG_MESSAGE_LENGTH]}")
    if len(team_response) > MAX_LOG_MESSAGE_LENGTH:
        logger.debug(f"TEAM RESPONSE (full): {team_response}")

    # Don't use @ mentions as that would trigger the agents again
    agent_names = [str(a.name) for a in agents if a.name]
    team_header = format_team_header(agent_names)

    return team_header + team_response


async def team_response_stream_raw(
    agent_ids: list[MatrixID],
    mode: TeamMode,
    message: str,
    orchestrator: MultiAgentOrchestrator,
    thread_history: list[dict] | None = None,
    model_name: str | None = None,
) -> AsyncIterator[Any]:
    """Yield raw team events (for structured live rendering). Falls back to a final response.

    Returns an async iterator of Agno events when supported; otherwise yields a
    single TeamRunOutput for non-streaming providers.
    """
    assert orchestrator.config is not None
    agent_names = [mid.agent_name(orchestrator.config) or mid.username for mid in agent_ids]
    agents = _get_agents_from_orchestrator(agent_names, orchestrator)

    if not agents:

        async def _empty() -> AsyncIterator[RunOutput]:
            yield RunOutput(content=NO_AGENTS_RESPONSE)

        return _empty()

    prompt = _build_prompt_with_context(message, thread_history)
    team = _create_team_instance(agents, agent_names, mode, orchestrator, model_name)

    logger.info(f"Created team with {len(agents)} agents in {mode.value} mode")
    for agent in agents:
        logger.debug(f"Team member: {agent.name}")

    try:
        return team.arun(prompt, stream=True, stream_events=True)
    except Exception as e:
        logger.exception(f"Error in team streaming with agents {agent_names}")
        team_name = f"Team ({', '.join(agent_names)})"
        error_message = get_user_friendly_error_message(e, team_name)

        async def _error() -> AsyncIterator[RunOutput]:
            yield RunOutput(content=error_message)

        return _error()


async def team_response_stream(  # noqa: C901, PLR0912, PLR0915
    agent_ids: list[MatrixID],
    message: str,
    orchestrator: MultiAgentOrchestrator,
    mode: TeamMode = TeamMode.COORDINATE,
    thread_history: list[dict] | None = None,
    model_name: str | None = None,
) -> AsyncIterator[TeamStreamChunk]:
    """Aggregate team streaming into a non-stream-style document, live.

    Renders a header and per-member sections, optionally adding a team
    consensus if present. Rebuilds the entire document as new events
    arrive so the final shape matches the non-stream style.
    """
    assert orchestrator.config is not None
    agent_names: list[str] = []
    display_names: list[str] = []

    for mid in agent_ids:
        agent_name = mid.agent_name(orchestrator.config)
        assert agent_name is not None
        agent_names.append(agent_name)

        agent_config = orchestrator.config.agents[agent_name]
        display_name = agent_config.display_name or agent_name
        display_names.append(display_name)

    # Buffers keyed by display names (Agno emits display name as agent_name)
    per_member: dict[str, str] = dict.fromkeys(display_names, "")
    consensus: str = ""
    tool_trace: list[ToolTraceEntry] = []

    logger.info(f"Team streaming setup - agents: {agent_names}, display names: {display_names}")

    # Acquire raw event stream
    raw_stream = await team_response_stream_raw(
        agent_ids=agent_ids,
        mode=mode,
        message=message,
        orchestrator=orchestrator,
        thread_history=thread_history,
        model_name=model_name,
    )

    async for event in raw_stream:
        # Handle error case
        if isinstance(event, RunOutput):
            content = _get_response_content(event)
            if NO_AGENTS_RESPONSE in content:
                yield content
                return
            logger.warning(f"Unexpected RunOutput in team stream: {content[:100]}")
            continue

        # Individual agent response event
        elif isinstance(event, AgentRunContentEvent):
            agent_name = event.agent_name
            if agent_name:
                content = str(event.content or "")
                if agent_name not in per_member:
                    per_member[agent_name] = ""
                per_member[agent_name] += content

        # Agent tool call started
        elif isinstance(event, AgentToolCallStartedEvent):
            agent_name = event.agent_name
            tool_msg, trace_entry = format_tool_started_event(event.tool)
            if agent_name and tool_msg:
                if agent_name not in per_member:
                    per_member[agent_name] = ""
                per_member[agent_name] += tool_msg
            if trace_entry:
                tool_trace.append(trace_entry)

        # Agent tool call completed
        elif isinstance(event, AgentToolCallCompletedEvent):
            agent_name = event.agent_name
            info = extract_tool_completed_info(event.tool)
            if agent_name and info:
                tool_name, result = info
                if agent_name not in per_member:
                    per_member[agent_name] = ""
                per_member[agent_name], trace_entry = complete_pending_tool_block(
                    per_member[agent_name],
                    tool_name,
                    result,
                )
                tool_trace.append(trace_entry)

        # Team consensus content event
        elif isinstance(event, TeamRunContentEvent):
            if event.content:
                consensus += str(event.content)
            else:
                logger.debug("Empty team consensus event received")

        # Team-level tool call events (no specific agent context)
        elif isinstance(event, TeamToolCallStartedEvent):
            tool_msg, trace_entry = format_tool_started_event(event.tool)
            if tool_msg:
                consensus += tool_msg
            if trace_entry:
                tool_trace.append(trace_entry)

        elif isinstance(event, TeamToolCallCompletedEvent):
            info = extract_tool_completed_info(event.tool)
            if info:
                tool_name, result = info
                consensus, trace_entry = complete_pending_tool_block(consensus, tool_name, result)
                tool_trace.append(trace_entry)

        # Skip other event types
        else:
            logger.debug(f"Ignoring event type: {type(event).__name__}")
            continue

        parts: list[str] = []

        # First render configured agents (display names) in order
        for display in display_names:
            body = per_member.get(display, "").strip()
            if body:
                parts.append(format_member_contribution(display, body))
        # Then render any late/unknown agents that appeared during stream
        for display, body in per_member.items():
            if display not in display_names and body.strip():
                parts.append(format_member_contribution(display, body.strip()))

        if consensus.strip():
            parts.extend(format_team_consensus(consensus.strip()))
        elif parts:
            parts.append(format_no_consensus_note())

        if parts:
            header = format_team_header(agent_names)
            full_text = "\n\n".join(parts)
            yield StructuredStreamChunk(content=header + full_text, tool_trace=tool_trace.copy())

================================================================================
# FILE: src/mindroom/thread_utils.py
================================================================================

"""Utilities for thread analysis and agent detection."""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

from .constants import ROUTER_AGENT_NAME
from .matrix.identity import MatrixID, extract_agent_name
from .matrix.rooms import resolve_room_aliases

if TYPE_CHECKING:
    import nio

    from .config import Config


def check_agent_mentioned(event_source: dict, agent_id: MatrixID | None, config: Config) -> tuple[list[MatrixID], bool]:
    """Check if an agent is mentioned in a message.

    Returns (mentioned_agents, am_i_mentioned).
    """
    mentions = event_source.get("content", {}).get("m.mentions", {})
    mentioned_agents = get_mentioned_agents(mentions, config)
    am_i_mentioned = agent_id in mentioned_agents
    return mentioned_agents, am_i_mentioned


def create_session_id(room_id: str, thread_id: str | None) -> str:
    """Create a session ID with thread awareness."""
    # Thread sessions include thread ID
    return f"{room_id}:{thread_id}" if thread_id else room_id


def get_agents_in_thread(thread_history: list[dict[str, Any]], config: Config) -> list[MatrixID]:
    """Get list of unique agents that have participated in thread.

    Note: Router agent is excluded from the participant list as it's not
    a conversation participant.

    Preserves the order of first participation while preventing duplicates.
    """
    agents: list[MatrixID] = []
    seen_ids: set[str] = set()

    for msg in thread_history:
        sender: str = msg.get("sender", "")
        agent_name = extract_agent_name(sender, config)

        # Skip router agent and invalid senders
        if not agent_name or agent_name == ROUTER_AGENT_NAME:
            continue

        if sender not in seen_ids:
            try:
                matrix_id = MatrixID.parse(sender)
                agents.append(matrix_id)
                seen_ids.add(sender)
            except ValueError:
                # Skip invalid Matrix IDs
                pass

    return agents


def get_agent_matrix_ids_in_thread(thread_history: list[dict[str, Any]], config: Config) -> list[MatrixID]:
    """Get list of unique agent Matrix IDs that have participated in thread.

    Note: Router agent is excluded from the participant list as it's not
    a conversation participant.

    Preserves the order of first participation while preventing duplicates.

    Returns:
        List of MatrixID objects for agents who participated in the thread.

    """
    agent_ids = []
    seen_ids = set()

    for msg in thread_history:
        sender = msg.get("sender", "")
        agent_name = extract_agent_name(sender, config)

        # Skip router agent and invalid senders
        if not agent_name or agent_name == ROUTER_AGENT_NAME:
            continue

        try:
            matrix_id = MatrixID.parse(sender)
            if matrix_id.full_id not in seen_ids:
                agent_ids.append(matrix_id)
                seen_ids.add(matrix_id.full_id)
        except ValueError:
            # Skip invalid Matrix IDs
            pass

    return agent_ids


def get_mentioned_agents(mentions: dict[str, Any], config: Config) -> list[MatrixID]:
    """Extract agent names from mentions."""
    user_ids = mentions.get("user_ids", [])
    agents: list[MatrixID] = []

    for user_id in user_ids:
        mid = MatrixID.parse(user_id)
        if mid.agent_name(config):
            agents.append(mid)

    return agents


def has_user_responded_after_message(
    thread_history: list[dict],
    target_event_id: str,
    user_id: MatrixID,
) -> bool:
    """Check if a user has sent any messages after a specific message in the thread.

    Args:
        thread_history: List of messages in the thread
        target_event_id: The event ID to check after
        user_id: The user ID to check for

    Returns:
        True if the user has responded after the target message

    """
    # Find the target message and check for user responses after it
    found_target = False
    for msg in thread_history:
        if msg["event_id"] == target_event_id:
            found_target = True
        elif found_target and msg["sender"] == user_id.full_id:
            return True
    return False


def get_available_agents_in_room(room: nio.MatrixRoom, config: Config) -> list[MatrixID]:
    """Get list of available agent MatrixIDs in a room.

    Note: Router agent is excluded as it's not a regular conversation participant.
    """
    agents: list[MatrixID] = []

    for member_id in room.users:
        mid = MatrixID.parse(member_id)
        agent_name = mid.agent_name(config)
        # Exclude router agent
        if agent_name and agent_name != ROUTER_AGENT_NAME:
            agents.append(mid)

    return sorted(agents, key=lambda x: x.full_id)


def get_available_agent_matrix_ids_in_room(room: nio.MatrixRoom, config: Config) -> list[MatrixID]:
    """Get list of available agent Matrix IDs in a room.

    Note: Router agent is excluded as it's not a regular conversation participant.

    Returns:
        List of MatrixID objects for agents in the room.

    """
    agent_ids = []

    for member_id in room.users:
        agent_name = extract_agent_name(member_id, config)
        # Exclude router agent
        if agent_name and agent_name != ROUTER_AGENT_NAME:
            try:
                matrix_id = MatrixID.parse(member_id)
                agent_ids.append(matrix_id)
            except ValueError:
                # Skip invalid Matrix IDs
                pass

    return sorted(agent_ids, key=lambda x: x.full_id)


def get_configured_agents_for_room(room_id: str, config: Config) -> list[MatrixID]:
    """Get list of agent MatrixIDs configured for a specific room.

    This returns only agents that have the room in their configuration,
    not just agents that happen to be present in the room.

    Note: Router agent is excluded as it's not a regular conversation participant.
    """
    configured_agents: list[MatrixID] = []

    # Check which agents should be in this room
    for agent_name, agent_config in config.agents.items():
        if agent_name != ROUTER_AGENT_NAME:
            resolved_rooms = resolve_room_aliases(agent_config.rooms)
            if room_id in resolved_rooms:
                configured_agents.append(config.ids[agent_name])

    return sorted(configured_agents, key=lambda x: x.full_id)


def has_any_agent_mentions_in_thread(thread_history: list[dict[str, Any]], config: Config) -> bool:
    """Check if any agents are mentioned anywhere in the thread."""
    for msg in thread_history:
        content = msg.get("content", {})
        mentions = content.get("m.mentions", {})
        if get_mentioned_agents(mentions, config):
            return True
    return False


def get_all_mentioned_agents_in_thread(thread_history: list[dict[str, Any]], config: Config) -> list[MatrixID]:
    """Get all unique agent MatrixIDs that have been mentioned anywhere in the thread.

    Preserves the order of first mention while preventing duplicates.
    """
    mentioned_agents = []
    seen_ids = set()

    for msg in thread_history:
        content = msg.get("content", {})
        mentions = content.get("m.mentions", {})
        agents = get_mentioned_agents(mentions, config)

        # Add agents in order, but only if not seen before
        for agent in agents:
            if agent.full_id not in seen_ids:
                mentioned_agents.append(agent)
                seen_ids.add(agent.full_id)

    return mentioned_agents


def is_authorized_sender(sender_id: str, config: Config, room_id: str) -> bool:
    """Check if a sender is authorized to interact with agents.

    Args:
        sender_id: Matrix ID of the message sender
        config: Application configuration
        room_id: Room ID for permission checks

    Returns:
        True if the sender is authorized, False otherwise

    """
    # Always allow mindroom_user on the current domain
    mindroom_user_id = f"@mindroom_user:{config.domain}"
    if sender_id == mindroom_user_id:
        return True

    # Check if sender is an agent or team
    agent_name = extract_agent_name(sender_id, config)
    if agent_name:
        # Agent is either in config.agents, config.teams, or is the router
        return agent_name in config.agents or agent_name in config.teams or agent_name == ROUTER_AGENT_NAME

    # Check global authorized users (they have access to all rooms)
    if sender_id in config.authorization.global_users:
        return True

    # Check room-specific permissions
    if room_id in config.authorization.room_permissions:
        return sender_id in config.authorization.room_permissions[room_id]

    # Use default access for rooms not explicitly configured
    return config.authorization.default_room_access


def should_agent_respond(
    agent_name: str,
    am_i_mentioned: bool,
    is_thread: bool,
    room: nio.MatrixRoom,
    thread_history: list[dict],
    config: Config,
    mentioned_agents: list[MatrixID] | None = None,
) -> bool:
    """Determine if an agent should respond to a message individually.

    Team formation is handled elsewhere - this just determines individual responses.

    Args:
        agent_name: Name of the agent checking if it should respond
        am_i_mentioned: Whether this specific agent is mentioned
        is_thread: Whether the message is in a thread
        room: The Matrix room object
        thread_history: History of messages in the thread
        config: Application configuration
        mentioned_agents: List of all agent MatrixIDs mentioned in the message

    """
    # Always respond if mentioned
    if am_i_mentioned:
        return True

    # Never respond if other agents are mentioned but not this one
    # (User explicitly wants a different agent)
    if mentioned_agents:
        return False

    # Non-thread messages: allow a single available agent to respond automatically
    # This applies to both DM and regular rooms. Router is excluded from availability.
    if not is_thread:
        available_agents = get_available_agents_in_room(room, config)
        return len(available_agents) == 1

    agent_matrix_id = config.ids[agent_name]

    # For threads, check if agents have already participated
    if is_thread:
        agents_in_thread = get_agents_in_thread(thread_history, config)
        if agents_in_thread:
            # Continue only if we're the single agent
            return len(agents_in_thread) == 1 and agents_in_thread[0] == agent_matrix_id

    # No agents in thread yet OR DM room without thread
    # Respond if we're the only agent available
    available_agents = get_available_agents_in_room(room, config)
    return len(available_agents) == 1

================================================================================
# FILE: src/mindroom/tool_events.py
================================================================================

"""Tool-event formatting and metadata helpers for Matrix messages."""

from __future__ import annotations

import json
import re
from dataclasses import dataclass
from html import escape
from typing import TYPE_CHECKING, Literal

from agno.models.response import ToolExecution  # noqa: TC002 - used in isinstance checks

if TYPE_CHECKING:
    from collections.abc import Sequence

TOOL_TRACE_KEY = "io.mindroom.tool_trace"
TOOL_TRACE_VERSION = 1

MAX_TOOL_ARGS_PREVIEW_CHARS = 1200
MAX_TOOL_ARG_VALUE_PREVIEW_CHARS = 250
MAX_TOOL_RESULT_DISPLAY_CHARS = 500
MAX_TOOL_TRACE_EVENTS = 120


@dataclass(slots=True)
class ToolTraceEntry:
    """Normalized representation of a tool event for message metadata."""

    type: Literal["tool_call_started", "tool_call_completed"]
    tool_name: str
    args_preview: str | None = None
    result_preview: str | None = None
    truncated: bool = False


@dataclass(slots=True)
class StructuredStreamChunk:
    """Streaming chunk that carries fully-rendered content plus structured metadata."""

    content: str
    tool_trace: list[ToolTraceEntry] | None = None


def _to_compact_text(value: object) -> str:
    if isinstance(value, str):
        return value
    try:
        return json.dumps(value, ensure_ascii=False, sort_keys=True)
    except TypeError:
        return str(value)


def _truncate(text: str, limit: int) -> tuple[str, bool]:
    if len(text) <= limit:
        return text, False
    if limit <= 1:
        return "â€¦", True
    return f"{text[: limit - 1]}â€¦", True


def _neutralize_mentions(text: str) -> str:
    # Avoid accidental mentions being parsed out of tool arguments/results.
    return text.replace("@", "@\u200b")


def _format_tool_args(tool_args: dict[str, object]) -> tuple[str, bool]:
    parts: list[str] = []
    truncated = False
    # Preserve insertion order for easier debugging of tool-call construction.
    for key, value in tool_args.items():
        value_text = _to_compact_text(value)
        # Collapse newlines so the call line stays single-line.  This is
        # critical: complete_pending_tool_block uses "\n" inside a <tool>
        # block to distinguish pending (call-only) from completed (call+result).
        value_text = value_text.replace("\n", " ")
        value_preview, value_truncated = _truncate(value_text, MAX_TOOL_ARG_VALUE_PREVIEW_CHARS)
        if value_truncated:
            truncated = True
        parts.append(f"{key}={value_preview}")

    args_preview, args_truncated = _truncate(", ".join(parts), MAX_TOOL_ARGS_PREVIEW_CHARS)
    return args_preview, truncated or args_truncated


def format_tool_started(tool_name: str, tool_args: dict[str, object]) -> tuple[str, ToolTraceEntry]:
    """Format a tool-call start marker and return associated trace metadata."""
    if tool_args:
        args_preview, truncated = _format_tool_args(tool_args)
        call_display = f"{tool_name}({args_preview})"
        trace = ToolTraceEntry(
            type="tool_call_started",
            tool_name=tool_name,
            args_preview=args_preview,
            truncated=truncated,
        )
    else:
        call_display = f"{tool_name}()"
        trace = ToolTraceEntry(type="tool_call_started", tool_name=tool_name)

    safe_display = escape(_neutralize_mentions(call_display))
    return f"\n\n<tool>{safe_display}</tool>\n", trace


def format_tool_combined(
    tool_name: str,
    tool_args: dict[str, object],
    result: object | None,
) -> tuple[str, ToolTraceEntry]:
    """Format a complete tool call (call + result) as a single <tool> block."""
    if tool_args:
        args_preview, truncated = _format_tool_args(tool_args)
        call_display = f"{tool_name}({args_preview})"
    else:
        args_preview = ""
        truncated = False
        call_display = f"{tool_name}()"

    result_display = ""
    if result is not None and result != "":
        result_text = _to_compact_text(result)
        result_display, result_truncated = _truncate(result_text, MAX_TOOL_RESULT_DISPLAY_CHARS)
        truncated = truncated or result_truncated

    safe_call = escape(_neutralize_mentions(call_display))
    if result_display:
        safe_result = escape(_neutralize_mentions(result_display))
        block = f"\n\n<tool>{safe_call}\n{safe_result}</tool>\n"
    else:
        # Trailing \n signals "completed" to the frontend (no-result tool).
        # Without it the block looks identical to a pending/in-progress block.
        block = f"\n\n<tool>{safe_call}\n</tool>\n"

    trace = ToolTraceEntry(
        type="tool_call_completed",
        tool_name=tool_name,
        args_preview=args_preview or None,
        result_preview=result_display or None,
        truncated=truncated,
    )
    return block, trace


def complete_pending_tool_block(
    accumulated_text: str,
    tool_name: str,
    result: object | None,
) -> tuple[str, ToolTraceEntry]:
    """Find the last pending <tool> block for tool_name and inject the result.

    Returns (updated_text, trace_entry).
    If no pending block is found, appends a new combined block instead.
    """
    result_display = ""
    truncated = False
    if result is not None and result != "":
        result_text = _to_compact_text(result)
        result_display, truncated = _truncate(result_text, MAX_TOOL_RESULT_DISPLAY_CHARS)

    safe_result = escape(_neutralize_mentions(result_display)) if result_display else ""

    # Search backwards for the last <tool>...tool_name(...</tool> without a newline
    # (a newline inside means it already has a result).
    safe_tool_name = re.escape(escape(_neutralize_mentions(tool_name)))
    pattern = re.compile(rf"<tool>({safe_tool_name}\([^<]*)</tool>")
    matches = list(pattern.finditer(accumulated_text))

    updated = accumulated_text
    for match in reversed(matches):
        inner = match.group(1)
        # Pending blocks have no newline (just the call). Completed blocks have \n.
        if "\n" not in inner:
            # Always inject \n to mark as completed, even when result is empty.
            replacement = f"<tool>{inner}\n{safe_result}</tool>"
            updated = updated[: match.start()] + replacement + updated[match.end() :]
            break
    else:
        # No pending block found â€” append a standalone completed block
        if safe_result:
            escaped_name = escape(_neutralize_mentions(tool_name))
            updated += f"<tool>{escaped_name}\n{safe_result}</tool>\n"

    trace = ToolTraceEntry(
        type="tool_call_completed",
        tool_name=tool_name,
        result_preview=result_display or None,
        truncated=truncated,
    )
    return updated, trace


def format_tool_started_event(tool: ToolExecution | None) -> tuple[str, ToolTraceEntry | None]:
    """Format an Agno tool-call start into display text and trace metadata."""
    if tool is None:
        return "", None
    tool_name = tool.tool_name or "tool"
    tool_args = {str(k): v for k, v in tool.tool_args.items()} if isinstance(tool.tool_args, dict) else {}
    text, trace = format_tool_started(tool_name, tool_args)
    return text, trace


def extract_tool_completed_info(tool: ToolExecution | None) -> tuple[str, str | None] | None:
    """Extract tool name and result from a ToolExecution.

    Returns (tool_name, result) or None if tool is absent.
    Uses ``tool.result`` (actual tool output), not ``event.content``
    which Agno sets to a timing string like ``"tool() completed in 0.12s"``.
    """
    if tool is None:
        return None
    tool_name = tool.tool_name or "tool"
    return tool_name, tool.result


def build_tool_trace_content(tool_trace: Sequence[ToolTraceEntry] | None) -> dict[str, object] | None:
    """Build message content payload for tool trace metadata."""
    if not tool_trace:
        return None

    trace_list = list(tool_trace)
    overflow = max(0, len(trace_list) - MAX_TOOL_TRACE_EVENTS)
    if overflow:
        trace_list = trace_list[-MAX_TOOL_TRACE_EVENTS:]

    events: list[dict[str, object]] = []
    has_truncated_content = False
    for entry in trace_list:
        event: dict[str, object] = {
            "type": entry.type,
            "tool_name": entry.tool_name,
        }
        if entry.args_preview is not None:
            event["args_preview"] = entry.args_preview
        if entry.result_preview is not None:
            event["result_preview"] = entry.result_preview
        if entry.truncated:
            event["truncated"] = True
            has_truncated_content = True
        events.append(event)

    payload: dict[str, object] = {
        "version": TOOL_TRACE_VERSION,
        "events": events,
    }
    if overflow:
        payload["events_truncated"] = overflow
    if has_truncated_content:
        payload["content_truncated"] = True

    return {TOOL_TRACE_KEY: payload}

================================================================================
# FILE: src/mindroom/tools/__init__.py
================================================================================

"""Tools registry for all available Agno tools.

This module provides a centralized registry for all tools that can be used by agents.
Tools are registered by string name and can be instantiated dynamically when loading agents.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

from . import memory as _memory_registration  # noqa: F401
from .agentql import agentql_tools
from .airflow import airflow_tools
from .apify import apify_tools
from .arxiv import arxiv_tools
from .aws_lambda import aws_lambda_tools
from .aws_ses import aws_ses_tools
from .baidusearch import baidusearch_tools
from .bitbucket import bitbucket_tools
from .brandfetch import brandfetch_tools
from .bravesearch import bravesearch_tools
from .brightdata import brightdata_tools
from .browserbase import browserbase_tools
from .cal_com import cal_com_tools
from .calculator import calculator_tools
from .cartesia import cartesia_tools
from .clickup import clickup_tools
from .composio import composio_tools
from .config_manager import config_manager_tools
from .confluence import confluence_tools
from .crawl4ai import crawl4ai_tools
from .csv import csv_tools
from .custom_api import custom_api_tools
from .dalle import dalle_tools
from .daytona import daytona_tools
from .desi_vocal import desi_vocal_tools
from .discord import discord_tools
from .docker import docker_tools
from .duckdb import duckdb_tools
from .duckduckgo import duckduckgo_tools
from .e2b import e2b_tools
from .eleven_labs import eleven_labs_tools
from .email import email_tools
from .exa import exa_tools
from .fal import fal_tools
from .file import file_tools
from .file_generation import file_generation_tools
from .financial_datasets_api import financial_datasets_api_tools
from .firecrawl import firecrawl_tools
from .gemini import gemini_tools
from .giphy import giphy_tools
from .github import github_tools
from .gmail import gmail_tools
from .google_bigquery import google_bigquery_tools
from .google_calendar import google_calendar_tools
from .google_maps import google_maps_tools
from .google_sheets import google_sheets_tools
from .googlesearch import googlesearch_tools
from .groq import groq_tools
from .hackernews import hackernews_tools
from .jina import jina_tools
from .jira import jira_tools
from .linear import linear_tools
from .linkup import linkup_tools
from .lumalabs import lumalabs_tools
from .mem0 import mem0_tools
from .modelslabs import modelslabs_tools
from .moviepy_video_tools import moviepy_video_tools
from .neo4j import neo4j_tools
from .newspaper4k import newspaper4k_tools
from .notion import notion_tools
from .openai import openai_tools
from .openbb import openbb_tools
from .openweather import openweather_tools
from .oxylabs import oxylabs_tools
from .pandas import pandas_tools
from .postgres import postgres_tools
from .pubmed import pubmed_tools
from .python import python_tools
from .reasoning import reasoning_tools
from .reddit import reddit_tools
from .redshift import redshift_tools
from .replicate import replicate_tools
from .resend import resend_tools
from .scrapegraph import scrapegraph_tools
from .searxng import searxng_tools
from .serpapi import serpapi_tools
from .serper import serper_tools
from .shell import shell_tools
from .shopify import shopify_tools
from .slack import slack_tools
from .sleep import sleep_tools
from .spider import spider_tools
from .spotify import spotify_tools
from .sql import sql_tools
from .tavily import tavily_tools
from .telegram import telegram_tools
from .todoist import todoist_tools
from .trafilatura import trafilatura_tools
from .trello import trello_tools
from .twilio import twilio_tools
from .unsplash import unsplash_tools
from .visualization import visualization_tools
from .web_browser_tools import web_browser_tools
from .webex import webex_tools
from .website import website_tools
from .whatsapp import whatsapp_tools
from .wikipedia import wikipedia_tools
from .x import x_tools
from .yfinance import yfinance_tools
from .youtube import youtube_tools
from .zendesk import zendesk_tools
from .zep import zep_tools
from .zoom import zoom_tools

if TYPE_CHECKING:
    from agno.tools import Toolkit


__all__ = [
    "agentql_tools",
    "airflow_tools",
    "apify_tools",
    "arxiv_tools",
    "aws_lambda_tools",
    "aws_ses_tools",
    "baidusearch_tools",
    "bitbucket_tools",
    "brandfetch_tools",
    "bravesearch_tools",
    "brightdata_tools",
    "browserbase_tools",
    "cal_com_tools",
    "calculator_tools",
    "cartesia_tools",
    "clickup_tools",
    "composio_tools",
    "config_manager_tools",
    "confluence_tools",
    "crawl4ai_tools",
    "csv_tools",
    "custom_api_tools",
    "dalle_tools",
    "daytona_tools",
    "desi_vocal_tools",
    "discord_tools",
    "docker_tools",
    "duckdb_tools",
    "duckduckgo_tools",
    "e2b_tools",
    "eleven_labs_tools",
    "email_tools",
    "exa_tools",
    "fal_tools",
    "file_generation_tools",
    "file_tools",
    "financial_datasets_api_tools",
    "firecrawl_tools",
    "gemini_tools",
    "giphy_tools",
    "github_tools",
    "gmail_tools",
    "google_bigquery_tools",
    "google_calendar_tools",
    "google_maps_tools",
    "google_sheets_tools",
    "googlesearch_tools",
    "groq_tools",
    "hackernews_tools",
    "jina_tools",
    "jira_tools",
    "linear_tools",
    "linkup_tools",
    "lumalabs_tools",
    "mem0_tools",
    "modelslabs_tools",
    "moviepy_video_tools",
    "neo4j_tools",
    "newspaper4k_tools",
    "notion_tools",
    "openai_tools",
    "openbb_tools",
    "openweather_tools",
    "oxylabs_tools",
    "pandas_tools",
    "postgres_tools",
    "pubmed_tools",
    "python_tools",
    "reasoning_tools",
    "reddit_tools",
    "redshift_tools",
    "replicate_tools",
    "resend_tools",
    "scrapegraph_tools",
    "searxng_tools",
    "serpapi_tools",
    "serper_tools",
    "shell_tools",
    "shopify_tools",
    "slack_tools",
    "sleep_tools",
    "spider_tools",
    "spotify_tools",
    "sql_tools",
    "tavily_tools",
    "telegram_tools",
    "todoist_tools",
    "trafilatura_tools",
    "trello_tools",
    "twilio_tools",
    "unsplash_tools",
    "visualization_tools",
    "web_browser_tools",
    "webex_tools",
    "website_tools",
    "whatsapp_tools",
    "wikipedia_tools",
    "x_tools",
    "yfinance_tools",
    "youtube_tools",
    "zendesk_tools",
    "zep_tools",
    "zoom_tools",
]


@register_tool_with_metadata(
    name="homeassistant",
    display_name="Home Assistant",
    description="Control and monitor smart home devices",
    category=ToolCategory.SMART_HOME,
    icon="Home",
    icon_color="text-blue-500",
    dependencies=["httpx"],
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    config_fields=[
        ConfigField(
            name="HOMEASSISTANT_URL",
            label="Home Assistant URL",
            type="url",
            required=True,
            placeholder="http://homeassistant.local:8123",
            description="URL to your Home Assistant instance",
        ),
        ConfigField(
            name="HOMEASSISTANT_TOKEN",
            label="Access Token",
            type="password",
            required=True,
            placeholder="Bearer token",
            description="Long-lived access token from Home Assistant",
        ),
    ],
    docs_url="https://www.home-assistant.io/integrations/",
)
def homeassistant_tools() -> type[Toolkit]:
    """Return Home Assistant tools for smart home control."""
    from mindroom.custom_tools.homeassistant import HomeAssistantTools

    return HomeAssistantTools


# Coming Soon Tools - These are planned integrations that are not yet implemented
# They raise NotImplementedError but provide metadata for the UI


@register_tool_with_metadata(
    name="outlook",
    display_name="Microsoft Outlook",
    description="Email and calendar integration",
    category=ToolCategory.EMAIL,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="FaMicrosoft",
    icon_color="text-blue-600",
)
def outlook_tools() -> type[Toolkit]:
    """Outlook integration - coming soon."""
    msg = "Outlook integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="yahoo_mail",
    display_name="Yahoo Mail",
    description="Email and calendar access",
    category=ToolCategory.EMAIL,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="FaYahoo",
    icon_color="text-purple-600",
)
def yahoo_mail_tools() -> type[Toolkit]:
    """Yahoo Mail integration - coming soon."""
    msg = "Yahoo Mail integration is coming soon"
    raise NotImplementedError(msg)


# Shopping integrations (coming soon)
@register_tool_with_metadata(
    name="amazon",
    display_name="Amazon",
    description="Search products and track orders",
    category=ToolCategory.SHOPPING,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiAmazon",
    icon_color="text-orange-500",
)
def amazon_tools() -> type[Toolkit]:
    """Amazon integration - coming soon."""
    msg = "Amazon integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="walmart",
    display_name="Walmart",
    description="Product search and price tracking",
    category=ToolCategory.SHOPPING,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiWalmart",
    icon_color="text-blue-500",
)
def walmart_tools() -> type[Toolkit]:
    """Walmart integration - coming soon."""
    msg = "Walmart integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="ebay",
    display_name="eBay",
    description="Auction monitoring and bidding",
    category=ToolCategory.SHOPPING,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiEbay",
    icon_color="text-blue-500",  # eBay blue
)
def ebay_tools() -> type[Toolkit]:
    """EBay integration - coming soon."""
    msg = "eBay integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="target",
    display_name="Target",
    description="Product search and availability",
    category=ToolCategory.SHOPPING,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiTarget",
    icon_color="text-red-600",
)
def target_tools() -> type[Toolkit]:
    """Target integration - coming soon."""
    msg = "Target integration is coming soon"
    raise NotImplementedError(msg)


# Entertainment integrations (coming soon)
@register_tool_with_metadata(
    name="netflix",
    display_name="Netflix",
    description="Track watch history and get recommendations",
    category=ToolCategory.ENTERTAINMENT,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiNetflix",
    icon_color="text-red-600",
)
def netflix_tools() -> type[Toolkit]:
    """Netflix integration - coming soon."""
    msg = "Netflix integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="apple_music",
    display_name="Apple Music",
    description="Library and playlist management",
    category=ToolCategory.ENTERTAINMENT,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiApplemusic",
    icon_color="text-gray-800",
)
def apple_music_tools() -> type[Toolkit]:
    """Apple Music integration - coming soon."""
    msg = "Apple Music integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="hbo",
    display_name="HBO Max",
    description="Watch history and content discovery",
    category=ToolCategory.ENTERTAINMENT,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiHbo",
    icon_color="text-purple-600",  # HBO purple
)
def hbo_tools() -> type[Toolkit]:
    """HBO Max integration - coming soon."""
    msg = "HBO Max integration is coming soon"
    raise NotImplementedError(msg)


# Social media integrations (coming soon)
@register_tool_with_metadata(
    name="facebook",
    display_name="Facebook",
    description="Access posts and pages",
    category=ToolCategory.SOCIAL,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiFacebook",
    icon_color="text-blue-600",
)
def facebook_tools() -> type[Toolkit]:
    """Facebook integration - coming soon."""
    msg = "Facebook integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="instagram",
    display_name="Instagram",
    description="View posts and stories",
    category=ToolCategory.SOCIAL,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiInstagram",
    icon_color="text-pink-600",
)
def instagram_tools() -> type[Toolkit]:
    """Instagram integration - coming soon."""
    msg = "Instagram integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="linkedin",
    display_name="LinkedIn",
    description="Professional network access",
    category=ToolCategory.SOCIAL,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiLinkedin",
    icon_color="text-blue-700",
)
def linkedin_tools() -> type[Toolkit]:
    """LinkedIn integration - coming soon."""
    msg = "LinkedIn integration is coming soon"
    raise NotImplementedError(msg)


# Development tools (coming soon)
@register_tool_with_metadata(
    name="gitlab",
    display_name="GitLab",
    description="Code and CI/CD management",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiGitlab",
    icon_color="text-orange-600",
)
def gitlab_tools() -> type[Toolkit]:
    """GitLab integration - coming soon."""
    msg = "GitLab integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="dropbox",
    display_name="Dropbox",
    description="File storage and sharing",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiDropbox",
    icon_color="text-blue-600",
)
def dropbox_tools() -> type[Toolkit]:
    """Dropbox integration - coming soon."""
    msg = "Dropbox integration is coming soon"
    raise NotImplementedError(msg)


# Information tools (coming soon)
@register_tool_with_metadata(
    name="goodreads",
    display_name="Goodreads",
    description="Book tracking and recommendations",
    category=ToolCategory.INFORMATION,
    status=ToolStatus.COMING_SOON,
    setup_type=SetupType.COMING_SOON,
    icon="SiGoodreads",
    icon_color="text-amber-700",
)
def goodreads_tools() -> type[Toolkit]:
    """Goodreads integration - coming soon."""
    msg = "Goodreads integration is coming soon"
    raise NotImplementedError(msg)


@register_tool_with_metadata(
    name="imdb",
    display_name="IMDb",
    description="Movie and TV show information",
    category=ToolCategory.ENTERTAINMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="Film",
    icon_color="text-yellow-500",
    config_fields=[
        ConfigField(
            name="OMDB_API_KEY",
            label="OMDb API Key",
            type="password",
            required=True,
            placeholder="Enter your OMDb API key",
            description="Your OMDb API key for movie and TV show information",
        ),
    ],
    helper_text="Get a free API key from [OMDb API website](http://www.omdbapi.com/apikey.aspx)",
    docs_url="http://www.omdbapi.com/",
)
def imdb_tools() -> type[Toolkit]:
    """IMDb integration - coming soon."""
    msg = "IMDb integration is coming soon"
    raise NotImplementedError(msg)

================================================================================
# FILE: src/mindroom/tools/agentql.py
================================================================================

"""AgentQL tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.agentql import AgentQLTools


@register_tool_with_metadata(
    name="agentql",
    display_name="AgentQL",
    description="AI-powered web scraping and data extraction from websites",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaSpider",
    icon_color="text-purple-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_scrape_website",
            label="Enable Scrape Website",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_custom_scrape_website",
            label="Enable Custom Scrape Website",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="agentql_query",
            label="Agentql Query",
            type="text",
            required=False,
            default="",
        ),
    ],
    dependencies=["agentql", "playwright"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/agentql",
)
def agentql_tools() -> type[AgentQLTools]:
    """Return AgentQL tools for AI-powered web scraping."""
    from agno.tools.agentql import AgentQLTools

    return AgentQLTools

================================================================================
# FILE: src/mindroom/tools/airflow.py
================================================================================

"""Airflow tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.airflow import AirflowTools


@register_tool_with_metadata(
    name="airflow",
    display_name="Airflow",
    description="Apache Airflow DAG file management for workflow orchestration",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaCog",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="dags_dir",
            label="Dags Dir",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_save_dag_file",
            label="Enable Save Dag File",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_read_dag_file",
            label="Enable Read Dag File",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=[],  # No additional dependencies required beyond agno
    docs_url="https://docs.agno.com/tools/toolkits/others/airflow",
)
def airflow_tools() -> type[AirflowTools]:
    """Return Airflow tools for DAG file management."""
    from agno.tools.airflow import AirflowTools

    return AirflowTools

================================================================================
# FILE: src/mindroom/tools/apify.py
================================================================================

"""Apify tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.apify import ApifyTools


@register_tool_with_metadata(
    name="apify",
    display_name="Apify",
    description="Web scraping, crawling, data extraction, and web automation platform with ready-to-use Actors",
    category=ToolCategory.DEVELOPMENT,  # Based on agno docs URL path 'others/'
    status=ToolStatus.REQUIRES_CONFIG,  # Requires API token
    setup_type=SetupType.API_KEY,  # Uses API token authentication
    icon="FaCode",  # Web scraping/automation icon
    icon_color="text-blue-600",  # Apify brand color
    config_fields=[
        # Authentication
        ConfigField(
            name="apify_api_token",
            label="API Token",
            type="password",
            required=False,
            placeholder="apify_api_...",
            description="Apify API token for authentication (can also be set via APIFY_API_TOKEN env var)",
        ),
        # Configuration
        ConfigField(
            name="actors",
            label="Actors",
            type="text",
            required=False,
            placeholder="apify/rag-web-browser,compass/crawler-google-places",
            description="Single Actor ID as string or comma-separated list of Actor IDs to register as individual tools (e.g., 'apify/rag-web-browser' for web content extraction)",
        ),
    ],
    dependencies=["apify-client"],  # Required dependency for Apify integration
    docs_url="https://docs.agno.com/tools/toolkits/others/apify",
)
def apify_tools() -> type[ApifyTools]:
    """Return Apify tools for web scraping and automation."""
    from agno.tools.apify import ApifyTools

    return ApifyTools

================================================================================
# FILE: src/mindroom/tools/arxiv.py
================================================================================

"""ArXiv tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.arxiv import ArxivTools


@register_tool_with_metadata(
    name="arxiv",
    display_name="ArXiv",
    description="Search and read academic papers from ArXiv",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="SiArxiv",
    icon_color="text-red-600",  # ArXiv red
    config_fields=[
        ConfigField(
            name="enable_search_arxiv",
            label="Enable Search Arxiv",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_read_arxiv_papers",
            label="Enable Read Arxiv Papers",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="download_dir",
            label="Download Dir",
            type="text",
            required=False,
            default=None,
        ),
    ],
    dependencies=["arxiv", "pypdf"],
    docs_url="https://docs.agno.com/tools/toolkits/search/arxiv",
)
def arxiv_tools() -> type[ArxivTools]:
    """Return ArXiv tools for academic paper research."""
    from agno.tools.arxiv import ArxivTools

    return ArxivTools

================================================================================
# FILE: src/mindroom/tools/aws_lambda.py
================================================================================

"""AWS Lambda tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.aws_lambda import AWSLambdaTools


@register_tool_with_metadata(
    name="aws_lambda",
    display_name="AWS Lambda",
    description="Serverless function management and execution",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaAws",
    icon_color="text-orange-500",
    config_fields=[
        ConfigField(
            name="region_name",
            label="Region Name",
            type="text",
            required=False,
            default="us-east-1",
        ),
        ConfigField(
            name="enable_list_functions",
            label="Enable List Functions",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_invoke_function",
            label="Enable Invoke Function",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["boto3"],
    docs_url="https://docs.agno.com/tools/toolkits/others/aws_lambda",
)
def aws_lambda_tools() -> type[AWSLambdaTools]:
    """Return AWS Lambda tools for serverless function management."""
    from agno.tools.aws_lambda import AWSLambdaTools

    return AWSLambdaTools

================================================================================
# FILE: src/mindroom/tools/aws_ses.py
================================================================================

"""AWS SES tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.aws_ses import AWSSESTool


@register_tool_with_metadata(
    name="aws_ses",
    display_name="AWS SES",
    description="Send emails using Amazon Simple Email Service",
    category=ToolCategory.EMAIL,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaAws",
    icon_color="text-orange-500",
    config_fields=[
        ConfigField(
            name="sender_email",
            label="Sender Email",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="sender_name",
            label="Sender Name",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="region_name",
            label="Region Name",
            type="text",
            required=False,
            default="us-east-1",
        ),
        ConfigField(
            name="enable_send_email",
            label="Enable Send Email",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["boto3"],
    docs_url="https://docs.agno.com/tools/toolkits/others/aws_ses",
)
def aws_ses_tools() -> type[AWSSESTool]:
    """Return AWS SES tools for sending emails."""
    from agno.tools.aws_ses import AWSSESTool

    return AWSSESTool

================================================================================
# FILE: src/mindroom/tools/baidusearch.py
================================================================================

"""BaiduSearch tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.baidusearch import BaiduSearchTools


@register_tool_with_metadata(
    name="baidusearch",
    display_name="Baidu Search",
    description="Search the web using Baidu search engine with Chinese language support",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="Search",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="fixed_max_results",
            label="Fixed Max Results",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="fixed_language",
            label="Fixed Language",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="headers",
            label="Headers",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="proxy",
            label="Proxy",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=10,
        ),
        ConfigField(
            name="debug",
            label="Debug",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_baidu_search",
            label="Enable Baidu Search",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["baidusearch", "pycountry"],
    docs_url="https://docs.agno.com/tools/toolkits/search/baidusearch",
)
def baidusearch_tools() -> type[BaiduSearchTools]:
    """Return Baidu search tools for web search."""
    from agno.tools.baidusearch import BaiduSearchTools

    return BaiduSearchTools

================================================================================
# FILE: src/mindroom/tools/bitbucket.py
================================================================================

"""Bitbucket tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.bitbucket import BitbucketTools


@register_tool_with_metadata(
    name="bitbucket",
    display_name="Bitbucket",
    description="Manage Bitbucket repositories, pull requests, commits, and issues",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    icon="SiBitbucket",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="username",
            label="Username",
            type="text",
            required=True,
            placeholder="Bitbucket username",
            description="Bitbucket username (falls back to BITBUCKET_USERNAME env var)",
        ),
        ConfigField(
            name="password",
            label="App Password",
            type="password",
            required=False,
            description="App password (falls back to BITBUCKET_PASSWORD env var). Use either this or token.",
        ),
        ConfigField(
            name="token",
            label="Token",
            type="password",
            required=False,
            description="Access token (falls back to BITBUCKET_TOKEN env var). Use either this or password.",
        ),
        ConfigField(
            name="workspace",
            label="Workspace",
            type="text",
            required=True,
            placeholder="my-workspace",
        ),
        ConfigField(
            name="repo_slug",
            label="Repository Slug",
            type="text",
            required=True,
            placeholder="my-repo",
        ),
        ConfigField(
            name="server_url",
            label="Server URL",
            type="url",
            required=False,
            default="api.bitbucket.org",
        ),
        ConfigField(
            name="api_version",
            label="API Version",
            type="text",
            required=False,
            default="2.0",
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/others/bitbucket",
    helper_text="Create an app password at [Bitbucket Settings](https://bitbucket.org/account/settings/app-passwords/)",
)
def bitbucket_tools() -> type[BitbucketTools]:
    """Return Bitbucket tools for repository management."""
    from agno.tools.bitbucket import BitbucketTools

    return BitbucketTools

================================================================================
# FILE: src/mindroom/tools/brandfetch.py
================================================================================

"""Brandfetch tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.brandfetch import BrandfetchTools


@register_tool_with_metadata(
    name="brandfetch",
    display_name="Brandfetch",
    description="Retrieve brand data including logos, colors, and fonts by domain or name",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaPalette",
    icon_color="text-blue-500",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            description="Brandfetch API key for Brand API (falls back to BRANDFETCH_API_KEY env var)",
        ),
        ConfigField(
            name="client_id",
            label="Client ID",
            type="text",
            required=False,
            description="Brandfetch Client ID for Brand Search API (falls back to BRANDFETCH_CLIENT_ID env var)",
        ),
        ConfigField(
            name="enable_search_by_identifier",
            label="Enable Search by Identifier",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_by_brand",
            label="Enable Search by Brand",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="base_url",
            label="Base URL",
            type="url",
            required=False,
            default="https://api.brandfetch.io/v2",
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=20.0,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="async_tools",
            label="Async Tools (Deprecated)",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["httpx"],
    docs_url="https://docs.agno.com/tools/toolkits/others/brandfetch",
    helper_text="Get API keys from [Brandfetch Developers](https://developers.brandfetch.com/)",
)
def brandfetch_tools() -> type[BrandfetchTools]:
    """Return Brandfetch tools for brand data retrieval."""
    from agno.tools.brandfetch import BrandfetchTools

    return BrandfetchTools

================================================================================
# FILE: src/mindroom/tools/bravesearch.py
================================================================================

"""Brave Search tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.bravesearch import BraveSearchTools


@register_tool_with_metadata(
    name="bravesearch",
    display_name="Brave Search",
    description="Search the web using Brave Search API",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiBrave",
    icon_color="text-orange-500",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=True,
            placeholder="Brave Search API key",
            description="API key from Brave Search",
        ),
        ConfigField(
            name="fixed_max_results",
            label="Fixed Max Results",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="fixed_language",
            label="Fixed Language",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["brave-search"],
    docs_url="https://docs.agno.com/tools/toolkits/search/bravesearch",
    helper_text="Get a free API key from [Brave Search API](https://brave.com/search/api/)",
)
def bravesearch_tools() -> type[BraveSearchTools]:
    """Return Brave Search tools for web search."""
    from agno.tools.bravesearch import BraveSearchTools

    return BraveSearchTools

================================================================================
# FILE: src/mindroom/tools/brightdata.py
================================================================================

"""BrightData tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.brightdata import BrightDataTools


@register_tool_with_metadata(
    name="brightdata",
    display_name="BrightData",
    description="Web scraping, search engine queries, screenshots, and structured data extraction",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaSpider",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_scrape_markdown",
            label="Enable Scrape Markdown",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_screenshot",
            label="Enable Screenshot",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_engine",
            label="Enable Search Engine",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_web_data_feed",
            label="Enable Web Data Feed",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="serp_zone",
            label="Serp Zone",
            type="text",
            required=False,
            default="serp_api",
        ),
        ConfigField(
            name="web_unlocker_zone",
            label="Web Unlocker Zone",
            type="text",
            required=False,
            default="web_unlocker1",
        ),
        ConfigField(
            name="verbose",
            label="Verbose",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=600,
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/brightdata",
)
def brightdata_tools() -> type[BrightDataTools]:
    """Return BrightData tools for web scraping and data extraction."""
    from agno.tools.brightdata import BrightDataTools

    return BrightDataTools

================================================================================
# FILE: src/mindroom/tools/browserbase.py
================================================================================

"""Browserbase tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.browserbase import BrowserbaseTools


@register_tool_with_metadata(
    name="browserbase",
    display_name="Browserbase",
    description="Browser automation and web scraping using headless browsers",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaChrome",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="project_id",
            label="Project ID",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="base_url",
            label="Base URL",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_navigate_to",
            label="Enable Navigate To",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_screenshot",
            label="Enable Screenshot",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_page_content",
            label="Enable Get Page Content",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_close_session",
            label="Enable Close Session",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="parse_html",
            label="Parse Html",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="max_content_length",
            label="Max Content Length",
            type="number",
            required=False,
            default=100000,
        ),
    ],
    dependencies=["browserbase", "playwright"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/browserbase",
)
def browserbase_tools() -> type[BrowserbaseTools]:
    """Return Browserbase tools for browser automation and web scraping."""
    from agno.tools.browserbase import BrowserbaseTools

    return BrowserbaseTools

================================================================================
# FILE: src/mindroom/tools/cal_com.py
================================================================================

"""Cal.com tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.calcom import CalComTools


@register_tool_with_metadata(
    name="cal_com",
    display_name="Cal.com",
    description="Calendar scheduling and booking management",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaCalendarAlt",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="event_type_id",
            label="Event Type ID",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="user_timezone",
            label="User Timezone",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_get_available_slots",
            label="Enable Get Available Slots",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_create_booking",
            label="Enable Create Booking",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_upcoming_bookings",
            label="Enable Get Upcoming Bookings",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_reschedule_booking",
            label="Enable Reschedule Booking",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_cancel_booking",
            label="Enable Cancel Booking",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["requests", "pytz"],
    docs_url="https://docs.agno.com/tools/toolkits/others/calcom",
)
def cal_com_tools() -> type[CalComTools]:
    """Return Cal.com tools for calendar scheduling and booking management."""
    from agno.tools.calcom import CalComTools

    return CalComTools

================================================================================
# FILE: src/mindroom/tools/calculator.py
================================================================================

"""Calculator tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.calculator import CalculatorTools


@register_tool_with_metadata(
    name="calculator",
    display_name="Calculator",
    description="Mathematical calculator with basic and advanced operations",
    category=ToolCategory.DEVELOPMENT,  # Local tool
    status=ToolStatus.AVAILABLE,  # No config needed
    setup_type=SetupType.NONE,  # No authentication required
    icon="Calculator",  # React icon name
    icon_color="text-blue-500",  # Tailwind color class
    config_fields=[],
    dependencies=["agno"],  # From agno requirements
    docs_url="https://docs.agno.com/tools/toolkits/local/calculator",
)
def calculator_tools() -> type[CalculatorTools]:
    """Return calculator tools for mathematical operations."""
    from agno.tools.calculator import CalculatorTools

    return CalculatorTools

================================================================================
# FILE: src/mindroom/tools/cartesia.py
================================================================================

"""Cartesia tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.cartesia import CartesiaTools


@register_tool_with_metadata(
    name="cartesia",
    display_name="Cartesia",
    description="Voice AI services including text-to-speech and voice localization",
    category=ToolCategory.DEVELOPMENT,  # others/ â†’ DEVELOPMENT according to mapping
    status=ToolStatus.REQUIRES_CONFIG,  # requires API key
    setup_type=SetupType.API_KEY,  # API key authentication
    icon="VolumeX",  # Voice/sound related icon
    icon_color="text-purple-500",  # Purple for voice AI
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="model_id",
            label="Model ID",
            type="text",
            required=False,
            default="sonic-2",
        ),
        ConfigField(
            name="default_voice_id",
            label="Default Voice ID",
            type="text",
            required=False,
            default="78ab82d5-25be-4f7d-82b3-7ad64e5b85b2",
        ),
        ConfigField(
            name="enable_text_to_speech",
            label="Enable Text To Speech",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_list_voices",
            label="Enable List Voices",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_localize_voice",
            label="Enable Localize Voice",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["cartesia"],
    docs_url="https://docs.agno.com/tools/toolkits/others/cartesia",
)
def cartesia_tools() -> type[CartesiaTools]:
    """Return Cartesia tools for voice AI services."""
    from agno.tools.cartesia import CartesiaTools

    return CartesiaTools

================================================================================
# FILE: src/mindroom/tools/clickup.py
================================================================================

"""ClickUp tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.clickup import ClickUpTools


@register_tool_with_metadata(
    name="clickup",
    display_name="ClickUp",
    description="Manage tasks, spaces, and lists in ClickUp project management",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiClickup",
    icon_color="text-purple-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=True,
            placeholder="ClickUp API key",
            description="API key from ClickUp (falls back to CLICKUP_API_KEY env var)",
        ),
        ConfigField(
            name="master_space_id",
            label="Master Space ID",
            type="text",
            required=True,
            placeholder="ClickUp space ID",
            description="ID of the master space to work with (falls back to MASTER_SPACE_ID env var)",
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/others/clickup",
    helper_text="Get your API key from [ClickUp Settings > Apps](https://app.clickup.com/settings/apps)",
)
def clickup_tools() -> type[ClickUpTools]:
    """Return ClickUp tools for project management."""
    from agno.tools.clickup import ClickUpTools

    return ClickUpTools

================================================================================
# FILE: src/mindroom/tools/composio.py
================================================================================

"""Composio tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from composio_agno import ComposioToolSet  # type: ignore[import-untyped]


@register_tool_with_metadata(
    name="composio",
    display_name="Composio",
    description="Access 1000+ integrations including Gmail, Salesforce, GitHub, and more",
    category=ToolCategory.INTEGRATIONS,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaConnectdevelop",
    icon_color="text-blue-600",
    config_fields=[
        # Authentication/Connection parameters first
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            placeholder="comp_...",
            description="Composio API key (can also be set via COMPOSIO_API_KEY env var)",
        ),
        ConfigField(
            name="base_url",
            label="Base URL",
            type="url",
            required=False,
            description="Base URL for Composio API (leave empty for default)",
        ),
        ConfigField(
            name="entity_id",
            label="Entity ID",
            type="text",
            required=False,
            default="default",
            placeholder="default",
            description="Entity identifier for Composio workspace",
        ),
        # Workspace Configuration
        ConfigField(
            name="workspace_id",
            label="Workspace ID",
            type="text",
            required=False,
            placeholder="workspace_123",
            description="Workspace identifier for organizing tools and data",
        ),
        ConfigField(
            name="workspace_config",
            label="Workspace Config",
            type="text",
            required=False,
            placeholder='{"type": "local"}',
            description="JSON configuration for workspace settings",
        ),
        # Connection Configuration
        ConfigField(
            name="connected_account_ids",
            label="Connected Account IDs",
            type="text",
            required=False,
            placeholder='{"github": "account_123"}',
            description="JSON mapping of app names to connected account IDs",
        ),
        # Advanced Configuration
        ConfigField(
            name="metadata",
            label="Metadata",
            type="text",
            required=False,
            placeholder='{"key": "value"}',
            description="JSON metadata for tools and actions configuration",
        ),
        ConfigField(
            name="processors",
            label="Processors",
            type="text",
            required=False,
            description="Custom processors configuration (JSON format)",
        ),
        ConfigField(
            name="output_dir",
            label="Output Directory",
            type="text",
            required=False,
            placeholder="/path/to/output",
            description="Directory path for output files",
        ),
        ConfigField(
            name="lockfile",
            label="Lock File Path",
            type="text",
            required=False,
            placeholder="/path/to/lockfile",
            description="Path to lock file for concurrency control",
        ),
        # Numerical Configuration
        ConfigField(
            name="max_retries",
            label="Max Retries",
            type="number",
            required=False,
            default=3,
            description="Maximum number of retries for failed operations",
        ),
        ConfigField(
            name="verbosity_level",
            label="Verbosity Level",
            type="number",
            required=False,
            placeholder="1",
            description="Logging verbosity level (0-3, higher = more verbose)",
        ),
        # Feature Flags
        ConfigField(
            name="output_in_file",
            label="Output in File",
            type="boolean",
            required=False,
            default=False,
            description="Enable file-based output for operations",
        ),
        ConfigField(
            name="allow_tracing",
            label="Allow Tracing",
            type="boolean",
            required=False,
            default=False,
            description="Enable operation tracing for debugging",
        ),
        ConfigField(
            name="lock",
            label="Enable Locking",
            type="boolean",
            required=False,
            default=True,
            description="Enable file locking for concurrent operations",
        ),
        # Logging Configuration
        ConfigField(
            name="logging_level",
            label="Logging Level",
            type="text",
            required=False,
            default="INFO",
            placeholder="INFO",
            description="Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)",
        ),
    ],
    dependencies=["composio-agno"],
    docs_url="https://docs.agno.com/tools/toolkits/others/composio",
)
def composio_tools() -> type[ComposioToolSet]:
    """Return Composio tools for accessing 1000+ integrations."""
    from composio_agno import ComposioToolSet

    return ComposioToolSet  # type: ignore[no-any-return]

================================================================================
# FILE: src/mindroom/tools/config_manager.py
================================================================================

"""Config Manager tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from mindroom.custom_tools.config_manager import ConfigManagerTools


@register_tool_with_metadata(
    name="config_manager",
    display_name="Config Manager",
    description="Build and manage MindRoom agents with expert knowledge of the system",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="Settings",
    icon_color="text-purple-500",
    config_fields=[
        ConfigField(
            name="config_path",
            label="Configuration Path",
            type="text",
            required=False,
            description="Path to the configuration file (uses default if not specified)",
        ),
    ],
    dependencies=["agno", "pydantic", "yaml"],
    docs_url="https://github.com/mindroom-ai/mindroom",
)
def config_manager_tools() -> type[ConfigManagerTools]:
    """Return config manager tools for agent building."""
    from mindroom.custom_tools.config_manager import ConfigManagerTools

    return ConfigManagerTools

================================================================================
# FILE: src/mindroom/tools/confluence.py
================================================================================

"""Confluence tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.confluence import ConfluenceTools


@register_tool_with_metadata(
    name="confluence",
    display_name="Confluence",
    description="Atlassian wiki platform for retrieving, creating, and updating pages",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiConfluence",
    icon_color="text-blue-600",
    config_fields=[
        # Authentication/Connection parameters
        ConfigField(
            name="url",
            label="Confluence URL",
            type="url",
            required=False,
            placeholder="https://your-confluence-instance.atlassian.net",
            description="Confluence instance URL (can also be set via CONFLUENCE_URL env var)",
        ),
        ConfigField(
            name="username",
            label="Username",
            type="text",
            required=False,
            placeholder="your-username",
            description="Confluence username (can also be set via CONFLUENCE_USERNAME env var)",
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=False,
            placeholder="your-password",
            description="Confluence password (can also be set via CONFLUENCE_PASSWORD env var)",
        ),
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            placeholder="your-api-key",
            description="Confluence API key - alternative to password (can also be set via CONFLUENCE_API_KEY env var)",
        ),
        # Configuration options
        ConfigField(
            name="verify_ssl",
            label="Verify SSL",
            type="boolean",
            required=False,
            default=True,
            description="Whether to verify SSL certificates when connecting to Confluence",
        ),
    ],
    dependencies=["atlassian-python-api"],
    docs_url="https://docs.agno.com/tools/toolkits/others/confluence",
)
def confluence_tools() -> type[ConfluenceTools]:
    """Return Confluence tools for wiki management."""
    from agno.tools.confluence import ConfluenceTools

    return ConfluenceTools

================================================================================
# FILE: src/mindroom/tools/crawl4ai.py
================================================================================

"""Crawl4AI tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.crawl4ai import Crawl4aiTools


@register_tool_with_metadata(
    name="crawl4ai",
    display_name="Crawl4AI",
    description="Web crawling and scraping using the Crawl4ai library",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaSpider",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="max_length",
            label="Max Length",
            type="number",
            required=False,
            default=5000,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=60,
        ),
        ConfigField(
            name="use_pruning",
            label="Use Pruning",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="pruning_threshold",
            label="Pruning Threshold",
            type="number",
            required=False,
            default=0.48,
        ),
        ConfigField(
            name="bm25_threshold",
            label="Bm25 Threshold",
            type="number",
            required=False,
            default=1.0,
        ),
        ConfigField(
            name="headless",
            label="Headless",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="wait_until",
            label="Wait Until",
            type="text",
            required=False,
            default="domcontentloaded",
        ),
        ConfigField(
            name="proxy_config",
            label="Proxy Config",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_crawl",
            label="Enable Crawl",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["crawl4ai"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/crawl4ai",
)
def crawl4ai_tools() -> type[Crawl4aiTools]:
    """Return Crawl4AI tools for web crawling and scraping."""
    from agno.tools.crawl4ai import Crawl4aiTools

    return Crawl4aiTools

================================================================================
# FILE: src/mindroom/tools/csv.py
================================================================================

"""CSV toolkit tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.csv_toolkit import CsvTools


@register_tool_with_metadata(
    name="csv",
    display_name="CSV Toolkit",
    description="CSV file analysis and querying with SQL support",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaFileCsv",
    icon_color="text-green-600",
    config_fields=[
        ConfigField(
            name="csvs",
            label="Csvs",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="row_limit",
            label="Row Limit",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="duckdb_connection",
            label="Duckdb Connection",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="duckdb_kwargs",
            label="Duckdb Kwargs",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_read_csv_file",
            label="Enable Read Csv File",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_list_csv_files",
            label="Enable List Csv Files",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_columns",
            label="Enable Get Columns",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_query_csv_file",
            label="Enable Query Csv File",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["duckdb"],
    docs_url="https://docs.agno.com/tools/toolkits/database/csv",
)
def csv_tools() -> type[CsvTools]:
    """Return CSV toolkit for data analysis and querying."""
    from agno.tools.csv_toolkit import CsvTools

    return CsvTools

================================================================================
# FILE: src/mindroom/tools/custom_api.py
================================================================================

"""Custom API tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.api import CustomApiTools


@register_tool_with_metadata(
    name="custom_api",
    display_name="Custom API",
    description="Make HTTP requests to any external API with customizable authentication and parameters",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="Globe",
    icon_color="text-blue-500",
    config_fields=[
        ConfigField(
            name="base_url",
            label="Base URL",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="username",
            label="Username",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="headers",
            label="Headers",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="verify_ssl",
            label="Verify Ssl",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=30,
        ),
        ConfigField(
            name="enable_make_request",
            label="Enable Make Request",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/others/custom_api",
)
def custom_api_tools() -> type[CustomApiTools]:
    """Return Custom API tools for making HTTP requests to external APIs."""
    from agno.tools.api import CustomApiTools

    return CustomApiTools

================================================================================
# FILE: src/mindroom/tools/dalle.py
================================================================================

"""DALL-E tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.dalle import DalleTools


@register_tool_with_metadata(
    name="dalle",
    display_name="DALL-E",
    description="OpenAI DALL-E image generation from text prompts",
    category=ToolCategory.DEVELOPMENT,  # others/ maps to DEVELOPMENT
    status=ToolStatus.REQUIRES_CONFIG,  # Requires API key
    setup_type=SetupType.API_KEY,  # Uses OpenAI API key
    icon="FaImage",  # React icon for image generation
    icon_color="text-green-600",  # OpenAI brand color
    config_fields=[
        ConfigField(
            name="model",
            label="Model",
            type="text",
            required=False,
            default="dall-e-3",
        ),
        ConfigField(
            name="n",
            label="N",
            type="number",
            required=False,
            default=1,
        ),
        ConfigField(
            name="size",
            label="Size",
            type="text",
            required=False,
            default="1024x1024",
        ),
        ConfigField(
            name="quality",
            label="Quality",
            type="text",
            required=False,
            default="standard",
        ),
        ConfigField(
            name="style",
            label="Style",
            type="text",
            required=False,
            default="vivid",
        ),
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_create_image",
            label="Enable Create Image",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["openai"],  # OpenAI Python package
    docs_url="https://docs.agno.com/tools/toolkits/others/dalle",  # URL without .md extension
)
def dalle_tools() -> type[DalleTools]:
    """Return DALL-E tools for image generation from text prompts."""
    from agno.tools.dalle import DalleTools

    return DalleTools

================================================================================
# FILE: src/mindroom/tools/daytona.py
================================================================================

"""Daytona tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.daytona import DaytonaTools


@register_tool_with_metadata(
    name="daytona",
    display_name="Daytona",
    description="Execute code in secure, remote sandbox environments",
    category=ToolCategory.DEVELOPMENT,  # others/ maps to DEVELOPMENT
    status=ToolStatus.REQUIRES_CONFIG,  # Requires API key
    setup_type=SetupType.API_KEY,  # Uses API key authentication
    icon="FaTerminal",  # Terminal icon for code execution
    icon_color="text-blue-600",  # Blue color for development tools
    config_fields=[
        # Authentication parameters
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            placeholder="dt_...",
            description="Daytona API key (can also be set via DAYTONA_API_KEY env var)",
        ),
        ConfigField(
            name="api_url",
            label="API URL",
            type="url",
            required=False,
            placeholder="https://api.daytona.io",
            description="Daytona API URL (can also be set via DAYTONA_API_URL env var)",
        ),
        # Sandbox configuration
        ConfigField(
            name="sandbox_id",
            label="Sandbox ID",
            type="text",
            required=False,
            placeholder="sandbox-123",
            description="Specific sandbox ID to use. If None, creates or uses persistent sandbox",
        ),
        ConfigField(
            name="sandbox_language",
            label="Sandbox Language",
            type="text",
            required=False,
            default="PYTHON",
            placeholder="PYTHON",
            description="Primary language for the sandbox (PYTHON, JAVASCRIPT, TYPESCRIPT)",
        ),
        ConfigField(
            name="sandbox_target",
            label="Sandbox Target",
            type="text",
            required=False,
            placeholder="target-config",
            description="Target configuration for the sandbox",
        ),
        ConfigField(
            name="sandbox_os",
            label="Sandbox OS",
            type="text",
            required=False,
            placeholder="ubuntu-20.04",
            description="Operating system for the sandbox",
        ),
        ConfigField(
            name="auto_stop_interval",
            label="Auto Stop Interval",
            type="number",
            required=False,
            default=60,
            description="Auto-stop interval in minutes (0 to disable)",
        ),
        ConfigField(
            name="sandbox_os_user",
            label="Sandbox OS User",
            type="text",
            required=False,
            placeholder="daytona",
            description="OS user for the sandbox",
        ),
        ConfigField(
            name="sandbox_env_vars",
            label="Sandbox Environment Variables",
            type="text",
            required=False,
            placeholder='{"ENV_VAR": "value"}',
            description="Environment variables for the sandbox (JSON format)",
        ),
        ConfigField(
            name="sandbox_labels",
            label="Sandbox Labels",
            type="text",
            required=False,
            default="{}",
            placeholder='{"label": "value"}',
            description="Labels for the sandbox (JSON format)",
        ),
        ConfigField(
            name="organization_id",
            label="Organization ID",
            type="text",
            required=False,
            placeholder="org-123",
            description="Organization ID for the sandbox",
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=300,
            description="Timeout for sandbox operations in seconds",
        ),
        # Feature flags
        ConfigField(
            name="auto_create_sandbox",
            label="Auto Create Sandbox",
            type="boolean",
            required=False,
            default=True,
            description="Automatically create sandbox if none exists",
        ),
        ConfigField(
            name="verify_ssl",
            label="Verify SSL",
            type="boolean",
            required=False,
            default=False,
            description="Whether to verify SSL certificates",
        ),
        ConfigField(
            name="persistent",
            label="Persistent",
            type="boolean",
            required=False,
            default=True,
            description="Whether to reuse the same sandbox across agent sessions",
        ),
        ConfigField(
            name="sandbox_public",
            label="Sandbox Public",
            type="boolean",
            required=False,
            description="Whether the sandbox should be public",
        ),
        # Custom instructions
        ConfigField(
            name="instructions",
            label="Instructions",
            type="text",
            required=False,
            placeholder="Custom guidelines for the toolkit",
            description="Custom instructions for the toolkit",
        ),
        ConfigField(
            name="add_instructions",
            label="Add Instructions",
            type="boolean",
            required=False,
            default=False,
            description="Whether to add instructions to the agent",
        ),
    ],
    dependencies=["daytona"],
    docs_url="https://docs.agno.com/tools/toolkits/others/daytona",
)
def daytona_tools() -> type[DaytonaTools]:
    """Return Daytona tools for secure code execution in remote sandbox environments."""
    from agno.tools.daytona import DaytonaTools

    return DaytonaTools

================================================================================
# FILE: src/mindroom/tools/desi_vocal.py
================================================================================

"""DesiVocal tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.desi_vocal import DesiVocalTools


@register_tool_with_metadata(
    name="desi_vocal",
    display_name="DesiVocal",
    description="Hindi and Indian language text-to-speech synthesis",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaMicrophone",
    icon_color="text-orange-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=True,
            description="DesiVocal API key (falls back to DESI_VOCAL_API_KEY env var)",
        ),
        ConfigField(
            name="voice_id",
            label="Voice ID",
            type="text",
            required=False,
            default="f27d74e5-ea71-4697-be3e-f04bbd80c1a8",
            description="Default voice ID to use for speech synthesis",
        ),
        ConfigField(
            name="enable_get_voices",
            label="Enable Get Voices",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_text_to_speech",
            label="Enable Text to Speech",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/others/desi_vocal",
    helper_text="Get an API key from [DesiVocal](https://desivocal.com/)",
)
def desi_vocal_tools() -> type[DesiVocalTools]:
    """Return DesiVocal tools for text-to-speech."""
    from agno.tools.desi_vocal import DesiVocalTools

    return DesiVocalTools

================================================================================
# FILE: src/mindroom/tools/discord.py
================================================================================

"""Discord tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.discord import DiscordTools


@register_tool_with_metadata(
    name="discord",
    display_name="Discord",
    description="Tool for interacting with Discord channels and servers",
    category=ToolCategory.COMMUNICATION,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiDiscord",
    icon_color="text-indigo-500",  # Discord brand color
    config_fields=[
        ConfigField(
            name="bot_token",
            label="Bot Token",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_send_message",
            label="Enable Send Message",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_channel_messages",
            label="Enable Get Channel Messages",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_channel_info",
            label="Enable Get Channel Info",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_list_channels",
            label="Enable List Channels",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_delete_message",
            label="Enable Delete Message",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/social/discord",
)
def discord_tools() -> type[DiscordTools]:
    """Return Discord tools for interacting with Discord channels and servers."""
    from agno.tools.discord import DiscordTools

    return DiscordTools

================================================================================
# FILE: src/mindroom/tools/docker.py
================================================================================

"""Docker tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.docker import DockerTools


@register_tool_with_metadata(
    name="docker",
    display_name="Docker",
    description="Container, image, volume, and network management",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="SiDocker",
    icon_color="text-blue-500",
    config_fields=[],
    dependencies=["docker"],
    docs_url="https://docs.agno.com/tools/toolkits/local/docker",
)
def docker_tools() -> type[DockerTools]:
    """Return Docker tools for container management."""
    from agno.tools.docker import DockerTools

    return DockerTools

================================================================================
# FILE: src/mindroom/tools/duckdb.py
================================================================================

"""DuckDB tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.duckdb import DuckDbTools


@register_tool_with_metadata(
    name="duckdb",
    display_name="DuckDB",
    description="In-memory analytical database for data processing and analysis",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="Database",
    icon_color="text-yellow-600",
    config_fields=[
        ConfigField(
            name="db_path",
            label="Db Path",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="connection",
            label="Connection",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="init_commands",
            label="Init Commands",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="read_only",
            label="Read Only",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="config",
            label="Config",
            type="text",
            required=False,
            default=None,
        ),
    ],
    dependencies=["duckdb"],
    docs_url="https://docs.agno.com/tools/toolkits/database/duckdb",
)
def duckdb_tools() -> type[DuckDbTools]:
    """Return DuckDB tools for data analysis and processing."""
    from agno.tools.duckdb import DuckDbTools

    return DuckDbTools

================================================================================
# FILE: src/mindroom/tools/duckduckgo.py
================================================================================

"""DuckDuckGo tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.duckduckgo import DuckDuckGoTools


@register_tool_with_metadata(
    name="duckduckgo",
    display_name="DuckDuckGo",
    description="Search engine for web search and news",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="SiDuckduckgo",
    icon_color="text-orange-500",  # DuckDuckGo orange
    config_fields=[
        ConfigField(
            name="enable_search",
            label="Enable Search",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_news",
            label="Enable News",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="modifier",
            label="Modifier",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="fixed_max_results",
            label="Fixed Max Results",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="proxy",
            label="Proxy",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=10,
        ),
        ConfigField(
            name="verify_ssl",
            label="Verify Ssl",
            type="boolean",
            required=False,
            default=True,
        ),
    ],
    dependencies=["ddgs"],
    docs_url="https://docs.agno.com/tools/toolkits/search/duckduckgo",
)
def duckduckgo_tools() -> type[DuckDuckGoTools]:
    """Return DuckDuckGo tools for web search and news."""
    from agno.tools.duckduckgo import DuckDuckGoTools

    return DuckDuckGoTools

================================================================================
# FILE: src/mindroom/tools/e2b.py
================================================================================

"""E2B code execution tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.e2b import E2BTools


@register_tool_with_metadata(
    name="e2b",
    display_name="E2B Code Execution",
    description="Code execution sandbox environment with Python, file operations, and web server capabilities",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="Terminal",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=300,
        ),
        ConfigField(
            name="sandbox_options",
            label="Sandbox Options",
            type="text",
            required=False,
            default=None,
        ),
    ],
    dependencies=["e2b_code_interpreter"],
    docs_url="https://docs.agno.com/tools/toolkits/others/e2b",
)
def e2b_tools() -> type[E2BTools]:
    """Return E2B code execution tools for secure sandbox environments."""
    from agno.tools.e2b import E2BTools

    return E2BTools

================================================================================
# FILE: src/mindroom/tools/eleven_labs.py
================================================================================

"""Eleven Labs tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.eleven_labs import ElevenLabsTools


@register_tool_with_metadata(
    name="eleven_labs",
    display_name="Eleven Labs",
    description="Text-to-speech and sound effect generation using AI voices",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiElevenlabs",
    icon_color="text-orange-500",
    config_fields=[
        ConfigField(
            name="voice_id",
            label="Voice ID",
            type="text",
            required=False,
            default="JBFqnCBsd6RMkjVDRZzb",
        ),
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="target_directory",
            label="Target Directory",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="model_id",
            label="Model ID",
            type="text",
            required=False,
            default="eleven_multilingual_v2",
        ),
        ConfigField(
            name="output_format",
            label="Output Format",
            type="text",
            required=False,
            default="mp3_44100_64",
        ),
        ConfigField(
            name="enable_get_voices",
            label="Enable Get Voices",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_generate_sound_effect",
            label="Enable Generate Sound Effect",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_text_to_speech",
            label="Enable Text To Speech",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["elevenlabs"],
    docs_url="https://docs.agno.com/tools/toolkits/others/eleven_labs",
)
def eleven_labs_tools() -> type[ElevenLabsTools]:
    """Return Eleven Labs tools for text-to-speech and sound effect generation."""
    from agno.tools.eleven_labs import ElevenLabsTools

    return ElevenLabsTools

================================================================================
# FILE: src/mindroom/tools/email.py
================================================================================

"""Email tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.email import EmailTools


@register_tool_with_metadata(
    name="email",
    display_name="Email",
    description="Send emails via SMTP (Gmail)",
    category=ToolCategory.EMAIL,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="Mail",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="receiver_email",
            label="Receiver Email",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="sender_name",
            label="Sender Name",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="sender_email",
            label="Sender Email",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="sender_passkey",
            label="Sender Passkey",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_email_user",
            label="Enable Email User",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=[],  # Uses built-in smtplib
    docs_url="https://docs.agno.com/tools/toolkits/social/email",
)
def email_tools() -> type[EmailTools]:
    """Return email tools for sending messages via SMTP."""
    from agno.tools.email import EmailTools

    return EmailTools

================================================================================
# FILE: src/mindroom/tools/exa.py
================================================================================

"""Exa tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.exa import ExaTools


@register_tool_with_metadata(
    name="exa",
    display_name="Exa",
    description="Advanced AI-powered web search engine for research and content discovery",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaSearch",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="enable_search",
            label="Enable Search",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_contents",
            label="Enable Get Contents",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_find_similar",
            label="Enable Find Similar",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_answer",
            label="Enable Answer",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_research",
            label="Enable Research",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="text",
            label="Text",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="text_length_limit",
            label="Text Length Limit",
            type="number",
            required=False,
            default=1000,
        ),
        ConfigField(
            name="summary",
            label="Summary",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="num_results",
            label="Num Results",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="livecrawl",
            label="Livecrawl",
            type="text",
            required=False,
            default="always",
        ),
        ConfigField(
            name="start_crawl_date",
            label="Start Crawl Date",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="end_crawl_date",
            label="End Crawl Date",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="start_published_date",
            label="Start Published Date",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="end_published_date",
            label="End Published Date",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="type",
            label="Type",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="category",
            label="Category",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="include_domains",
            label="Include Domains",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="exclude_domains",
            label="Exclude Domains",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="show_results",
            label="Show Results",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="model",
            label="Model",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=30,
        ),
        ConfigField(
            name="research_model",
            label="Research Model",
            type="text",
            required=False,
            default="exa-research",
        ),
    ],
    dependencies=["exa_py"],
    docs_url="https://docs.agno.com/tools/toolkits/search/exa",
)
def exa_tools() -> type[ExaTools]:
    """Return Exa tools for AI-powered web search and research."""
    from agno.tools.exa import ExaTools

    return ExaTools

================================================================================
# FILE: src/mindroom/tools/fal.py
================================================================================

"""Fal tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.fal import FalTools


@register_tool_with_metadata(
    name="fal",
    display_name="Fal",
    description="AI model serving platform for media generation (images and videos)",
    category=ToolCategory.DEVELOPMENT,  # others category maps to DEVELOPMENT
    status=ToolStatus.REQUIRES_CONFIG,  # requires FAL_KEY API key
    setup_type=SetupType.API_KEY,  # uses API key authentication
    icon="FaRobot",  # AI/robot icon for AI model serving
    icon_color="text-purple-600",  # Purple for AI/ML services
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="model",
            label="Model",
            type="text",
            required=False,
            default="fal-ai/hunyuan-video",
        ),
        ConfigField(
            name="enable_generate_media",
            label="Enable Generate Media",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_image_to_image",
            label="Enable Image To Image",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["fal-client"],  # From agno requirements
    docs_url="https://docs.agno.com/tools/toolkits/others/fal",  # URL without .md extension
)
def fal_tools() -> type[FalTools]:
    """Return Fal tools for AI model serving and media generation."""
    from agno.tools.fal import FalTools

    return FalTools

================================================================================
# FILE: src/mindroom/tools/file.py
================================================================================

"""File tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.file import FileTools


@register_tool_with_metadata(
    name="file",
    display_name="File Tools",
    description="Local file operations including read, write, list, and search",
    category=ToolCategory.DEVELOPMENT,  # Local tool
    status=ToolStatus.AVAILABLE,  # No config needed
    setup_type=SetupType.NONE,  # No authentication required
    icon="FaFolder",  # React icon name
    icon_color="text-yellow-500",  # Tailwind color class
    config_fields=[
        ConfigField(
            name="base_dir",
            label="Base Dir",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_save_file",
            label="Enable Save File",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_read_file",
            label="Enable Read File",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_delete_file",
            label="Enable Delete File",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_list_files",
            label="Enable List Files",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_files",
            label="Enable Search Files",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_read_file_chunk",
            label="Enable Read File Chunk",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_replace_file_chunk",
            label="Enable Replace File Chunk",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="expose_base_directory",
            label="Expose Base Directory",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="max_file_length",
            label="Max File Length",
            type="number",
            required=False,
            default=10000000,
        ),
        ConfigField(
            name="max_file_lines",
            label="Max File Lines",
            type="number",
            required=False,
            default=100000,
        ),
        ConfigField(
            name="line_separator",
            label="Line Separator",
            type="text",
            required=False,
            default="\n",
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["agno"],  # From agno requirements
    docs_url="https://docs.agno.com/tools/toolkits/local/file",
)
def file_tools() -> type[FileTools]:
    """Return file tools for local file operations."""
    from agno.tools.file import FileTools

    return FileTools

================================================================================
# FILE: src/mindroom/tools/file_generation.py
================================================================================

"""File generation tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.file_generation import FileGenerationTools


@register_tool_with_metadata(
    name="file_generation",
    display_name="File Generation",
    description="Generate JSON, CSV, PDF, and text files from data",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaFileExport",
    icon_color="text-green-600",
    config_fields=[
        ConfigField(
            name="output_directory",
            label="Output Directory",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_json_generation",
            label="Enable JSON Generation",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_csv_generation",
            label="Enable CSV Generation",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_pdf_generation",
            label="Enable PDF Generation",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_txt_generation",
            label="Enable TXT Generation",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["reportlab"],
    docs_url="https://docs.agno.com/tools/toolkits/others/file_generation",
)
def file_generation_tools() -> type[FileGenerationTools]:
    """Return File Generation tools for creating JSON, CSV, PDF, and text files."""
    from agno.tools.file_generation import FileGenerationTools

    return FileGenerationTools

================================================================================
# FILE: src/mindroom/tools/financial_datasets_api.py
================================================================================

"""Financial Datasets API tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.financial_datasets import FinancialDatasetsTools


@register_tool_with_metadata(
    name="financial_datasets_api",
    display_name="Financial Datasets API",
    description="Comprehensive financial data API for stocks, financial statements, SEC filings, and cryptocurrency",
    category=ToolCategory.DEVELOPMENT,  # From /tools/toolkits/others/ path
    status=ToolStatus.REQUIRES_CONFIG,  # Requires API key
    setup_type=SetupType.API_KEY,  # Uses API key authentication
    icon="TrendingUp",  # Financial/trending icon
    icon_color="text-green-600",  # Financial green color
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
    ],
    dependencies=["requests"],  # Only standard dependency needed
    docs_url="https://docs.agno.com/tools/toolkits/others/financial_datasets",
)
def financial_datasets_api_tools() -> type[FinancialDatasetsTools]:
    """Return Financial Datasets API tools for comprehensive financial data access."""
    from agno.tools.financial_datasets import FinancialDatasetsTools

    return FinancialDatasetsTools

================================================================================
# FILE: src/mindroom/tools/firecrawl.py
================================================================================

"""Firecrawl tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.firecrawl import FirecrawlTools


@register_tool_with_metadata(
    name="firecrawl",
    display_name="Firecrawl",
    description="Web scraping and crawling tool for extracting content from websites",
    category=ToolCategory.RESEARCH,  # Web scraping tool for research
    status=ToolStatus.REQUIRES_CONFIG,  # Requires API key
    setup_type=SetupType.API_KEY,  # API key authentication
    icon="FaSpider",  # Web crawler icon
    icon_color="text-orange-500",  # Orange color for fire/crawling theme
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_scrape",
            label="Enable Scrape",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_crawl",
            label="Enable Crawl",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_mapping",
            label="Enable Mapping",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_search",
            label="Enable Search",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="formats",
            label="Formats",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="limit",
            label="Limit",
            type="number",
            required=False,
            default=10,
        ),
        ConfigField(
            name="poll_interval",
            label="Poll Interval",
            type="number",
            required=False,
            default=30,
        ),
        ConfigField(
            name="search_params",
            label="Search Params",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="api_url",
            label="API URL",
            type="url",
            required=False,
            default="https://api.firecrawl.dev",
        ),
    ],
    dependencies=["firecrawl-py"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/firecrawl",
)
def firecrawl_tools() -> type[FirecrawlTools]:
    """Return Firecrawl tools for web scraping and crawling."""
    from agno.tools.firecrawl import FirecrawlTools

    return FirecrawlTools

================================================================================
# FILE: src/mindroom/tools/gemini.py
================================================================================

"""Gemini tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.models.gemini import GeminiTools


@register_tool_with_metadata(
    name="gemini",
    display_name="Gemini",
    description="Google AI API services for generating images and videos using Gemini models",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiGooglegemini",
    icon_color="text-blue-500",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="vertexai",
            label="Vertexai",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="project_id",
            label="Project ID",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="location",
            label="Location",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="image_generation_model",
            label="Image Generation Model",
            type="text",
            required=False,
            default="imagen-3.0-generate-002",
        ),
        ConfigField(
            name="video_generation_model",
            label="Video Generation Model",
            type="text",
            required=False,
            default="veo-2.0-generate-001",
        ),
        ConfigField(
            name="enable_generate_image",
            label="Enable Generate Image",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_generate_video",
            label="Enable Generate Video",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["google-genai"],
    docs_url="https://docs.agno.com/tools/toolkits/models/gemini",
)
def gemini_tools() -> type[GeminiTools]:
    """Return Gemini tools for image and video generation."""
    from agno.tools.models.gemini import GeminiTools

    return GeminiTools

================================================================================
# FILE: src/mindroom/tools/giphy.py
================================================================================

"""Giphy tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.giphy import GiphyTools


@register_tool_with_metadata(
    name="giphy",
    display_name="Giphy",
    description="GIF search and integration",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="GiGift",
    icon_color="text-purple-500",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="limit",
            label="Limit",
            type="number",
            required=False,
            default=1,
        ),
        ConfigField(
            name="enable_search_gifs",
            label="Enable Search Gifs",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["httpx"],
    docs_url="https://docs.agno.com/tools/toolkits/others/giphy",
)
def giphy_tools() -> type[GiphyTools]:
    """Return Giphy tools for GIF search and integration."""
    from agno.tools.giphy import GiphyTools

    return GiphyTools

================================================================================
# FILE: src/mindroom/tools/github.py
================================================================================

"""Tools registry for all available Agno tools.

This module provides a centralized registry for all tools that can be used by agents.
Tools are registered by string name and can be instantiated dynamically when loading agents.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.github import GithubTools


@register_tool_with_metadata(
    name="github",
    display_name="GitHub",
    description="Repository and issue management",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiGithub",
    icon_color="text-gray-800",  # GitHub black
    config_fields=[
        ConfigField(
            name="access_token",
            label="Access Token",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="base_url",
            label="Base URL",
            type="url",
            required=False,
            default=None,
        ),
    ],
    dependencies=["PyGithub"],
    docs_url="https://docs.agno.com/tools/toolkits/others/github",
)
def github_tools() -> type[GithubTools]:
    """Return GitHub tools for repository management."""
    from agno.tools.github import GithubTools

    return GithubTools

================================================================================
# FILE: src/mindroom/tools/gmail.py
================================================================================

"""Gmail tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from mindroom.custom_tools.gmail import GmailTools


@register_tool_with_metadata(
    name="gmail",
    display_name="Gmail",
    description="Read, search, and manage Gmail emails",
    category=ToolCategory.EMAIL,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    auth_provider="google",  # Authentication provided by Google Services integration
    icon="SiGmail",
    icon_color="text-red-500",
    config_fields=[
        ConfigField(
            name="get_latest_emails",
            label="Get Latest Emails",
            type="boolean",
            required=False,
            default=True,
            description="Allow retrieving the latest emails",
        ),
        ConfigField(
            name="get_emails_from_user",
            label="Get Emails From User",
            type="boolean",
            required=False,
            default=True,
            description="Allow retrieving emails from specific users",
        ),
        ConfigField(
            name="get_unread_emails",
            label="Get Unread Emails",
            type="boolean",
            required=False,
            default=True,
            description="Allow retrieving unread emails",
        ),
        ConfigField(
            name="get_starred_emails",
            label="Get Starred Emails",
            type="boolean",
            required=False,
            default=True,
            description="Allow retrieving starred emails",
        ),
        ConfigField(
            name="search_emails",
            label="Search Emails",
            type="boolean",
            required=False,
            default=True,
            description="Allow searching through emails",
        ),
        ConfigField(
            name="create_draft_email",
            label="Create Draft Emails",
            type="boolean",
            required=False,
            default=True,
            description="Allow creating draft emails",
        ),
        ConfigField(
            name="send_email",
            label="Send Emails",
            type="boolean",
            required=False,
            default=True,
            description="Allow sending emails",
        ),
        ConfigField(
            name="send_email_reply",
            label="Send Email Replies",
            type="boolean",
            required=False,
            default=True,
            description="Allow sending replies to emails",
        ),
    ],
    dependencies=["google-api-python-client", "google-auth", "google-auth-oauthlib", "google-auth-httplib2"],
    docs_url="https://docs.agno.com/tools/toolkits/social/gmail",
)
def gmail_tools() -> type[GmailTools]:
    """Return Gmail tools for email management."""
    from mindroom.custom_tools.gmail import GmailTools

    return GmailTools

================================================================================
# FILE: src/mindroom/tools/google_bigquery.py
================================================================================

"""Google BigQuery tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.google_bigquery import GoogleBigQueryTools


@register_tool_with_metadata(
    name="google_bigquery",
    display_name="Google BigQuery",
    description="Query Google BigQuery - list tables, describe schemas, and run SQL queries",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    icon="SiGooglebigquery",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="dataset",
            label="Dataset",
            type="text",
            required=True,
            placeholder="my_dataset",
            description="BigQuery dataset name",
        ),
        ConfigField(
            name="project",
            label="Project",
            type="text",
            required=True,
            placeholder="my-gcp-project",
            description="Google Cloud project ID (falls back to GOOGLE_CLOUD_PROJECT env var)",
        ),
        ConfigField(
            name="location",
            label="Location",
            type="text",
            required=True,
            placeholder="US",
            description="BigQuery location (falls back to GOOGLE_CLOUD_LOCATION env var)",
        ),
        ConfigField(
            name="credentials",
            label="Credentials",
            type="text",
            required=False,
            default=None,
            description="Google Cloud credentials object (optional, uses Application Default Credentials if not set)",
        ),
        ConfigField(
            name="enable_list_tables",
            label="Enable List Tables",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_describe_table",
            label="Enable Describe Table",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_run_sql_query",
            label="Enable Run SQL Query",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["google-cloud-bigquery"],
    docs_url="https://docs.agno.com/tools/toolkits/others/google_bigquery",
    helper_text="Requires Google Cloud credentials. Set up [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc)",
)
def google_bigquery_tools() -> type[GoogleBigQueryTools]:
    """Return Google BigQuery tools for data analytics."""
    from agno.tools.google_bigquery import GoogleBigQueryTools

    return GoogleBigQueryTools

================================================================================
# FILE: src/mindroom/tools/google_calendar.py
================================================================================

"""Google Calendar tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from mindroom.custom_tools.google_calendar import GoogleCalendarTools


@register_tool_with_metadata(
    name="google_calendar",
    display_name="Google Calendar",
    description="View and schedule meetings with Google Calendar",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    auth_provider="google",  # Authentication provided by Google Services integration
    icon="SiGooglecalendar",
    icon_color="text-blue-600",  # Google Calendar blue
    config_fields=[
        ConfigField(
            name="calendar_id",
            label="Calendar ID",
            type="text",
            required=False,
            default="primary",
            placeholder="primary",
            description="The Google Calendar ID to use (default: 'primary' for the user's main calendar)",
        ),
        ConfigField(
            name="allow_update",
            label="Allow Updates",
            type="boolean",
            required=False,
            default=False,
            description="Allow the agent to create, update, and delete calendar events",
        ),
    ],
    dependencies=["google-api-python-client", "google-auth", "google-auth-httplib2", "google-auth-oauthlib"],
    docs_url="https://docs.agno.com/tools/toolkits/others/googlecalendar",
)
def google_calendar_tools() -> type[GoogleCalendarTools]:
    """Return Google Calendar tools for calendar management."""
    from mindroom.custom_tools.google_calendar import GoogleCalendarTools

    return GoogleCalendarTools

================================================================================
# FILE: src/mindroom/tools/google_maps.py
================================================================================

"""Google Maps tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.google_maps import GoogleMapTools


@register_tool_with_metadata(
    name="google_maps",
    display_name="Google Maps",
    description="Tools for interacting with Google Maps services including place search, directions, geocoding, and more",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiGooglemaps",
    icon_color="text-red-500",
    config_fields=[
        ConfigField(
            name="key",
            label="Key",
            type="password",
            required=False,
            default=None,
        ),
    ],
    dependencies=["googlemaps", "google-maps-places"],
    docs_url="https://docs.agno.com/tools/toolkits/others/google_maps",
)
def google_maps_tools() -> type[GoogleMapTools]:
    """Return Google Maps tools for location services."""
    from agno.tools.google_maps import GoogleMapTools

    return GoogleMapTools

================================================================================
# FILE: src/mindroom/tools/google_sheets.py
================================================================================

"""Google Sheets tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from mindroom.custom_tools.google_sheets import GoogleSheetsTools


@register_tool_with_metadata(
    name="google_sheets",
    display_name="Google Sheets",
    description="Read, create, update, and duplicate Google Sheets spreadsheets",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    auth_provider="google",  # Authentication provided by Google Services integration
    icon="SiGooglesheets",
    icon_color="text-green-600",
    config_fields=[
        ConfigField(
            name="spreadsheet_id",
            label="Spreadsheet ID",
            type="text",
            required=False,
            placeholder="Leave empty to work with multiple spreadsheets",
            description="The ID of the Google Spreadsheet to work with. If not specified, you can work with multiple spreadsheets.",
        ),
        ConfigField(
            name="spreadsheet_range",
            label="Default Range",
            type="text",
            required=False,
            placeholder="e.g., Sheet1!A1:Z100",
            description="Default range to use for operations (optional)",
        ),
        ConfigField(
            name="read",
            label="Enable Read Operations",
            type="boolean",
            required=False,
            default=True,
            description="Allow reading data from spreadsheets",
        ),
        ConfigField(
            name="create",
            label="Enable Create Operations",
            type="boolean",
            required=False,
            default=False,
            description="Allow creating new spreadsheets",
        ),
        ConfigField(
            name="update",
            label="Enable Update Operations",
            type="boolean",
            required=False,
            default=False,
            description="Allow updating existing spreadsheets",
        ),
        ConfigField(
            name="duplicate",
            label="Enable Duplicate Operations",
            type="boolean",
            required=False,
            default=False,
            description="Allow duplicating spreadsheets",
        ),
    ],
    dependencies=["google-api-python-client", "google-auth-httplib2", "google-auth-oauthlib"],
    docs_url="https://docs.agno.com/tools/toolkits/others/google_sheets",
)
def google_sheets_tools() -> type[GoogleSheetsTools]:
    """Return Google Sheets tools for spreadsheet integration."""
    from mindroom.custom_tools.google_sheets import GoogleSheetsTools

    return GoogleSheetsTools

================================================================================
# FILE: src/mindroom/tools/googlesearch.py
================================================================================

"""Google Search tool configuration."""

from __future__ import annotations

from agno.tools.websearch import WebSearchTools

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)


class GoogleSearchTools(WebSearchTools):
    """Convenience wrapper for WebSearchTools with Google as the backend."""

    def __init__(
        self,
        enable_search: bool = True,
        enable_news: bool = True,
        modifier: str | None = None,
        fixed_max_results: int | None = None,
        proxy: str | None = None,
        timeout: int | None = 10,
        verify_ssl: bool = True,
        **kwargs: object,
    ) -> None:
        super().__init__(
            enable_search=enable_search,
            enable_news=enable_news,
            backend="google",
            modifier=modifier,
            fixed_max_results=fixed_max_results,
            proxy=proxy,
            timeout=timeout,
            verify_ssl=verify_ssl,
            **kwargs,
        )


@register_tool_with_metadata(
    name="googlesearch",
    display_name="Google Search",
    description="Search Google for web results using the WebSearch backend",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaGoogle",
    icon_color="text-blue-500",
    config_fields=[
        ConfigField(
            name="enable_search",
            label="Enable Search",
            type="text",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_news",
            label="Enable News",
            type="text",
            required=False,
            default=True,
        ),
        ConfigField(
            name="modifier",
            label="Modifier",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="fixed_max_results",
            label="Fixed Max Results",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="proxy",
            label="Proxy",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="text",
            required=False,
            default=10,
        ),
        ConfigField(
            name="verify_ssl",
            label="Verify Ssl",
            type="text",
            required=False,
            default=True,
        ),
    ],
    dependencies=["ddgs"],
    docs_url="https://docs.agno.com/tools/toolkits/search/websearch",
)
def googlesearch_tools() -> type[GoogleSearchTools]:
    """Return Google Search tools for web search."""
    return GoogleSearchTools

================================================================================
# FILE: src/mindroom/tools/groq.py
================================================================================

"""Groq tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.models.groq import GroqTools


@register_tool_with_metadata(
    name="groq",
    display_name="Groq",
    description="Fast AI inference for audio transcription, translation, and text-to-speech",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="TbBrain",
    icon_color="text-orange-500",  # Groq brand orange
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="transcription_model",
            label="Transcription Model",
            type="text",
            required=False,
            default="whisper-large-v3",
        ),
        ConfigField(
            name="translation_model",
            label="Translation Model",
            type="text",
            required=False,
            default="whisper-large-v3",
        ),
        ConfigField(
            name="tts_model",
            label="Tts Model",
            type="text",
            required=False,
            default="playai-tts",
        ),
        ConfigField(
            name="tts_voice",
            label="Tts Voice",
            type="text",
            required=False,
            default="Chip-PlayAI",
        ),
        ConfigField(
            name="enable_transcribe_audio",
            label="Enable Transcribe Audio",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_translate_audio",
            label="Enable Translate Audio",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_generate_speech",
            label="Enable Generate Speech",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["groq"],
    docs_url="https://docs.agno.com/tools/toolkits/models/groq",
)
def groq_tools() -> type[GroqTools]:
    """Return Groq AI tools for fast audio transcription, translation, and text-to-speech."""
    from agno.tools.models.groq import GroqTools

    return GroqTools

================================================================================
# FILE: src/mindroom/tools/hackernews.py
================================================================================

"""Hacker News tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.hackernews import HackerNewsTools


@register_tool_with_metadata(
    name="hackernews",
    display_name="Hacker News",
    description="Get top stories and user details from Hacker News",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaHackerNews",
    icon_color="text-orange-600",  # Hacker News orange
    config_fields=[
        ConfigField(
            name="enable_get_top_stories",
            label="Enable Get Top Stories",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_user_details",
            label="Enable Get User Details",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["httpx"],
    docs_url="https://docs.agno.com/tools/toolkits/search/hackernews",
)
def hackernews_tools() -> type[HackerNewsTools]:
    """Return Hacker News tools for getting stories and user details."""
    from agno.tools.hackernews import HackerNewsTools

    return HackerNewsTools

================================================================================
# FILE: src/mindroom/tools/jina.py
================================================================================

"""Jina Reader tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.jina import JinaReaderTools


@register_tool_with_metadata(
    name="jina",
    display_name="Jina Reader",
    description="Web content reading and search using Jina AI Reader API",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaGlobe",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="base_url",
            label="Base URL",
            type="url",
            required=False,
            default="https://r.jina.ai/",
        ),
        ConfigField(
            name="search_url",
            label="Search URL",
            type="url",
            required=False,
            default="https://s.jina.ai/",
        ),
        ConfigField(
            name="max_content_length",
            label="Max Content Length",
            type="number",
            required=False,
            default=10000,
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="search_query_content",
            label="Search Query Content",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_read_url",
            label="Enable Read URL",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_query",
            label="Enable Search Query",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["httpx", "pydantic"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/jina_reader",
)
def jina_tools() -> type[JinaReaderTools]:
    """Return Jina Reader tools for web content reading and search."""
    from agno.tools.jina import JinaReaderTools

    return JinaReaderTools

================================================================================
# FILE: src/mindroom/tools/jira.py
================================================================================

"""Jira tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.jira import JiraTools


@register_tool_with_metadata(
    name="jira",
    display_name="Jira",
    description="Issue tracking and project management",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiJira",
    icon_color="text-blue-600",  # Jira blue
    config_fields=[
        ConfigField(
            name="server_url",
            label="Server URL",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="username",
            label="Username",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="token",
            label="Token",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_get_issue",
            label="Enable Get Issue",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_create_issue",
            label="Enable Create Issue",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_issues",
            label="Enable Search Issues",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_add_comment",
            label="Enable Add Comment",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_add_worklog",
            label="Enable Add Worklog",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["jira"],
    docs_url="https://docs.agno.com/tools/toolkits/others/jira",
)
def jira_tools() -> type[JiraTools]:
    """Return Jira tools for issue tracking and project management."""
    from agno.tools.jira import JiraTools

    return JiraTools

================================================================================
# FILE: src/mindroom/tools/linear.py
================================================================================

"""Linear tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.linear import LinearTools


@register_tool_with_metadata(
    name="linear",
    display_name="Linear",
    description="Issue tracking and project management",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiLinear",
    icon_color="text-purple-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/others/linear",
)
def linear_tools() -> type[LinearTools]:
    """Return Linear tools for issue tracking and project management."""
    from agno.tools.linear import LinearTools

    return LinearTools

================================================================================
# FILE: src/mindroom/tools/linkup.py
================================================================================

"""Linkup tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.linkup import LinkupTools


@register_tool_with_metadata(
    name="linkup",
    display_name="Linkup",
    description="Web search using Linkup API for real-time information",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaSearch",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="depth",
            label="Depth",
            type="text",
            required=False,
            default="standard",
        ),
        ConfigField(
            name="output_type",
            label="Output Type",
            type="text",
            required=False,
            default="searchResults",
        ),
        ConfigField(
            name="enable_web_search_with_linkup",
            label="Enable Web Search With Linkup",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["linkup-sdk"],
    docs_url="https://docs.agno.com/tools/toolkits/search/linkup",
)
def linkup_tools() -> type[LinkupTools]:
    """Return Linkup tools for web search."""
    from agno.tools.linkup import LinkupTools

    return LinkupTools

================================================================================
# FILE: src/mindroom/tools/lumalabs.py
================================================================================

"""Luma Labs tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.lumalab import LumaLabTools


@register_tool_with_metadata(
    name="lumalabs",
    display_name="Luma Labs",
    description="3D content creation and video generation using Luma AI Dream Machine",
    category=ToolCategory.DEVELOPMENT,  # others/ category maps to DEVELOPMENT
    status=ToolStatus.REQUIRES_CONFIG,  # Requires LUMAAI_API_KEY
    setup_type=SetupType.API_KEY,  # API key authentication
    icon="FaVideo",  # Video-related icon
    icon_color="text-purple-600",  # Purple color for AI/ML tools
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="wait_for_completion",
            label="Wait For Completion",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="poll_interval",
            label="Poll Interval",
            type="number",
            required=False,
            default=3,
        ),
        ConfigField(
            name="max_wait_time",
            label="Max Wait Time",
            type="number",
            required=False,
            default=300,
        ),
        ConfigField(
            name="enable_generate_video",
            label="Enable Generate Video",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_image_to_video",
            label="Enable Image To Video",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["lumaai"],
    docs_url="https://docs.agno.com/tools/toolkits/others/lumalabs",
)
def lumalabs_tools() -> type[LumaLabTools]:
    """Return Luma Labs tools for 3D content creation and video generation."""
    from agno.tools.lumalab import LumaLabTools

    return LumaLabTools

================================================================================
# FILE: src/mindroom/tools/mem0.py
================================================================================

"""Mem0 Memory tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.mem0 import Mem0Tools


@register_tool_with_metadata(
    name="mem0",
    display_name="Mem0 Memory",
    description="Persistent memory system that stores, retrieves, searches, and manages user memories and context",
    category=ToolCategory.PRODUCTIVITY,  # Database tools â†’ Productivity
    status=ToolStatus.REQUIRES_CONFIG,  # Requires API key for cloud usage
    setup_type=SetupType.API_KEY,  # Optional API key for cloud usage
    icon="Brain",
    icon_color="text-purple-600",  # Memory/brain theme
    config_fields=[
        ConfigField(
            name="config",
            label="Config",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="user_id",
            label="User ID",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="org_id",
            label="Org ID",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="project_id",
            label="Project ID",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="infer",
            label="Infer",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_add_memory",
            label="Enable Add Memory",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_memory",
            label="Enable Search Memory",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_all_memories",
            label="Enable Get All Memories",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_delete_all_memories",
            label="Enable Delete All Memories",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["mem0ai"],  # Already in pyproject.toml
    docs_url="https://docs.agno.com/tools/toolkits/database/mem0",
)
def mem0_tools() -> type[Mem0Tools]:
    """Return Mem0 memory tools for persistent memory management."""
    from agno.tools.mem0 import Mem0Tools

    return Mem0Tools

================================================================================
# FILE: src/mindroom/tools/memory.py
================================================================================

"""Memory tool metadata registration.

Registers the ``memory`` tool in the metadata registry for UI display.
The actual toolkit (``mindroom.custom_tools.memory.MemoryTools``) requires
agent context and is instantiated directly in ``create_agent()``, so it
is NOT added to ``TOOL_REGISTRY`` (no generic factory).
"""

from mindroom.tools_metadata import (
    TOOL_METADATA,
    SetupType,
    ToolCategory,
    ToolMetadata,
    ToolStatus,
)

TOOL_METADATA["memory"] = ToolMetadata(
    name="memory",
    display_name="Agent Memory",
    description="Explicitly store and search agent memories on demand",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="Brain",
    icon_color="text-violet-500",
    config_fields=[],
    dependencies=[],
)

================================================================================
# FILE: src/mindroom/tools/modelslabs.py
================================================================================

"""ModelsLabs tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.models_labs import ModelsLabTools


@register_tool_with_metadata(
    name="modelslabs",
    display_name="ModelsLabs",
    description="AI model marketplace for generating videos, audio, and GIFs from text prompts",
    category=ToolCategory.DEVELOPMENT,  # From docs URL: /tools/toolkits/others/
    status=ToolStatus.REQUIRES_CONFIG,  # Requires API key
    setup_type=SetupType.API_KEY,  # Uses api_key parameter
    icon="Video",  # React icon for video generation
    icon_color="text-purple-600",  # Purple color for creative/AI tools
    config_fields=[
        # Authentication parameter
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
            placeholder="your-modelslabs-api-key",
            description="The ModelsLab API key for authentication (can also be set via MODELS_LAB_API_KEY env var)",
        ),
        # Media generation parameters
        ConfigField(
            name="file_type",
            label="File Type",
            type="text",
            required=False,
            # The agno default is FileType.MP4, which has value "mp4"
            default="mp4",
            placeholder="mp4",
            description="The type of file to generate (mp4, gif, or mp3)",
        ),
        # Timing and completion parameters
        ConfigField(
            name="wait_for_completion",
            label="Wait for Completion",
            type="boolean",
            required=False,
            default=False,
            description="Whether to wait for the media generation to complete before returning",
        ),
        ConfigField(
            name="add_to_eta",
            label="Add to ETA",
            type="number",
            required=False,
            default=15,
            placeholder="15",
            description="Time in seconds to add to the ETA to account for the time it takes to fetch the media",
        ),
        ConfigField(
            name="max_wait_time",
            label="Max Wait Time",
            type="number",
            required=False,
            default=60,
            placeholder="60",
            description="Maximum time in seconds to wait for the media to be ready",
        ),
    ],
    dependencies=["requests"],  # Already in pyproject.toml
    docs_url="https://docs.agno.com/tools/toolkits/others/models_labs",
)
def modelslabs_tools() -> type[ModelsLabTools]:
    """Return ModelsLabs tool for AI-powered media generation."""
    from agno.tools.models_labs import ModelsLabTools

    return ModelsLabTools

================================================================================
# FILE: src/mindroom/tools/moviepy_video_tools.py
================================================================================

"""MoviePy Video Tools configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.moviepy_video import MoviePyVideoTools


@register_tool_with_metadata(
    name="moviepy_video_tools",
    display_name="MoviePy Video Tools",
    description="Process videos, extract audio, generate SRT caption files, and embed rich word-highlighted captions",
    category=ToolCategory.DEVELOPMENT,  # Derived from docs URL (/others/)
    status=ToolStatus.AVAILABLE,  # No authentication required
    setup_type=SetupType.NONE,  # No authentication needed
    icon="FaVideo",  # React icon name for video
    icon_color="text-purple-600",  # Purple color for video processing
    config_fields=[
        ConfigField(
            name="enable_process_video",
            label="Enable Process Video",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_generate_captions",
            label="Enable Generate Captions",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_embed_captions",
            label="Enable Embed Captions",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["moviepy"],  # From agno requirements
    docs_url="https://docs.agno.com/tools/toolkits/others/moviepy",  # URL from llms.txt but WITHOUT .md extension
)
def moviepy_video_tools() -> type[MoviePyVideoTools]:
    """Return MoviePy Video Tools for video processing, audio extraction, and caption generation."""
    from agno.tools.moviepy_video import MoviePyVideoTools

    return MoviePyVideoTools

================================================================================
# FILE: src/mindroom/tools/neo4j.py
================================================================================

"""Neo4j tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.neo4j import Neo4jTools


@register_tool_with_metadata(
    name="neo4j",
    display_name="Neo4j",
    description="Query Neo4j graph databases - list labels, relationships, get schema, and run Cypher queries",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    icon="SiNeo4J",
    icon_color="text-blue-500",
    config_fields=[
        ConfigField(
            name="uri",
            label="URI",
            type="url",
            required=False,
            default=None,
            placeholder="bolt://localhost:7687",
            description="Neo4j connection URI (falls back to NEO4J_URI env var)",
        ),
        ConfigField(
            name="user",
            label="Username",
            type="text",
            required=True,
            placeholder="neo4j",
            description="Neo4j username (falls back to NEO4J_USERNAME env var)",
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=True,
            description="Neo4j password (falls back to NEO4J_PASSWORD env var)",
        ),
        ConfigField(
            name="database",
            label="Database",
            type="text",
            required=False,
            default=None,
            placeholder="neo4j",
        ),
        ConfigField(
            name="enable_list_labels",
            label="Enable List Labels",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_list_relationships",
            label="Enable List Relationships",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_schema",
            label="Enable Get Schema",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_run_cypher",
            label="Enable Run Cypher",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["neo4j"],
    docs_url="https://docs.agno.com/tools/toolkits/others/neo4j",
)
def neo4j_tools() -> type[Neo4jTools]:
    """Return Neo4j tools for graph database operations."""
    from agno.tools.neo4j import Neo4jTools

    return Neo4jTools

================================================================================
# FILE: src/mindroom/tools/newspaper4k.py
================================================================================

"""Newspaper4k tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.newspaper4k import Newspaper4kTools


@register_tool_with_metadata(
    name="newspaper4k",
    display_name="Newspaper4k",
    description="Read and extract content from news articles using advanced web scraping",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaNewspaper",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="include_summary",
            label="Include Summary",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="article_length",
            label="Article Length",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_read_article",
            label="Enable Read Article",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["newspaper4k", "lxml_html_clean"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/newspaper4k",
)
def newspaper4k_tools() -> type[Newspaper4kTools]:
    """Return Newspaper4k tools for news article extraction."""
    from agno.tools.newspaper4k import Newspaper4kTools

    return Newspaper4kTools

================================================================================
# FILE: src/mindroom/tools/notion.py
================================================================================

"""Notion tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.notion import NotionTools


@register_tool_with_metadata(
    name="notion",
    display_name="Notion",
    description="Create, update, and search pages in Notion databases",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiNotion",
    icon_color="text-gray-800",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=True,
            placeholder="Notion integration token",
            description="Internal integration token (falls back to NOTION_API_KEY env var)",
        ),
        ConfigField(
            name="database_id",
            label="Database ID",
            type="text",
            required=True,
            placeholder="Notion database ID",
            description="ID of the Notion database to work with (falls back to NOTION_DATABASE_ID env var)",
        ),
        ConfigField(
            name="enable_create_page",
            label="Enable Create Page",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_update_page",
            label="Enable Update Page",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_pages",
            label="Enable Search Pages",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["notion-client"],
    docs_url="https://docs.agno.com/tools/toolkits/others/notion",
    helper_text="Create an integration at [Notion Developers](https://www.notion.so/my-integrations) and share the database with it",
)
def notion_tools() -> type[NotionTools]:
    """Return Notion tools for page management."""
    from agno.tools.notion import NotionTools

    return NotionTools

================================================================================
# FILE: src/mindroom/tools/openai.py
================================================================================

"""OpenAI tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.openai import OpenAITools


@register_tool_with_metadata(
    name="openai",
    display_name="OpenAI",
    description="AI-powered tools for transcription, image generation, and speech synthesis",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiOpenai",
    icon_color="text-green-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_transcription",
            label="Enable Transcription",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_image_generation",
            label="Enable Image Generation",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_speech_generation",
            label="Enable Speech Generation",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="transcription_model",
            label="Transcription Model",
            type="text",
            required=False,
            default="whisper-1",
        ),
        ConfigField(
            name="text_to_speech_voice",
            label="Text To Speech Voice",
            type="text",
            required=False,
            default="alloy",
        ),
        ConfigField(
            name="text_to_speech_model",
            label="Text To Speech Model",
            type="text",
            required=False,
            default="tts-1",
        ),
        ConfigField(
            name="text_to_speech_format",
            label="Text To Speech Format",
            type="text",
            required=False,
            default="mp3",
        ),
        ConfigField(
            name="image_model",
            label="Image Model",
            type="text",
            required=False,
            default="dall-e-3",
        ),
        ConfigField(
            name="image_quality",
            label="Image Quality",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="image_size",
            label="Image Size",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="image_style",
            label="Image Style",
            type="text",
            required=False,
            default=None,
        ),
    ],
    dependencies=["openai"],
    docs_url="https://docs.agno.com/tools/toolkits/models/openai",
)
def openai_tools() -> type[OpenAITools]:
    """Return OpenAI tools for AI-powered transcription, image generation, and speech synthesis."""
    from agno.tools.openai import OpenAITools

    return OpenAITools

================================================================================
# FILE: src/mindroom/tools/openbb.py
================================================================================

"""OpenBB tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.openbb import OpenBBTools


@register_tool_with_metadata(
    name="openbb",
    display_name="OpenBB",
    description="Get stock prices, company news, price targets, and company profiles from OpenBB financial platform",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaChartArea",
    icon_color="text-blue-500",
    config_fields=[
        ConfigField(
            name="obb",
            label="OpenBB Instance",
            type="text",
            required=False,
            default=None,
            description="Optional pre-configured OpenBB instance (advanced usage)",
        ),
        ConfigField(
            name="openbb_pat",
            label="Personal Access Token",
            type="text",
            required=False,
            default=None,
            description="OpenBB PAT for premium data providers (falls back to OPENBB_PAT env var). Optional - works without it using yfinance.",
        ),
        ConfigField(
            name="provider",
            label="Data Provider",
            type="text",
            required=False,
            default="yfinance",
            description="Data provider: yfinance, benzinga, fmp, intrinio, polygon, tiingo, or tmx",
        ),
        ConfigField(
            name="enable_get_stock_price",
            label="Enable Get Stock Price",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_company_symbol",
            label="Enable Search Company Symbol",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_get_company_news",
            label="Enable Get Company News",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_get_company_profile",
            label="Enable Get Company Profile",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_get_price_targets",
            label="Enable Get Price Targets",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["openbb"],
    docs_url="https://docs.agno.com/tools/toolkits/others/openbb",
)
def openbb_tools() -> type[OpenBBTools]:
    """Return OpenBB tools for financial data."""
    from agno.tools.openbb import OpenBBTools

    return OpenBBTools

================================================================================
# FILE: src/mindroom/tools/openweather.py
================================================================================

"""OpenWeather tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.openweather import OpenWeatherTools


@register_tool_with_metadata(
    name="openweather",
    display_name="OpenWeather",
    description="Weather data services from OpenWeatherMap API",
    category=ToolCategory.DEVELOPMENT,  # Based on agno docs structure: /tools/toolkits/others/
    status=ToolStatus.REQUIRES_CONFIG,  # Requires API key
    setup_type=SetupType.API_KEY,  # Uses api_key parameter
    icon="WiDaySunny",  # Weather icon
    icon_color="text-orange-500",  # Orange sun color
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="units",
            label="Units",
            type="text",
            required=False,
            default="metric",
        ),
        ConfigField(
            name="enable_current_weather",
            label="Enable Current Weather",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_forecast",
            label="Enable Forecast",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_air_pollution",
            label="Enable Air Pollution",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_geocoding",
            label="Enable Geocoding",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["requests"],  # From agno requirements
    docs_url="https://docs.agno.com/tools/toolkits/others/openweather",  # URL without .md extension
)
def openweather_tools() -> type[OpenWeatherTools]:
    """Return OpenWeather tools for weather data access."""
    from agno.tools.openweather import OpenWeatherTools

    return OpenWeatherTools

================================================================================
# FILE: src/mindroom/tools/oxylabs.py
================================================================================

"""Oxylabs tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.oxylabs import OxylabsTools


@register_tool_with_metadata(
    name="oxylabs",
    display_name="Oxylabs",
    description="Powerful web scraping capabilities including SERP, Amazon product data, and universal web scraping",
    category=ToolCategory.RESEARCH,  # web_scrape maps to RESEARCH
    status=ToolStatus.REQUIRES_CONFIG,  # requires username and password
    setup_type=SetupType.API_KEY,  # uses username/password credentials
    icon="FaGlobe",
    icon_color="text-blue-600",  # Web/scraping theme
    config_fields=[
        # Authentication credentials
        ConfigField(
            name="username",
            label="Username",
            type="text",
            required=False,
            placeholder="your_oxylabs_username",
            description="Oxylabs dashboard username (can also be set via OXYLABS_USERNAME env var)",
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=False,
            placeholder="your_oxylabs_password",
            description="Oxylabs dashboard password (can also be set via OXYLABS_PASSWORD env var)",
        ),
    ],
    dependencies=["oxylabs"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/oxylabs",
)
def oxylabs_tools() -> type[OxylabsTools]:
    """Return Oxylabs tools for web scraping and data extraction."""
    from agno.tools.oxylabs import OxylabsTools

    return OxylabsTools

================================================================================
# FILE: src/mindroom/tools/pandas.py
================================================================================

"""Pandas tools configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.pandas import PandasTools


@register_tool_with_metadata(
    name="pandas",
    display_name="Pandas",
    description="Advanced data manipulation and analysis",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="Database",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="enable_create_pandas_dataframe",
            label="Enable Create Pandas Dataframe",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_run_dataframe_operation",
            label="Enable Run Dataframe Operation",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["pandas"],
    docs_url="https://docs.agno.com/tools/toolkits/database/pandas",
)
def pandas_tools() -> type[PandasTools]:
    """Return Pandas tools for data manipulation and analysis."""
    from agno.tools.pandas import PandasTools

    return PandasTools

================================================================================
# FILE: src/mindroom/tools/postgres.py
================================================================================

"""PostgreSQL tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.postgres import PostgresTools


@register_tool_with_metadata(
    name="postgres",
    display_name="PostgreSQL",
    description="Query PostgreSQL databases - list tables, describe schemas, run SQL queries, and export data",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    icon="SiPostgresql",
    icon_color="text-blue-700",
    config_fields=[
        ConfigField(
            name="host",
            label="Host",
            type="url",
            required=True,
            placeholder="localhost",
            description="PostgreSQL server hostname",
        ),
        ConfigField(
            name="port",
            label="Port",
            type="number",
            required=False,
            default=5432,
        ),
        ConfigField(
            name="db_name",
            label="Database Name",
            type="text",
            required=True,
            placeholder="mydb",
        ),
        ConfigField(
            name="user",
            label="Username",
            type="text",
            required=True,
            placeholder="postgres",
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=True,
        ),
        ConfigField(
            name="table_schema",
            label="Table Schema",
            type="text",
            required=False,
            default="public",
        ),
    ],
    dependencies=["psycopg-binary"],
    docs_url="https://docs.agno.com/tools/toolkits/others/postgres",
)
def postgres_tools() -> type[PostgresTools]:
    """Return PostgreSQL tools for database operations."""
    from agno.tools.postgres import PostgresTools

    return PostgresTools

================================================================================
# FILE: src/mindroom/tools/pubmed.py
================================================================================

"""PubMed tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.pubmed import PubmedTools


@register_tool_with_metadata(
    name="pubmed",
    display_name="PubMed",
    description="Search and retrieve medical and life science literature from PubMed",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="SiPubmed",
    icon_color="text-blue-600",  # Medical blue
    config_fields=[
        ConfigField(
            name="email",
            label="Email",
            type="text",
            required=False,
            default="your_email@example.com",
        ),
        ConfigField(
            name="max_results",
            label="Max Results",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="results_expanded",
            label="Results Expanded",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_search_pubmed",
            label="Enable Search Pubmed",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["httpx"],
    docs_url="https://docs.agno.com/tools/toolkits/search/pubmed",
)
def pubmed_tools() -> type[PubmedTools]:
    """Return PubMed tools for medical research and literature search."""
    from agno.tools.pubmed import PubmedTools

    return PubmedTools

================================================================================
# FILE: src/mindroom/tools/python.py
================================================================================

"""Python tools configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.python import PythonTools


@register_tool_with_metadata(
    name="python",
    display_name="Python Tools",
    description="Execute Python code, manage files, and install packages",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="SiPython",
    icon_color="text-blue-500",
    config_fields=[
        ConfigField(
            name="base_dir",
            label="Base Dir",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="safe_globals",
            label="Safe Globals",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="safe_locals",
            label="Safe Locals",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="restrict_to_base_dir",
            label="Restrict To Base Dir",
            type="boolean",
            required=False,
            default=True,
        ),
    ],
    dependencies=["agno"],
    docs_url="https://docs.agno.com/tools/toolkits/local/python",
)
def python_tools() -> type[PythonTools]:
    """Return Python tools for code execution and file management."""
    from agno.tools.python import PythonTools

    return PythonTools

================================================================================
# FILE: src/mindroom/tools/reasoning.py
================================================================================

"""Reasoning tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.reasoning import ReasoningTools


@register_tool_with_metadata(
    name="reasoning",
    display_name="Reasoning",
    description="Step-by-step reasoning scratchpad with think and analyze tools for structured problem solving",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaBrain",
    icon_color="text-purple-500",
    config_fields=[
        ConfigField(
            name="enable_think",
            label="Enable Think",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_analyze",
            label="Enable Analyze",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="add_instructions",
            label="Add Instructions",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="add_few_shot",
            label="Add Few-Shot Examples",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="instructions",
            label="Instructions",
            type="text",
            required=False,
            default=None,
            description="Custom instructions to override the default reasoning instructions",
        ),
        ConfigField(
            name="few_shot_examples",
            label="Few-Shot Examples",
            type="text",
            required=False,
            default=None,
            description="Custom few-shot examples for the reasoning tools",
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=[],
    docs_url="https://docs.agno.com/tools/toolkits/others/reasoning",
)
def reasoning_tools() -> type[ReasoningTools]:
    """Return Reasoning tools for step-by-step problem solving."""
    from agno.tools.reasoning import ReasoningTools

    return ReasoningTools

================================================================================
# FILE: src/mindroom/tools/reddit.py
================================================================================

"""Reddit tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.reddit import RedditTools


@register_tool_with_metadata(
    name="reddit",
    display_name="Reddit",
    description="Social media platform for browsing, posting, and interacting with Reddit communities",
    category=ToolCategory.SOCIAL,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiReddit",
    icon_color="text-orange-500",  # Reddit's signature orange color
    config_fields=[
        ConfigField(
            name="reddit_instance",
            label="Reddit Instance",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="client_id",
            label="Client ID",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="client_secret",
            label="Client Secret",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="user_agent",
            label="User Agent",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="username",
            label="Username",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=False,
            default=None,
        ),
    ],
    dependencies=["praw"],
    docs_url=None,
)
def reddit_tools() -> type[RedditTools]:
    """Return Reddit tools for social media interaction."""
    from agno.tools.reddit import RedditTools

    return RedditTools

================================================================================
# FILE: src/mindroom/tools/redshift.py
================================================================================

"""Amazon Redshift tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.redshift import RedshiftTools


@register_tool_with_metadata(
    name="redshift",
    display_name="Amazon Redshift",
    description="Query Amazon Redshift data warehouse - list tables, run SQL, and export results",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    icon="SiAmazonredshift",
    icon_color="text-red-600",
    config_fields=[
        ConfigField(
            name="host",
            label="Host",
            type="url",
            required=True,
            placeholder="my-cluster.xxxx.region.redshift.amazonaws.com",
            description="Redshift cluster endpoint (falls back to REDSHIFT_HOST env var)",
        ),
        ConfigField(
            name="port",
            label="Port",
            type="number",
            required=False,
            default=5439,
        ),
        ConfigField(
            name="database",
            label="Database",
            type="text",
            required=True,
            placeholder="dev",
            description="Database name (falls back to REDSHIFT_DATABASE env var)",
        ),
        ConfigField(
            name="user",
            label="Username",
            type="text",
            required=True,
            placeholder="admin",
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=True,
        ),
        ConfigField(
            name="iam",
            label="Use IAM Authentication",
            type="boolean",
            required=False,
            default=False,
            description="Use IAM authentication instead of password",
        ),
        ConfigField(
            name="cluster_identifier",
            label="Cluster Identifier",
            type="text",
            required=False,
            default=None,
            description="Redshift cluster identifier (required for IAM auth)",
        ),
        ConfigField(
            name="region",
            label="Region",
            type="text",
            required=False,
            default=None,
            placeholder="us-east-1",
        ),
        ConfigField(
            name="db_user",
            label="DB User",
            type="text",
            required=False,
            default=None,
            description="Database user for IAM authentication",
        ),
        ConfigField(
            name="access_key_id",
            label="Access Key ID",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="secret_access_key",
            label="Secret Access Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="session_token",
            label="Session Token",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="profile",
            label="AWS Profile",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="ssl",
            label="SSL",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="table_schema",
            label="Table Schema",
            type="text",
            required=False,
            default="public",
        ),
    ],
    dependencies=["redshift-connector"],
    docs_url="https://docs.agno.com/tools/toolkits/others/redshift",
)
def redshift_tools() -> type[RedshiftTools]:
    """Return Amazon Redshift tools for data warehouse operations."""
    from agno.tools.redshift import RedshiftTools

    return RedshiftTools

================================================================================
# FILE: src/mindroom/tools/replicate.py
================================================================================

"""Replicate tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.replicate import ReplicateTools


@register_tool_with_metadata(
    name="replicate",
    display_name="Replicate",
    description="Generate images and videos using AI models on the Replicate platform",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaVideo",
    icon_color="text-purple-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="model",
            label="Model",
            type="text",
            required=False,
            default="minimax/video-01",
        ),
        ConfigField(
            name="enable_generate_media",
            label="Enable Generate Media",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["replicate"],
    docs_url="https://docs.agno.com/tools/toolkits/others/replicate",
)
def replicate_tools() -> type[ReplicateTools]:
    """Return Replicate tools for AI media generation."""
    from agno.tools.replicate import ReplicateTools

    return ReplicateTools

================================================================================
# FILE: src/mindroom/tools/resend.py
================================================================================

"""Resend email tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.resend import ResendTools


@register_tool_with_metadata(
    name="resend",
    display_name="Resend",
    description="Email delivery service for sending transactional emails",
    category=ToolCategory.EMAIL,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="Mail",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="from_email",
            label="From Email",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_send_email",
            label="Enable Send Email",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["resend"],
    docs_url="https://docs.agno.com/tools/toolkits/others/resend",
)
def resend_tools() -> type[ResendTools]:
    """Return Resend email tools for sending transactional emails."""
    from agno.tools.resend import ResendTools

    return ResendTools

================================================================================
# FILE: src/mindroom/tools/scrapegraph.py
================================================================================

"""ScrapeGraph tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.scrapegraph import ScrapeGraphTools


@register_tool_with_metadata(
    name="scrapegraph",
    display_name="ScrapeGraph",
    description="Extract structured data from webpages using AI and natural language prompts",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaGlobe",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_smartscraper",
            label="Enable Smartscraper",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_markdownify",
            label="Enable Markdownify",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_crawl",
            label="Enable Crawl",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_searchscraper",
            label="Enable Searchscraper",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_agentic_crawler",
            label="Enable Agentic Crawler",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_scrape",
            label="Enable Scrape",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="render_heavy_js",
            label="Render Heavy Js",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["scrapegraph-py"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/scrapegraph",
)
def scrapegraph_tools() -> type[ScrapeGraphTools]:
    """Return ScrapeGraph tools for web data extraction."""
    from agno.tools.scrapegraph import ScrapeGraphTools

    return ScrapeGraphTools

================================================================================
# FILE: src/mindroom/tools/searxng.py
================================================================================

"""Searxng tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.searxng import Searxng


@register_tool_with_metadata(
    name="searxng",
    display_name="SearxNG",
    description="Open source search engine for web, images, news, science, and specialized content",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.NONE,
    icon="FaSearch",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="host",
            label="Host",
            type="url",
            required=True,
        ),
        ConfigField(
            name="engines",
            label="Engines",
            type="text",
            required=False,
        ),
        ConfigField(
            name="fixed_max_results",
            label="Fixed Max Results",
            type="number",
            required=False,
            default=None,
        ),
    ],
    dependencies=[],  # httpx already included in main dependencies
    docs_url="https://docs.agno.com/tools/toolkits/search/searxng",
)
def searxng_tools() -> type[Searxng]:
    """Return SearxNG search tools for web, images, news, and specialized content search."""
    from agno.tools.searxng import Searxng

    return Searxng

================================================================================
# FILE: src/mindroom/tools/serpapi.py
================================================================================

"""SerpApi tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.serpapi import SerpApiTools


@register_tool_with_metadata(
    name="serpapi",
    display_name="SerpApi",
    description="Google and YouTube search using SerpApi",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaGoogle",
    icon_color="text-blue-500",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_search_google",
            label="Enable Search Google",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_youtube",
            label="Enable Search Youtube",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["google-search-results"],
    docs_url="https://docs.agno.com/tools/toolkits/search/serpapi",
)
def serpapi_tools() -> type[SerpApiTools]:
    """Return SerpApi tools for Google and YouTube search."""
    from agno.tools.serpapi import SerpApiTools

    return SerpApiTools

================================================================================
# FILE: src/mindroom/tools/serper.py
================================================================================

"""Serper tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.serper import SerperTools


@register_tool_with_metadata(
    name="serper",
    display_name="Serper",
    description="Search Google, news, academic papers, and scrape webpages using Serper API",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaSearch",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="location",
            label="Location",
            type="text",
            required=False,
            default="us",
        ),
        ConfigField(
            name="language",
            label="Language",
            type="text",
            required=False,
            default="en",
        ),
        ConfigField(
            name="num_results",
            label="Num Results",
            type="number",
            required=False,
            default=10,
        ),
        ConfigField(
            name="date_range",
            label="Date Range",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_search",
            label="Enable Search",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_news",
            label="Enable Search News",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_scholar",
            label="Enable Search Scholar",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_scrape_webpage",
            label="Enable Scrape Webpage",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/search/serper",
)
def serper_tools() -> type[SerperTools]:
    """Return Serper tools for Google search, news, academic papers, and web scraping."""
    from agno.tools.serper import SerperTools

    return SerperTools

================================================================================
# FILE: src/mindroom/tools/shell.py
================================================================================

"""Shell tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.shell import ShellTools


@register_tool_with_metadata(
    name="shell",
    display_name="Shell Commands",
    description="Execute shell commands and scripts",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="Terminal",
    icon_color="text-green-500",
    config_fields=[
        ConfigField(
            name="base_dir",
            label="Base Dir",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_run_shell_command",
            label="Enable Run Shell Command",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=[],
    docs_url="https://docs.agno.com/tools/toolkits/local/shell",
)
def shell_tools() -> type[ShellTools]:
    """Return shell tools for command execution."""
    from agno.tools.shell import ShellTools

    return ShellTools

================================================================================
# FILE: src/mindroom/tools/shopify.py
================================================================================

"""Shopify tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.shopify import ShopifyTools


@register_tool_with_metadata(
    name="shopify",
    display_name="Shopify",
    description="Analyze sales data, products, orders, and customer insights from your Shopify store",
    category=ToolCategory.INTEGRATIONS,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiShopify",
    icon_color="text-green-600",
    config_fields=[
        ConfigField(
            name="shop_name",
            label="Shop Name",
            type="text",
            required=True,
            placeholder="my-store",
            description="Your Shopify store name (e.g., 'my-store' from my-store.myshopify.com). Falls back to SHOPIFY_SHOP_NAME env var.",
        ),
        ConfigField(
            name="access_token",
            label="Access Token",
            type="password",
            required=True,
            description="Shopify Admin API access token. Falls back to SHOPIFY_ACCESS_TOKEN env var.",
        ),
        ConfigField(
            name="api_version",
            label="API Version",
            type="text",
            required=False,
            default="2025-10",
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=30,
        ),
    ],
    dependencies=["httpx"],
    docs_url="https://docs.agno.com/tools/toolkits/others/shopify",
    helper_text="Create a custom app in your [Shopify Admin](https://admin.shopify.com/) to get an access token",
)
def shopify_tools() -> type[ShopifyTools]:
    """Return Shopify tools for e-commerce analytics."""
    from agno.tools.shopify import ShopifyTools

    return ShopifyTools

================================================================================
# FILE: src/mindroom/tools/slack.py
================================================================================

"""Slack tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.slack import SlackTools


@register_tool_with_metadata(
    name="slack",
    display_name="Slack",
    description="Send messages and manage channels",
    category=ToolCategory.COMMUNICATION,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiSlack",
    icon_color="text-purple-600",
    config_fields=[
        ConfigField(
            name="token",
            label="Token",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="markdown",
            label="Markdown",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_send_message",
            label="Enable Send Message",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_send_message_thread",
            label="Enable Send Message Thread",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_list_channels",
            label="Enable List Channels",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_channel_history",
            label="Enable Get Channel History",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["slack-sdk"],
    docs_url="https://docs.agno.com/tools/toolkits/social/slack",
)
def slack_tools() -> type[SlackTools]:
    """Return Slack tools for messaging and channel management."""
    from agno.tools.slack import SlackTools

    return SlackTools

================================================================================
# FILE: src/mindroom/tools/sleep.py
================================================================================

"""Sleep tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.sleep import SleepTools


@register_tool_with_metadata(
    name="sleep",
    display_name="Sleep",
    description="Sleep utility for introducing delays and pauses in execution",
    category=ToolCategory.DEVELOPMENT,  # Local utility tool
    status=ToolStatus.AVAILABLE,  # No config needed
    setup_type=SetupType.NONE,  # No authentication required
    icon="Clock",  # React icon name
    icon_color="text-purple-500",  # Tailwind color class
    config_fields=[
        ConfigField(
            name="enable_sleep",
            label="Enable Sleep",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["agno"],  # From agno requirements
    docs_url="https://docs.agno.com/tools/toolkits/local/sleep",
)
def sleep_tools() -> type[SleepTools]:
    """Return sleep tools for introducing delays and pauses in execution."""
    from agno.tools.sleep import SleepTools

    return SleepTools

================================================================================
# FILE: src/mindroom/tools/spider.py
================================================================================

"""Spider tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.spider import SpiderTools


@register_tool_with_metadata(
    name="spider",
    display_name="Spider",
    description="Web scraper and crawler that returns LLM-ready data",
    category=ToolCategory.RESEARCH,  # Based on web_scrape category in docs
    status=ToolStatus.AVAILABLE,  # API key handled via SPIDER_API_KEY env var
    setup_type=SetupType.NONE,
    icon="FaSpider",
    icon_color="text-red-600",  # Spider red color
    config_fields=[
        ConfigField(
            name="max_results",
            label="Max Results",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="url",
            label="URL",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="optional_params",
            label="Optional Params",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_search",
            label="Enable Search",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_scrape",
            label="Enable Scrape",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_crawl",
            label="Enable Crawl",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["spider-client"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/spider",
    helper_text="Get your API key from the [Spider dashboard](https://spider.cloud)",
)
def spider_tools() -> type[SpiderTools]:
    """Return Spider tools for web scraping and crawling."""
    from agno.tools.spider import SpiderTools

    return SpiderTools

================================================================================
# FILE: src/mindroom/tools/spotify.py
================================================================================

"""Spotify tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.spotify import SpotifyTools


@register_tool_with_metadata(
    name="spotify",
    display_name="Spotify",
    description="Search tracks, manage playlists, get recommendations, and control playback on Spotify",
    category=ToolCategory.ENTERTAINMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiSpotify",
    icon_color="text-green-500",
    config_fields=[
        ConfigField(
            name="access_token",
            label="Access Token",
            type="password",
            required=True,
            placeholder="Spotify OAuth access token",
            description="OAuth access token with required scopes (user-read-private, playlist-modify-public, playlist-modify-private)",
        ),
        ConfigField(
            name="default_market",
            label="Default Market",
            type="text",
            required=False,
            default="US",
            placeholder="e.g., US, GB, DE",
        ),
        ConfigField(
            name="timeout",
            label="Timeout",
            type="number",
            required=False,
            default=30,
        ),
    ],
    dependencies=["httpx"],
    docs_url="https://docs.agno.com/tools/toolkits/others/spotify",
    helper_text="Get an access token from the [Spotify Developer Dashboard](https://developer.spotify.com/dashboard)",
)
def spotify_tools() -> type[SpotifyTools]:
    """Return Spotify tools for music search and playlist management."""
    from agno.tools.spotify import SpotifyTools

    return SpotifyTools

================================================================================
# FILE: src/mindroom/tools/sql.py
================================================================================

"""SQL tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.sql import SQLTools


@register_tool_with_metadata(
    name="sql",
    display_name="SQL Tools",
    description="Database query and management tools for SQL databases",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.SPECIAL,
    icon="Database",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="db_url",
            label="Db URL",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="db_engine",
            label="Db Engine",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="user",
            label="User",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="host",
            label="Host",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="port",
            label="Port",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="schema",
            label="Schema",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="dialect",
            label="Dialect",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="tables",
            label="Tables",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_list_tables",
            label="Enable List Tables",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_describe_table",
            label="Enable Describe Table",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_run_sql_query",
            label="Enable Run Sql Query",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["sqlalchemy"],
    docs_url="https://docs.agno.com/tools/toolkits/database/sql",
)
def sql_tools() -> type[SQLTools]:
    """Return SQL tools for database operations."""
    from agno.tools.sql import SQLTools

    return SQLTools

================================================================================
# FILE: src/mindroom/tools/tavily.py
================================================================================

"""Tavily tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.tavily import TavilyTools


@register_tool_with_metadata(
    name="tavily",
    display_name="Tavily",
    description="Real-time web search API for retrieving current information",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="FaSearch",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="api_base_url",
            label="API Base URL",
            type="url",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_search",
            label="Enable Search",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_context",
            label="Enable Search Context",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_extract",
            label="Enable Extract",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="max_tokens",
            label="Max Tokens",
            type="number",
            required=False,
            default=6000,
        ),
        ConfigField(
            name="include_answer",
            label="Include Answer",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="search_depth",
            label="Search Depth",
            type="text",
            required=False,
            default="advanced",
        ),
        ConfigField(
            name="extract_depth",
            label="Extract Depth",
            type="text",
            required=False,
            default="basic",
        ),
        ConfigField(
            name="include_images",
            label="Include Images",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="include_favicon",
            label="Include Favicon",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="extract_timeout",
            label="Extract Timeout",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="extract_format",
            label="Extract Format",
            type="text",
            required=False,
            default="markdown",
        ),
        ConfigField(
            name="format",
            label="Format",
            type="text",
            required=False,
            default="markdown",
        ),
    ],
    dependencies=["tavily-python"],
    docs_url="https://docs.agno.com/tools/toolkits/search/tavily",
)
def tavily_tools() -> type[TavilyTools]:
    """Return Tavily tools for real-time web search."""
    from agno.tools.tavily import TavilyTools

    return TavilyTools

================================================================================
# FILE: src/mindroom/tools/telegram.py
================================================================================

"""Telegram tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.telegram import TelegramTools


@register_tool_with_metadata(
    name="telegram",
    display_name="Telegram",
    description="Send messages via Telegram bot",
    category=ToolCategory.COMMUNICATION,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiTelegram",
    icon_color="text-blue-500",  # Telegram blue
    config_fields=[
        ConfigField(
            name="chat_id",
            label="Chat ID",
            type="text",
            required=True,
        ),
        ConfigField(
            name="token",
            label="Token",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_send_message",
            label="Enable Send Message",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["httpx"],
    docs_url="https://core.telegram.org/bots/api",
)
def telegram_tools() -> type[TelegramTools]:
    """Return Telegram tools for sending messages."""
    from agno.tools.telegram import TelegramTools

    return TelegramTools

================================================================================
# FILE: src/mindroom/tools/todoist.py
================================================================================

"""Todoist tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.todoist import TodoistTools


@register_tool_with_metadata(
    name="todoist",
    display_name="Todoist",
    description="Task management with Todoist - create, update, delete, and organize tasks and projects",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiTodoist",
    icon_color="text-red-500",
    config_fields=[
        ConfigField(
            name="api_token",
            label="API Token",
            type="password",
            required=False,
            default=None,
        ),
    ],
    dependencies=["todoist-api-python"],
    docs_url="https://docs.agno.com/tools/toolkits/others/todoist",
)
def todoist_tools() -> type[TodoistTools]:
    """Return Todoist tools for task management."""
    from agno.tools.todoist import TodoistTools

    return TodoistTools

================================================================================
# FILE: src/mindroom/tools/trafilatura.py
================================================================================

"""Trafilatura tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.trafilatura import TrafilaturaTools


@register_tool_with_metadata(
    name="trafilatura",
    display_name="Trafilatura",
    description="Extract text and metadata from web pages, crawl websites, and convert HTML to text",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaFileAlt",
    icon_color="text-teal-500",
    config_fields=[
        ConfigField(
            name="output_format",
            label="Output Format",
            type="text",
            required=False,
            default="txt",
            description="Output format: txt, json, markdown, xml, csv, or html",
        ),
        ConfigField(
            name="include_comments",
            label="Include Comments",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="include_tables",
            label="Include Tables",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="include_images",
            label="Include Images",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="include_formatting",
            label="Include Formatting",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="include_links",
            label="Include Links",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="with_metadata",
            label="Include Metadata",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="favor_precision",
            label="Favor Precision",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="favor_recall",
            label="Favor Recall",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="target_language",
            label="Target Language",
            type="text",
            required=False,
            default=None,
            placeholder="e.g., en, de, fr (ISO 639-1)",
        ),
        ConfigField(
            name="deduplicate",
            label="Deduplicate",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="max_tree_size",
            label="Max Tree Size",
            type="number",
            required=False,
            default=None,
        ),
        ConfigField(
            name="max_crawl_urls",
            label="Max Crawl URLs",
            type="number",
            required=False,
            default=10,
        ),
        ConfigField(
            name="max_known_urls",
            label="Max Known URLs",
            type="number",
            required=False,
            default=100000,
        ),
        ConfigField(
            name="enable_extract_text",
            label="Enable Extract Text",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_extract_metadata_only",
            label="Enable Extract Metadata Only",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_html_to_text",
            label="Enable HTML to Text",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_extract_batch",
            label="Enable Extract Batch",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_crawl_website",
            label="Enable Crawl Website",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["trafilatura"],
    docs_url="https://docs.agno.com/tools/toolkits/others/trafilatura",
)
def trafilatura_tools() -> type[TrafilaturaTools]:
    """Return Trafilatura tools for web content extraction."""
    from agno.tools.trafilatura import TrafilaturaTools

    return TrafilaturaTools

================================================================================
# FILE: src/mindroom/tools/trello.py
================================================================================

"""Trello tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.trello import TrelloTools


@register_tool_with_metadata(
    name="trello",
    display_name="Trello",
    description="Project board management with Trello API integration",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiTrello",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="api_secret",
            label="API Secret",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="token",
            label="Token",
            type="password",
            required=False,
            default=None,
        ),
    ],
    dependencies=["py-trello"],
    docs_url="https://docs.agno.com/tools/toolkits/others/trello",
)
def trello_tools() -> type[TrelloTools]:
    """Return Trello tools for project board management."""
    from agno.tools.trello import TrelloTools

    return TrelloTools

================================================================================
# FILE: src/mindroom/tools/twilio.py
================================================================================

"""Twilio tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.twilio import TwilioTools


@register_tool_with_metadata(
    name="twilio",
    display_name="Twilio",
    description="SMS messaging and voice communication platform",
    category=ToolCategory.COMMUNICATION,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiTwilio",
    icon_color="text-red-600",  # Twilio red
    config_fields=[
        ConfigField(
            name="account_sid",
            label="Account Sid",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="auth_token",
            label="Auth Token",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="api_secret",
            label="API Secret",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="region",
            label="Region",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="edge",
            label="Edge",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="debug",
            label="Debug",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_send_sms",
            label="Enable Send Sms",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_call_details",
            label="Enable Get Call Details",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_list_messages",
            label="Enable List Messages",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["twilio"],
    docs_url="https://docs.agno.com/tools/toolkits/social/twilio",
)
def twilio_tools() -> type[TwilioTools]:
    """Return Twilio tools for SMS messaging and voice communication."""
    from agno.tools.twilio import TwilioTools

    return TwilioTools

================================================================================
# FILE: src/mindroom/tools/unsplash.py
================================================================================

"""Unsplash tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.unsplash import UnsplashTools


@register_tool_with_metadata(
    name="unsplash",
    display_name="Unsplash",
    description="Search and retrieve high-quality, royalty-free images from Unsplash",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiUnsplash",
    icon_color="text-gray-800",
    config_fields=[
        ConfigField(
            name="access_key",
            label="Access Key",
            type="password",
            required=True,
            placeholder="Unsplash API access key",
            description="API access key from Unsplash Developer account",
        ),
        ConfigField(
            name="enable_search_photos",
            label="Enable Search Photos",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_photo",
            label="Enable Get Photo",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_random_photo",
            label="Enable Get Random Photo",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_download_photo",
            label="Enable Download Photo",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=[],
    docs_url="https://docs.agno.com/tools/toolkits/others/unsplash",
    helper_text="Get a free API key from [Unsplash Developers](https://unsplash.com/developers)",
)
def unsplash_tools() -> type[UnsplashTools]:
    """Return Unsplash tools for image search."""
    from agno.tools.unsplash import UnsplashTools

    return UnsplashTools

================================================================================
# FILE: src/mindroom/tools/visualization.py
================================================================================

"""Visualization tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.visualization import VisualizationTools


@register_tool_with_metadata(
    name="visualization",
    display_name="Visualization",
    description="Create bar charts, line charts, pie charts, scatter plots, and histograms using matplotlib",
    category=ToolCategory.DEVELOPMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaChartBar",
    icon_color="text-indigo-500",
    config_fields=[
        ConfigField(
            name="output_dir",
            label="Output Directory",
            type="text",
            required=False,
            default="charts",
        ),
        ConfigField(
            name="enable_create_bar_chart",
            label="Enable Bar Chart",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_create_line_chart",
            label="Enable Line Chart",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_create_pie_chart",
            label="Enable Pie Chart",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_create_scatter_plot",
            label="Enable Scatter Plot",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_create_histogram",
            label="Enable Histogram",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["matplotlib"],
    docs_url="https://docs.agno.com/tools/toolkits/others/visualization",
)
def visualization_tools() -> type[VisualizationTools]:
    """Return Visualization tools for creating charts and plots."""
    from agno.tools.visualization import VisualizationTools

    return VisualizationTools

================================================================================
# FILE: src/mindroom/tools/web_browser_tools.py
================================================================================

"""Web Browser Tools configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.webbrowser import WebBrowserTools


@register_tool_with_metadata(
    name="web_browser_tools",
    display_name="Web Browser Tools",
    description="Open URLs in web browser tabs or windows",
    category=ToolCategory.DEVELOPMENT,  # From docs URL: /tools/toolkits/others/
    status=ToolStatus.AVAILABLE,  # No configuration required
    setup_type=SetupType.NONE,  # No authentication needed
    icon="FaGlobe",
    icon_color="text-blue-600",  # Web browser blue
    config_fields=[
        ConfigField(
            name="enable_open_page",
            label="Enable Open Page",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=[],  # Uses standard library webbrowser module
    docs_url="https://docs.agno.com/tools/toolkits/others/web-browser",
)
def web_browser_tools() -> type[WebBrowserTools]:
    """Return Web Browser Tools for opening URLs."""
    from agno.tools.webbrowser import WebBrowserTools

    return WebBrowserTools

================================================================================
# FILE: src/mindroom/tools/webex.py
================================================================================

"""Webex tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.webex import WebexTools


@register_tool_with_metadata(
    name="webex",
    display_name="Webex",
    description="Video conferencing and messaging platform for teams",
    category=ToolCategory.COMMUNICATION,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiWebex",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="enable_send_message",
            label="Enable Send Message",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_list_rooms",
            label="Enable List Rooms",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="access_token",
            label="Access Token",
            type="password",
            required=False,
            default=None,
        ),
    ],
    dependencies=["webexpythonsdk"],
    docs_url="https://docs.agno.com/tools/toolkits/social/webex",
)
def webex_tools() -> type[WebexTools]:
    """Return Webex tools for video conferencing and messaging."""
    from agno.tools.webex import WebexTools

    return WebexTools

================================================================================
# FILE: src/mindroom/tools/website.py
================================================================================

"""Website tools configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.website import WebsiteTools


@register_tool_with_metadata(
    name="website",
    display_name="Website Tools",
    description="Web scraping and content extraction from websites",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaGlobe",
    icon_color="text-blue-600",
    config_fields=[
        ConfigField(
            name="knowledge",
            label="Knowledge",
            type="text",
            required=False,
            default=None,
        ),
    ],
    dependencies=["httpx", "beautifulsoup4"],
    docs_url="https://docs.agno.com/tools/toolkits/web_scrape/website",
)
def website_tools() -> type[WebsiteTools]:
    """Return website tools for web scraping and content extraction."""
    from agno.tools.website import WebsiteTools

    return WebsiteTools

================================================================================
# FILE: src/mindroom/tools/whatsapp.py
================================================================================

"""WhatsApp tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.whatsapp import WhatsAppTools


@register_tool_with_metadata(
    name="whatsapp",
    display_name="WhatsApp Business",
    description="Send text and template messages via WhatsApp Business API",
    category=ToolCategory.COMMUNICATION,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiWhatsapp",
    icon_color="text-green-500",
    config_fields=[
        # Authentication/Connection parameters first
        ConfigField(
            name="access_token",
            label="Access Token",
            type="password",
            required=False,
            placeholder="EAAxxxxxxx...",
            description="WhatsApp Business API access token (can also be set via WHATSAPP_ACCESS_TOKEN env var)",
        ),
        ConfigField(
            name="phone_number_id",
            label="Phone Number ID",
            type="text",
            required=False,
            placeholder="1234567890123456",
            description="WhatsApp Business Account phone number ID (can also be set via WHATSAPP_PHONE_NUMBER_ID env var)",
        ),
        ConfigField(
            name="version",
            label="API Version",
            type="text",
            required=False,
            default="v22.0",
            placeholder="v22.0",
            description="WhatsApp API version to use",
        ),
        ConfigField(
            name="recipient_waid",
            label="Default Recipient WhatsApp ID",
            type="text",
            required=False,
            default=None,
            placeholder="+1234567890",
            description="Default recipient WhatsApp ID or phone number (optional, can also be set via WHATSAPP_RECIPIENT_WAID env var)",
        ),
        # Feature flags/boolean parameters
        ConfigField(
            name="async_mode",
            label="Async Mode",
            type="boolean",
            required=False,
            default=False,
            description="Enable asynchronous message sending",
        ),
    ],
    dependencies=["httpx"],
    docs_url="https://docs.agno.com/tools/toolkits/social/whatsapp",
)
def whatsapp_tools() -> type[WhatsAppTools]:
    """Return WhatsApp Business API tools for messaging."""
    from agno.tools.whatsapp import WhatsAppTools

    return WhatsAppTools

================================================================================
# FILE: src/mindroom/tools/wikipedia.py
================================================================================

"""Wikipedia tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.wikipedia import WikipediaTools


@register_tool_with_metadata(
    name="wikipedia",
    display_name="Wikipedia",
    description="Search and retrieve information from Wikipedia",
    category=ToolCategory.RESEARCH,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="SiWikipedia",
    icon_color="text-gray-700",
    config_fields=[
        ConfigField(
            name="knowledge",
            label="Knowledge",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["wikipedia"],
    docs_url="https://docs.agno.com/tools/toolkits/search/wikipedia",
)
def wikipedia_tools() -> type[WikipediaTools]:
    """Return Wikipedia tools for searching and retrieving information."""
    from agno.tools.wikipedia import WikipediaTools

    return WikipediaTools

================================================================================
# FILE: src/mindroom/tools/x.py
================================================================================

"""X (Twitter) tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.x import XTools


@register_tool_with_metadata(
    name="x",
    display_name="X (Twitter)",
    description="Post tweets, send DMs, and search Twitter/X content",
    category=ToolCategory.SOCIAL,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="SiX",
    icon_color="text-blue-400",
    config_fields=[
        # Authentication/Connection parameters first
        ConfigField(
            name="bearer_token",
            label="Bearer Token",
            type="password",
            required=False,
            placeholder="AAA...",
            description="Twitter API Bearer token (can also be set via X_BEARER_TOKEN env var)",
        ),
        ConfigField(
            name="consumer_key",
            label="Consumer Key",
            type="password",
            required=False,
            placeholder="consumer_key...",
            description="Twitter API consumer key (can also be set via X_CONSUMER_KEY env var)",
        ),
        ConfigField(
            name="consumer_secret",
            label="Consumer Secret",
            type="password",
            required=False,
            placeholder="consumer_secret...",
            description="Twitter API consumer secret (can also be set via X_CONSUMER_SECRET env var)",
        ),
        ConfigField(
            name="access_token",
            label="Access Token",
            type="password",
            required=False,
            placeholder="access_token...",
            description="Twitter API access token (can also be set via X_ACCESS_TOKEN env var)",
        ),
        ConfigField(
            name="access_token_secret",
            label="Access Token Secret",
            type="password",
            required=False,
            placeholder="access_token_secret...",
            description="Twitter API access token secret (can also be set via X_ACCESS_TOKEN_SECRET env var)",
        ),
        # Feature flags/boolean parameters grouped by functionality
        # Search and content features
        ConfigField(
            name="include_post_metrics",
            label="Include Post Metrics",
            type="boolean",
            required=False,
            default=False,
            description="Enable including post metrics (likes, retweets, etc.) in search results",
        ),
        # API behavior settings
        ConfigField(
            name="wait_on_rate_limit",
            label="Wait on Rate Limit",
            type="boolean",
            required=False,
            default=False,
            description="Enable waiting when rate limit is reached instead of failing",
        ),
    ],
    dependencies=["tweepy"],
    docs_url="https://docs.agno.com/tools/toolkits/social/x",
)
def x_tools() -> type[XTools]:
    """Return X (Twitter) tools for posting tweets and social media interaction."""
    from agno.tools.x import XTools

    return XTools

================================================================================
# FILE: src/mindroom/tools/yfinance.py
================================================================================

"""Yahoo Finance tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.yfinance import YFinanceTools


@register_tool_with_metadata(
    name="yfinance",
    display_name="Yahoo Finance",
    description="Get financial data and stock information from Yahoo Finance",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="FaChartLine",
    icon_color="text-purple-600",
    config_fields=[
        ConfigField(
            name="session",
            label="Session",
            type="text",
            required=False,
            default=None,
        ),
    ],
    dependencies=["yfinance"],
    docs_url="https://docs.agno.com/tools/toolkits/others/yfinance",
)
def yfinance_tools() -> type[YFinanceTools]:
    """Return Yahoo Finance tools for financial data."""
    from agno.tools.yfinance import YFinanceTools

    return YFinanceTools

================================================================================
# FILE: src/mindroom/tools/youtube.py
================================================================================

"""YouTube tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.youtube import YouTubeTools


@register_tool_with_metadata(
    name="youtube",
    display_name="YouTube",
    description="Extract video data, captions, and timestamps from YouTube videos",
    category=ToolCategory.ENTERTAINMENT,
    status=ToolStatus.AVAILABLE,
    setup_type=SetupType.NONE,
    icon="SiYoutube",
    icon_color="text-red-600",
    config_fields=[
        ConfigField(
            name="enable_get_video_captions",
            label="Enable Get Video Captions",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_video_data",
            label="Enable Get Video Data",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_video_timestamps",
            label="Enable Get Video Timestamps",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="languages",
            label="Languages",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="proxies",
            label="Proxies",
            type="text",
            required=False,
            default=None,
        ),
    ],
    dependencies=["youtube_transcript_api"],
    docs_url="https://docs.agno.com/tools/toolkits/entertainment/youtube",
)
def youtube_tools() -> type[YouTubeTools]:
    """Return YouTube tools for video data extraction."""
    from agno.tools.youtube import YouTubeTools

    return YouTubeTools

================================================================================
# FILE: src/mindroom/tools/zendesk.py
================================================================================

"""Zendesk tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.zendesk import ZendeskTools


@register_tool_with_metadata(
    name="zendesk",
    display_name="Zendesk",
    description="Customer support platform for searching help center articles",
    category=ToolCategory.DEVELOPMENT,  # From others/ category
    status=ToolStatus.REQUIRES_CONFIG,  # Requires username, password, company_name
    setup_type=SetupType.API_KEY,  # Uses username/password authentication
    icon="SiZendesk",  # React icon for help/support
    icon_color="text-green-600",  # Zendesk brand green
    config_fields=[
        ConfigField(
            name="username",
            label="Username",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="password",
            label="Password",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="company_name",
            label="Company Name",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="enable_search_zendesk",
            label="Enable Search Zendesk",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/others/zendesk",
)
def zendesk_tools() -> type[ZendeskTools]:
    """Return Zendesk tools for searching help center articles."""
    from agno.tools.zendesk import ZendeskTools

    return ZendeskTools

================================================================================
# FILE: src/mindroom/tools/zep.py
================================================================================

"""Zep memory system tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import ConfigField, SetupType, ToolCategory, ToolStatus, register_tool_with_metadata

if TYPE_CHECKING:
    from agno.tools.zep import ZepTools


@register_tool_with_metadata(
    name="zep",
    display_name="Zep Memory",
    description="Memory system for storing, retrieving, and searching conversational data",
    category=ToolCategory.PRODUCTIVITY,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.API_KEY,
    icon="Brain",
    icon_color="text-purple-600",
    config_fields=[
        ConfigField(
            name="session_id",
            label="Session ID",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="user_id",
            label="User ID",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="api_key",
            label="API Key",
            type="password",
            required=False,
            default=None,
        ),
        ConfigField(
            name="ignore_assistant_messages",
            label="Ignore Assistant Messages",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="enable_add_zep_message",
            label="Enable Add Zep Message",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_get_zep_memory",
            label="Enable Get Zep Memory",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="enable_search_zep_memory",
            label="Enable Search Zep Memory",
            type="boolean",
            required=False,
            default=True,
        ),
        ConfigField(
            name="instructions",
            label="Instructions",
            type="text",
            required=False,
            default=None,
        ),
        ConfigField(
            name="add_instructions",
            label="Add Instructions",
            type="boolean",
            required=False,
            default=False,
        ),
        ConfigField(
            name="all",
            label="All",
            type="boolean",
            required=False,
            default=False,
        ),
    ],
    dependencies=["zep-cloud"],
    docs_url="https://docs.agno.com/tools/toolkits/database/zep",
)
def zep_tools() -> type[ZepTools]:
    """Return Zep memory tools for storing and retrieving conversational data."""
    from agno.tools.zep import ZepTools

    return ZepTools

================================================================================
# FILE: src/mindroom/tools/zoom.py
================================================================================

"""Zoom tool configuration."""

from __future__ import annotations

from typing import TYPE_CHECKING

from mindroom.tools_metadata import (
    ConfigField,
    SetupType,
    ToolCategory,
    ToolStatus,
    register_tool_with_metadata,
)

if TYPE_CHECKING:
    from agno.tools.zoom import ZoomTools


@register_tool_with_metadata(
    name="zoom",
    display_name="Zoom",
    description="Video conferencing platform for scheduling and managing meetings",
    category=ToolCategory.SOCIAL,
    status=ToolStatus.REQUIRES_CONFIG,
    setup_type=SetupType.OAUTH,
    icon="SiZoom",
    icon_color="text-blue-500",  # Zoom blue
    config_fields=[
        # Authentication parameters
        ConfigField(
            name="account_id",
            label="Account ID",
            type="text",
            required=False,
            placeholder="your_account_id",
            description="Zoom account ID from Server-to-Server OAuth app (can also be set via ZOOM_ACCOUNT_ID env var)",
        ),
        ConfigField(
            name="client_id",
            label="Client ID",
            type="text",
            required=False,
            placeholder="your_client_id",
            description="Client ID from Server-to-Server OAuth app (can also be set via ZOOM_CLIENT_ID env var)",
        ),
        ConfigField(
            name="client_secret",
            label="Client Secret",
            type="password",
            required=False,
            placeholder="your_client_secret",
            description="Client secret from Server-to-Server OAuth app (can also be set via ZOOM_CLIENT_SECRET env var)",
        ),
    ],
    dependencies=["requests"],
    docs_url="https://docs.agno.com/tools/toolkits/social/zoom",
)
def zoom_tools() -> type[ZoomTools]:
    """Return Zoom tools for video conferencing and meeting management."""
    from agno.tools.zoom import ZoomTools

    return ZoomTools

================================================================================
# FILE: src/mindroom/tools_metadata.py
================================================================================

"""Tool metadata and enhanced registration system."""

from __future__ import annotations

from dataclasses import asdict, dataclass
from enum import Enum
from typing import TYPE_CHECKING, Any, Literal

from loguru import logger

from mindroom.config import Config
from mindroom.plugins import load_plugins

if TYPE_CHECKING:
    from collections.abc import Callable
    from pathlib import Path

    from agno.tools import Toolkit

from mindroom.credentials import get_credentials_manager

# Registry mapping tool names to their factory functions
TOOL_REGISTRY: dict[str, Callable[[], type[Toolkit]]] = {}


def register_tool(name: str) -> Callable[[Callable[[], type[Toolkit]]], Callable[[], type[Toolkit]]]:
    """Decorator to register a tool factory function.

    Args:
        name: The name to register the tool under

    Returns:
        Decorator function

    """

    def decorator(func: Callable[[], type[Toolkit]]) -> Callable[[], type[Toolkit]]:
        TOOL_REGISTRY[name] = func
        return func

    return decorator


def get_tool_by_name(tool_name: str) -> Toolkit:
    """Get a tool instance by its registered name."""
    if tool_name not in TOOL_REGISTRY:
        available = ", ".join(sorted(TOOL_REGISTRY.keys()))
        msg = f"Unknown tool: {tool_name}. Available tools: {available}"
        raise ValueError(msg)

    try:
        tool_factory = TOOL_REGISTRY[tool_name]
        tool_class = tool_factory()

        creds_manager = get_credentials_manager()
        credentials = creds_manager.load_credentials(tool_name) or {}
        metadata = TOOL_METADATA[tool_name]

        init_kwargs = {}
        if metadata.config_fields:
            for field in metadata.config_fields:
                if field.name in credentials:
                    init_kwargs[field.name] = credentials[field.name]

        return tool_class(**init_kwargs)

    except ImportError as e:
        logger.warning(f"Could not import tool '{tool_name}': {e}")
        logger.warning(f"Make sure the required dependencies are installed for {tool_name}")
        raise


class ToolCategory(str, Enum):
    """Tool categories for organization."""

    EMAIL = "email"
    SHOPPING = "shopping"
    ENTERTAINMENT = "entertainment"
    SOCIAL = "social"
    DEVELOPMENT = "development"
    RESEARCH = "research"
    INFORMATION = "information"
    PRODUCTIVITY = "productivity"
    COMMUNICATION = "communication"
    INTEGRATIONS = "integrations"
    SMART_HOME = "smart_home"


class ToolStatus(str, Enum):
    """Tool availability status."""

    AVAILABLE = "available"
    COMING_SOON = "coming_soon"
    REQUIRES_CONFIG = "requires_config"


class SetupType(str, Enum):
    """Tool setup type."""

    NONE = "none"  # No setup required
    API_KEY = "api_key"  # Requires API key
    OAUTH = "oauth"  # OAuth flow
    SPECIAL = "special"  # Special setup (e.g., for Google)
    COMING_SOON = "coming_soon"  # Not yet available


@dataclass
class ConfigField:
    """Definition of a configuration field."""

    name: str  # Environment variable name (e.g., "SMTP_HOST")
    label: str  # Display label (e.g., "SMTP Host")
    type: Literal["boolean", "number", "password", "text", "url", "select"] = "text"
    required: bool = True
    default: Any = None
    placeholder: str | None = None
    description: str | None = None
    options: list[dict[str, str]] | None = None  # For select type
    validation: dict[str, Any] | None = None  # min, max, pattern, etc.


@dataclass
class ToolMetadata:
    """Complete metadata for a tool."""

    name: str  # Internal tool name (e.g., "gmail")
    display_name: str  # Display name (e.g., "Gmail")
    description: str  # Description for UI
    category: ToolCategory
    status: ToolStatus = ToolStatus.AVAILABLE
    setup_type: SetupType = SetupType.NONE
    icon: str | None = None  # Icon identifier for frontend
    icon_color: str | None = None  # Tailwind color class like "text-blue-500"
    config_fields: list[ConfigField] | None = None  # Detailed field definitions
    dependencies: list[str] | None = None  # Required pip packages
    auth_provider: str | None = None  # Name of integration that provides auth (e.g., "google")
    docs_url: str | None = None  # Documentation URL
    helper_text: str | None = None  # Additional help text for setup
    factory: Callable | None = None  # Factory function to create tool instance


# Global registry for tool metadata
TOOL_METADATA: dict[str, ToolMetadata] = {}


def register_tool_with_metadata(
    *,
    name: str,
    display_name: str,
    description: str,
    category: ToolCategory,
    status: ToolStatus = ToolStatus.AVAILABLE,
    setup_type: SetupType = SetupType.NONE,
    icon: str | None = None,
    icon_color: str | None = None,
    config_fields: list[ConfigField] | None = None,
    dependencies: list[str] | None = None,
    auth_provider: str | None = None,
    docs_url: str | None = None,
    helper_text: str | None = None,
) -> Callable[[Callable[[], type]], Callable[[], type]]:
    """Decorator to register a tool with metadata.

    This decorator stores comprehensive metadata about tools that can be used
    by the frontend and other components.

    Args:
        name: Tool identifier used in registry
        display_name: Human-readable name for UI
        description: Brief description of what the tool does
        category: Tool category for organization
        status: Availability status of the tool
        setup_type: Type of setup required
        icon: Icon identifier for frontend
        icon_color: CSS color class for the icon
        config_fields: List of configuration fields
        dependencies: Required Python packages
        auth_provider: Name of integration that provides authentication
        docs_url: Link to documentation
        helper_text: Additional setup instructions

    Returns:
        Decorator function

    """

    def decorator(func: Callable) -> Callable:
        # Create metadata object
        metadata = ToolMetadata(
            name=name,
            display_name=display_name,
            description=description,
            category=category,
            status=status,
            setup_type=setup_type,
            icon=icon,
            icon_color=icon_color,
            config_fields=config_fields,
            dependencies=dependencies,
            auth_provider=auth_provider,
            docs_url=docs_url,
            helper_text=helper_text,
            factory=func,
        )

        # Store in metadata registry
        TOOL_METADATA[name] = metadata

        # Also register in TOOL_REGISTRY for actual tool loading
        TOOL_REGISTRY[name] = func

        return func

    return decorator


def get_tool_metadata(name: str) -> ToolMetadata | None:
    """Get metadata for a tool by name."""
    return TOOL_METADATA.get(name)


def get_all_tool_metadata() -> dict[str, ToolMetadata]:
    """Get all tool metadata."""
    return TOOL_METADATA.copy()


def ensure_tool_registry_loaded(config: Config | None = None, *, config_path: Path | None = None) -> None:
    """Ensure core and plugin tools are registered in the metadata registry."""
    import mindroom.tools  # noqa: F401, PLC0415  # import here to avoid tools_metadata cycle

    if config is None and config_path is not None:
        config = Config.from_yaml(config_path)

    if config is None:
        return

    load_plugins(config, config_path=config_path)


def export_tools_metadata() -> list[dict[str, Any]]:
    """Export tool metadata as JSON-serializable dictionaries."""
    tools: list[dict[str, Any]] = []

    for metadata in TOOL_METADATA.values():
        tool_dict = asdict(metadata)
        tool_dict["category"] = metadata.category.value
        tool_dict["status"] = metadata.status.value
        tool_dict["setup_type"] = metadata.setup_type.value
        tool_dict.pop("factory", None)
        tools.append(tool_dict)

    tools.sort(key=lambda tool: (tool["category"], tool["name"]))
    return tools

================================================================================
# FILE: src/mindroom/topic_generator.py
================================================================================

"""Generate contextual topics for Matrix rooms using AI."""

from __future__ import annotations

from typing import TYPE_CHECKING

import nio
from agno.agent import Agent
from pydantic import BaseModel, Field

from mindroom.ai import _cached_agent_run, get_model_instance
from mindroom.constants import STORAGE_PATH_OBJ
from mindroom.logging_config import get_logger

if TYPE_CHECKING:
    from mindroom.config import Config

logger = get_logger(__name__)


class RoomTopic(BaseModel):
    """Structured room topic response."""

    topic: str = Field(description="The room topic - concise, informative, with emoji")


async def generate_room_topic_ai(room_key: str, room_name: str, config: Config) -> str | None:
    """Generate a contextual topic for a room using AI based on its purpose and configured agents.

    Args:
        room_key: The room key/alias (e.g., 'dev', 'analysis', 'lobby')
        room_name: Display name for the room
        config: Configuration with agent settings

    Returns:
        A contextual topic string for the room

    """
    # Get agents configured for this room
    agents_in_room = []
    for agent_name, agent_config in config.agents.items():
        if room_key in agent_config.rooms:
            display_name = agent_config.display_name or agent_name
            agents_in_room.append(display_name)

    # Build agent list for the prompt
    agent_list = ", ".join(agents_in_room)

    prompt = f"""Generate a concise, informative room topic for a MindRoom Matrix room.

Context about MindRoom:
MindRoom is a platform that frees AI agents from being trapped in single apps. Key features:
- AI agents with persistent memory that work across all platforms (Slack, Discord, Telegram, WhatsApp)
- Agents collaborate naturally in threads and remember everything across sessions
- Built on Matrix protocol for secure, federated communication
- 100+ integrations with tools like Gmail, GitHub, Spotify, Home Assistant
- Self-hosted or cloud options with military-grade encryption

Room details:
- Room key/alias: {room_key}
- Room name: {room_name}
- Configured agents: {agent_list if agent_list else "No specific agents configured yet"}

Create a topic that:
1. Describes the room's purpose based on its name
2. Mentions the AI agents or capabilities available
3. Highlights MindRoom's persistent memory or cross-platform nature when relevant
4. Is welcoming and informative
5. Uses 1-2 relevant emojis
6. Is under 100 characters
7. Follows this format: [emoji] [Description] â€¢ [Capabilities/Purpose]

Examples:
- ðŸ’» Development Hub â€¢ AI agents that remember your code patterns across sessions
- ðŸ“Š Analysis Center â€¢ Persistent insights with cross-platform data access
- ðŸ  Main Lobby â€¢ Your AI team headquarters with continuous memory
- ðŸ’° Finance Room â€¢ AI agents tracking markets 24/7 with full context
- ðŸ”¬ Research Lab â€¢ Collaborative AI exploration with shared knowledge

Generate the topic:"""

    model = get_model_instance(config, "default")

    agent = Agent(
        name="TopicGenerator",
        role="Generate contextual room topics",
        model=model,
        output_schema=RoomTopic,
    )

    session_id = f"topic_{room_key}"
    try:
        response = await _cached_agent_run(
            agent=agent,
            full_prompt=prompt,
            session_id=session_id,
            agent_name="TopicGenerator",
            storage_path=STORAGE_PATH_OBJ,
        )
    except Exception:
        logger.exception(f"Error generating topic for room {room_key}")
        return None
    content = response.content
    assert isinstance(content, RoomTopic)  # Type narrowing for mypy
    return content.topic


async def ensure_room_has_topic(
    client: nio.AsyncClient,
    room_id: str,
    room_key: str,
    room_name: str,
    config: Config,
) -> bool:
    """Ensure a room has a topic set, generating one if needed.

    Args:
        client: Matrix client
        room_id: The room ID
        room_key: The room key/alias
        room_name: Display name for the room
        config: Configuration with agent settings

    Returns:
        True if topic was set or already exists, False on error

    """
    # Check if room already has a topic
    response = await client.room_get_state_event(room_id, "m.room.topic")
    if isinstance(response, nio.RoomGetStateEventResponse) and response.content.get("topic"):
        logger.debug(f"Room {room_key} already has topic: {response.content['topic']}")
        return True

    # Generate and set topic
    logger.info(f"Generating AI topic for existing room {room_key}")
    topic = await generate_room_topic_ai(room_key, room_name, config)
    if topic is None:
        logger.warning(f"Failed to generate topic for room {room_key}")
        return False

    # Set the topic
    response = await client.room_put_state(
        room_id=room_id,
        event_type="m.room.topic",
        content={"topic": topic},
    )

    if isinstance(response, nio.RoomPutStateResponse):
        logger.info(f"Set topic for room {room_key}: {topic}")
        return True

    logger.warning(f"Failed to set topic for room {room_key}: {response}")
    return False

================================================================================
# FILE: src/mindroom/voice_handler.py
================================================================================

"""Voice message handler with speech-to-text and intelligent command recognition."""

from __future__ import annotations

import os
import ssl
import tempfile
import uuid
from pathlib import Path
from typing import TYPE_CHECKING

import aiofiles
import aiohttp
import nio
from agno.agent import Agent
from nio import crypto

from .ai import get_model_instance
from .commands import get_command_list
from .constants import VOICE_PREFIX
from .logging_config import get_logger

if TYPE_CHECKING:
    from .config import Config

logger = get_logger(__name__)


async def handle_voice_message(
    client: nio.AsyncClient,
    room: nio.MatrixRoom,  # noqa: ARG001
    event: nio.RoomMessageAudio | nio.RoomEncryptedAudio,
    config: Config,
) -> str | None:
    """Handle a voice message event.

    Args:
        client: Matrix client
        room: Matrix room
        event: Voice message event
        config: Application configuration

    Returns:
        The transcribed and formatted message, or None if transcription failed

    """
    if not config.voice.enabled:
        return None

    try:
        # Download the audio file
        audio_data = await _download_audio(client, event)
        if not audio_data:
            logger.error("Failed to download audio file")
            return None

        # Transcribe the audio
        transcription = await _transcribe_audio(audio_data, config)
        if not transcription:
            logger.warning("Failed to transcribe audio or empty transcription")
            return None

        logger.info(f"Raw transcription: {transcription}")

        # Process transcription with AI for command/agent recognition
        formatted_message = await _process_transcription(transcription, config)

        logger.info(f"Formatted message: {formatted_message}")

        if formatted_message:
            # Add a note that this was transcribed from voice
            return f"{VOICE_PREFIX}{formatted_message}"

    except Exception:
        logger.exception("Error handling voice message")
        return None
    return None


async def _download_audio(
    client: nio.AsyncClient,
    event: nio.RoomMessageAudio | nio.RoomEncryptedAudio,
) -> bytes | None:
    """Download and decrypt audio file from Matrix.

    Args:
        client: Matrix client
        event: Audio event

    Returns:
        Audio file bytes or None if failed

    """
    try:
        # Unencrypted audio
        mxc = event.url
        response = await client.download(mxc)
        if isinstance(response, nio.DownloadError):
            logger.error(f"Download failed: {response}")
            return None
        if isinstance(event, nio.RoomMessageAudio):
            return response.body  # type: ignore[no-any-return]

        assert isinstance(event, nio.RoomEncryptedAudio)
        # Decrypt the audio
        return crypto.attachments.decrypt_attachment(  # type: ignore[no-any-return]
            response.body,
            event.source["content"]["file"]["key"]["k"],
            event.source["content"]["file"]["hashes"]["sha256"],
            event.source["content"]["file"]["iv"],
        )

    except Exception:
        logger.exception("Error downloading audio")
    return None


async def _transcribe_audio(audio_data: bytes, config: Config) -> str | None:
    """Transcribe audio using OpenAI-compatible API.

    Args:
        audio_data: Audio file bytes
        config: Application configuration

    Returns:
        Transcription text or None if failed

    """
    try:
        # Save audio to temporary file (required by most STT APIs)
        with tempfile.NamedTemporaryFile(suffix=".ogg", delete=False) as tmp_file:
            tmp_file.write(audio_data)
            tmp_path = tmp_file.name

        try:
            # Use OpenAI-compatible API for transcription
            stt_host = config.voice.stt.host
            if stt_host:
                # Self-hosted solution
                url = f"{stt_host}/v1/audio/transcriptions"
            else:
                # OpenAI or compatible cloud service
                url = "https://api.openai.com/v1/audio/transcriptions"

            api_key = config.voice.stt.api_key or os.getenv("OPENAI_API_KEY")
            headers = {"Authorization": f"Bearer {api_key}"}

            # Prepare multipart form data
            async with aiofiles.open(tmp_path, "rb") as audio_file:
                audio_content = await audio_file.read()

            data = aiohttp.FormData()
            data.add_field("file", audio_content, filename="audio.ogg", content_type="audio/ogg")
            data.add_field("model", config.voice.stt.model)

            # Make the API request (with SSL verification disabled if needed)
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE

            connector = aiohttp.TCPConnector(ssl=ssl_context)
            async with (
                aiohttp.ClientSession(connector=connector) as session,
                session.post(url, headers=headers, data=data) as response,
            ):
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"STT API error: {response.status} - {error_text}")
                    return None

                result = await response.json()
                return result.get("text", "").strip()  # type: ignore[no-any-return]

        finally:
            # Clean up temporary file
            Path(tmp_path).unlink()

    except Exception:
        logger.exception("Error transcribing audio")
        return None


async def _process_transcription(transcription: str, config: Config) -> str:
    """Process transcription to recognize commands and agent names.

    Args:
        transcription: Raw transcription text
        config: Application configuration

    Returns:
        Formatted message with proper commands and mentions

    """
    try:
        # Get list of available agents and teams
        agent_names = list(config.agents.keys())
        agent_display_names = {name: cfg.display_name for name, cfg in config.agents.items()}

        team_names = list(config.teams.keys()) if config.teams else []
        team_display_names = {name: cfg.display_name for name, cfg in config.teams.items()} if config.teams else {}

        # Build the prompt for the AI
        prompt = f"""You are a voice command processor for a Matrix chat bot system.
Your task is to convert spoken transcriptions into properly formatted chat commands.

Available agents (use EXACT agent name after @):
{chr(10).join([f"  - @{name} or @mindroom_{name} (spoken as: {agent_display_names[name]})" for name in agent_names])}

Available teams (use EXACT team name after @):
{chr(10).join([f"  - @{name} (spoken as: {team_display_names[name]})" for name in team_names]) if team_names else "  (none)"}

Examples of correct formatting:
- User says "HomeAssistant turn on the fan" â†’ "@home turn on the fan"  (NOT @homeassistant)
- User says "schedule turn off the lights in 10 minutes" â†’ "!schedule in 10 minutes turn off the lights"
- User says "hey home assistant agent schedule to turn off the guest room lights in 10 seconds" â†’ "!schedule in 10 seconds @home turn off the guest room lights"
- User says "cancel schedule ABC123" â†’ "!cancel_schedule ABC123"
- User says "list my schedules" â†’ "!list_schedules"

{get_command_list()}

CRITICAL RULES:
1. ALWAYS use the EXACT agent name (the part before the parentheses) after @, NOT the display name
   - If agent is listed as "@home (spoken as: HomeAssistant)", use "@home" NOT "@homeassistant"
2. If the user speaks a command, format it as !command
3. !schedule commands MUST include a time (in X minutes, at 3pm, tomorrow, etc.)
   - The time should come right after !schedule
4. When both command AND agent are mentioned, command comes FIRST
5. Agent mentions come FIRST when just addressing them (no command):
   - "research agent, find papers" â†’ "@research find papers"
   - "ask the email agent to check mail" â†’ "@email check mail"
6. Fix common speech recognition errors (e.g., "at research" â†’ "@research")
7. Be smart about intent - "ask the research agent" means "@research"
8. Keep the natural language but add proper formatting
9. If unclear, prefer natural language over forcing commands

Transcription: "{transcription}"

Output the formatted message only, no explanation:"""

        # Get the AI model to process the transcription
        model = get_model_instance(config, config.voice.intelligence.model)

        # Create an agent for voice command processing
        agent = Agent(
            name="VoiceCommandProcessor",
            role="Convert voice transcriptions to properly formatted chat commands",
            model=model,
        )

        # Process the transcription with the agent
        session_id = f"voice_process_{uuid.uuid4()}"
        response = await agent.arun(prompt, session_id=session_id)

        # Extract the content from the response
        if response and response.content:
            return response.content.strip()  # type: ignore[no-any-return]

    except Exception as e:
        logger.exception("Error processing transcription")
        # Return error message so user knows what happened
        from .error_handling import get_user_friendly_error_message  # noqa: PLC0415

        return get_user_friendly_error_message(e, "VoiceProcessor")
    else:
        # Return original transcription if no valid response from model
        return transcription
